{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:106: UserWarning: Pandas requires version '1.2.1' or newer of 'bottleneck' (version '1.1.0' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchnlp.nn as nnnlp\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir\n",
    "from sys import getsizeof\n",
    "import os\n",
    "import nltk, re, pprint\n",
    "import nltk.tokenize as tk\n",
    "from collections import Counter\n",
    "import time\n",
    "import ast\n",
    "import sentencepiece as spm\n",
    "import seaborn as sns\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sentencepiece:\n",
      "\n",
      "NAME\n",
      "    sentencepiece\n",
      "\n",
      "DESCRIPTION\n",
      "    # This file was automatically generated by SWIG (http://www.swig.org).\n",
      "    # Version 3.0.10\n",
      "    #\n",
      "    # Do not make changes to this file unless you know what you are doing--modify\n",
      "    # the SWIG interface file instead.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        SentencePieceProcessor\n",
      "        SentencePieceTrainer\n",
      "    \n",
      "    class SentencePieceProcessor(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  DecodeIds(self, ids)\n",
      "     |  \n",
      "     |  DecodeIdsAsSerializedProto(self, ids)\n",
      "     |  \n",
      "     |  DecodePieces(self, pieces)\n",
      "     |  \n",
      "     |  DecodePiecesAsSerializedProto(self, pieces)\n",
      "     |  \n",
      "     |  EncodeAsIds(self, input)\n",
      "     |  \n",
      "     |  EncodeAsPieces(self, input)\n",
      "     |  \n",
      "     |  EncodeAsSerializedProto(self, input)\n",
      "     |  \n",
      "     |  GetPieceSize(self)\n",
      "     |  \n",
      "     |  GetScore(self, id)\n",
      "     |  \n",
      "     |  IdToPiece(self, id)\n",
      "     |  \n",
      "     |  IsControl(self, id)\n",
      "     |  \n",
      "     |  IsUnknown(self, id)\n",
      "     |  \n",
      "     |  IsUnused(self, id)\n",
      "     |  \n",
      "     |  Load(self, filename)\n",
      "     |  \n",
      "     |  LoadFromSerializedProto(self, serialized)\n",
      "     |  \n",
      "     |  LoadOrDie(self, filename)\n",
      "     |  \n",
      "     |  LoadVocabulary(self, filename, threshold)\n",
      "     |  \n",
      "     |  NBestEncodeAsIds(self, input, nbest_size)\n",
      "     |  \n",
      "     |  NBestEncodeAsPieces(self, input, nbest_size)\n",
      "     |  \n",
      "     |  NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
      "     |  \n",
      "     |  PieceToId(self, piece)\n",
      "     |  \n",
      "     |  ResetVocabulary(self)\n",
      "     |  \n",
      "     |  SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  SetDecodeExtraOptions(self, extra_option)\n",
      "     |  \n",
      "     |  SetEncodeExtraOptions(self, extra_option)\n",
      "     |  \n",
      "     |  SetVocabulary(self, valid_vocab)\n",
      "     |  \n",
      "     |  __del__ lambda self\n",
      "     |  \n",
      "     |  __getattr__ lambda self, name\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__ = _swig_repr(self)\n",
      "     |  \n",
      "     |  __setattr__ lambda self, name, value\n",
      "     |  \n",
      "     |  bos_id(self)\n",
      "     |  \n",
      "     |  decode_ids(self, input)\n",
      "     |  \n",
      "     |  decode_ids_as_serialized_proto(self, ids)\n",
      "     |  \n",
      "     |  decode_pieces(self, input)\n",
      "     |  \n",
      "     |  decode_pieces_as_serialized_proto(self, pieces)\n",
      "     |  \n",
      "     |  encode_as_ids(self, input)\n",
      "     |  \n",
      "     |  encode_as_pieces(self, input)\n",
      "     |  \n",
      "     |  encode_as_serialized_proto(self, input)\n",
      "     |  \n",
      "     |  eos_id(self)\n",
      "     |  \n",
      "     |  get_piece_size(self)\n",
      "     |  \n",
      "     |  get_score(self, id)\n",
      "     |  \n",
      "     |  id_to_piece(self, id)\n",
      "     |  \n",
      "     |  is_control(self, id)\n",
      "     |  \n",
      "     |  is_unknown(self, id)\n",
      "     |  \n",
      "     |  is_unused(self, id)\n",
      "     |  \n",
      "     |  load(self, filename)\n",
      "     |  \n",
      "     |  load_from_serialized_proto(self, filename)\n",
      "     |  \n",
      "     |  load_vocabulary(self, filename, threshold)\n",
      "     |  \n",
      "     |  nbest_encode_as_ids(self, input, nbest_size)\n",
      "     |  \n",
      "     |  nbest_encode_as_pieces(self, input, nbest_size)\n",
      "     |  \n",
      "     |  nbest_encode_as_serialized_proto(self, input, nbest_size)\n",
      "     |  \n",
      "     |  pad_id(self)\n",
      "     |  \n",
      "     |  piece_to_id(self, piece)\n",
      "     |  \n",
      "     |  reset_vocabulary(self)\n",
      "     |  \n",
      "     |  sample_encode_as_ids(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  sample_encode_as_pieces(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  sample_encode_as_serialized_proto(self, input, nbest_size, alpha)\n",
      "     |  \n",
      "     |  set_decode_extra_options(self, extra_option)\n",
      "     |  \n",
      "     |  set_encode_extra_options(self, extra_option)\n",
      "     |  \n",
      "     |  set_vocabulary(self, valid_vocab)\n",
      "     |  \n",
      "     |  unk_id(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __swig_getmethods__ = {}\n",
      "     |  \n",
      "     |  __swig_setmethods__ = {}\n",
      "    \n",
      "    class SentencePieceTrainer(builtins.object)\n",
      "     |  SentencePieceTrainer(*args, **kwargs)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getattr__ lambda self, name\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__ = _swig_repr(self)\n",
      "     |  \n",
      "     |  __setattr__ lambda self, name, value\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  Train = SentencePieceTrainer_Train(...)\n",
      "     |  \n",
      "     |  train = SentencePieceTrainer_train(...)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __swig_getmethods__ = {}\n",
      "     |  \n",
      "     |  __swig_setmethods__ = {}\n",
      "\n",
      "FUNCTIONS\n",
      "    SentencePieceProcessor_swigregister(...)\n",
      "    \n",
      "    SentencePieceTrainer_Train(...)\n",
      "    \n",
      "    SentencePieceTrainer_swigregister(...)\n",
      "    \n",
      "    SentencePieceTrainer_train(...)\n",
      "\n",
      "FILE\n",
      "    e:\\program files\\anaconda3\\lib\\site-packages\\sentencepiece.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended all lines from 002.txt\n",
      "Appended all lines from 003.txt\n",
      "Appended all lines from 004.txt\n",
      "Appended all lines from 005.txt\n"
     ]
    }
   ],
   "source": [
    "file = []\n",
    "for i in range(2, 6):\n",
    "    f = open('00' + str(i) + '.txt', 'r', encoding='UTF-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        file.append(line.strip('\\n'))\n",
    "    print('Appended all lines from 00' + str(i) + \".txt\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Propaganda is a concerted set of messages aime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Instead of impartially providing information, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The most effective propaganda is often complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The desired result is a change of the cognitiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The word originates from the Latin name Congre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424102</td>\n",
       "      <td>Pakistan does generally allow Indian citizens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424103</td>\n",
       "      <td>There are concerns that extensive deforestatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424104</td>\n",
       "      <td>There are also concerns that the Indus river m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424105</td>\n",
       "      <td>On numerous occasions, sediment clogging owing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424106</td>\n",
       "      <td>In addition, extreme heat has caused water to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424107 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "0       Propaganda is a concerted set of messages aime...\n",
       "1       Instead of impartially providing information, ...\n",
       "2       The most effective propaganda is often complet...\n",
       "3       The desired result is a change of the cognitiv...\n",
       "4       The word originates from the Latin name Congre...\n",
       "...                                                   ...\n",
       "424102  Pakistan does generally allow Indian citizens ...\n",
       "424103  There are concerns that extensive deforestatio...\n",
       "424104  There are also concerns that the Indus river m...\n",
       "424105  On numerous occasions, sediment clogging owing...\n",
       "424106  In addition, extreme heat has caused water to ...\n",
       "\n",
       "[424107 rows x 1 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "def printifmaxlen(s):\n",
    "    global maxlen\n",
    "    if(isinstance(s, float)):\n",
    "        print(\"float found: \" + str(s))\n",
    "        \n",
    "    elif len(s)> maxlen:\n",
    "        \n",
    "        print(len(s))\n",
    "        maxlen = len(s)\n",
    "#wiki_sentences.txt\n",
    "# lines = pd.Series(file)\n",
    "# lines = lines[~lines.isna()]\n",
    "# lines = lines.loc[lines!='+++$+++']\n",
    "# lines.to_csv(r'wiki_sentences.txt', header=False, index=None, encoding='UTF-8', sep='\\n')\n",
    "lines = pd.read_csv(r'wiki_sentences.txt', header=None, encoding='UTF-8', sep='\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader definition and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/train'\n",
    "def load_split_train_test(datadir, valid_size = .2):\n",
    "    train_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "    test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ])\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir,\n",
    "                    transform=test_transforms)\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=64)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=64)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = load_split_train_test(data_dir, .2)\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment and quality assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test = 0\n",
    "def train(model, train_loader, test_loader, \n",
    "          learning_rate=0.005, num_epoch=10, print_every=100):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    best_auc = [0, 0]\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = nn.CrossEntropyLoss()\n",
    "    loss_fns = [loss_fn, loss_fn2]\n",
    "    acc = 0\n",
    "    accs =[[], []]\n",
    "    aucs = [[], []]\n",
    "    losses = [[], []]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print('Beginning training for {} epochs'.format(num_epoch))\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            outputs = list(model(data))\n",
    "            model.zero_grad()\n",
    "            loss = []\n",
    "            loss_str = \"\"\n",
    "            #print(outputs)\n",
    "\n",
    "\n",
    "            for j in range(len(outputs)):\n",
    "                print(\"calculating loss\")\n",
    "                if len(outputs[j].shape) < 2:\n",
    "                    outputs[j] = outputs[j].unsqueeze(0)\n",
    "                #print(loss_fns)\n",
    "                loss.append(loss_fns[j](outputs[j].float(), labels[j].long().cuda()))\n",
    "                if(j <len(outputs)-1):\n",
    "                    loss[j].backward()\n",
    "                else:\n",
    "                    loss[j].backward()\n",
    "                loss_str += \"Last Loss {}: {:6.4f} |\".format(j, loss[j].item())\n",
    "            \n",
    "                \n",
    "                ##for each output, run the corresponding loss function with the corresponding label\n",
    "                \n",
    "            \n",
    "#             for output in outputs:\n",
    "#                 if(len(output.shape)<2):\n",
    "#                     output = output.unsqueeze(0)\n",
    "#             output0 = outputs[0]\n",
    "#             output1 = outputs[1]\n",
    "\n",
    "#             loss = loss_fn(output0.float(), y0.long())#labels.long())\n",
    "#             loss2 = loss_fn2(output1.float(), y1.long())#labels.long())\n",
    "\n",
    "#             loss.backward()\n",
    "#             loss2.backwards()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d}/{} | {:6d}/{:6d} batches |{} Elapsed Time: {:>9}'.format(\n",
    "                    epoch + 1, num_epoch, i + 1, len(train_loader), loss_str, time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))))     \n",
    "        correct = [0, 0]\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = [ [], []]\n",
    "        truths = [[], []]\n",
    "\n",
    "        full_output = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, labels) in enumerate(test_loader):\n",
    "                outputs = list(model(data))\n",
    "#                pred = []\n",
    "                for j in range(len(outputs)):#for each of the outputs\n",
    "                    while(len(outputs[j].shape)<2):##make sure each output has shape length 2\n",
    "                        outputs[j] = outputs[j].unsqueeze(0)\n",
    "                    if(outputs[j].shape[-1] <=2):##if output is binary, take the max along that dimension\n",
    "                        predictions[j].append(torch.max(outputs[j], 1)[1])\n",
    "                        predictions[j] += list(pred[j].cpu().numpy())\n",
    "                        correct[j] += (pred[j].cpu() == labels[j].squeeze().long().cpu()).sum()\n",
    "\n",
    "\n",
    "                    else:##otherwise, using the maximums as the indices, generate an array of the same shape to the output\n",
    "\n",
    "                        idx = torch.max(outputs[j], 1)[1]\n",
    "                        preds = np.zeros_like(outputs.cpu().numpy())\n",
    "                        for i in range(preds.shape[0]):\n",
    "                            preds[i, idx[i]] = 1\n",
    "                        \n",
    "                        predictions[j].append(preds)\n",
    "                        correct[j] += (idx.cpu() == labels[j].squeeze().long().cpu()).sum()\n",
    "\n",
    "                        \n",
    "                    #print(pred)\n",
    "                    truths[j] += list(labels[j].cpu().numpy().flatten())\n",
    "                    total += labels[0].size(0)\n",
    "\n",
    "\n",
    "#                 pred = []\n",
    "#                 pred.append(torch.max(output[0], 1)[1])\n",
    "#                 pred.append(torch.max(output[1], 1)[1])\n",
    "#                 #print(output, pred)\n",
    "#                 #print(output.shape)\n",
    "#                 #output = nn.Softmax(dim=1)(output)\n",
    "#                 full_output += list(output[0].cpu().numpy())\n",
    "#                 predictions[0] += list(pred[0].cpu().numpy())\n",
    "#                 truths[0] += list(labels[0].cpu().numpy().flatten())\n",
    "#                 correct[0] += (pred[0].cpu() == labels[0].squeeze().long().cpu()).sum()\n",
    "#                 predictions[1] += list(pred[1].cpu().numpy())\n",
    "#                 truths[1] += list(labels[1].cpu().numpy().flatten())\n",
    "#                 correct[1] += (pred[1].cpu() == labels[1].squeeze().long().cpu()).sum()\n",
    "               \n",
    "            #print(truths)\n",
    "            #print(predictions)\n",
    "#            acc= [[], []]\n",
    "            for j in range(len(outputs)):\n",
    "                if(outputs[j].shape[-1] >=2):\n",
    "                    preds[j] = np.concatenate(preds[j], axis=1)\n",
    "            acc_str = \"\"\n",
    "            auc_str = \"\"\n",
    "            for j in range(len(outputs)):\n",
    "                \n",
    "                accs[j].append(100.0 * correct[j] / total)\n",
    "                #print(accs[j][len(accs[j])-1])\n",
    "                acc_str += \"Accuracy {}: {:6.4f} |\".format(j, accs[j][len(accs[j])-1].item())\n",
    "                if(outputs.shape[-1] <= 2):\n",
    "                    aucs[j].append(metrics.roc_auc_score(truths[j], predictions[j]))\n",
    "                else:\n",
    "                    aucs[j].append(metrics.roc_auc_score(truths[j], predictions[j], multi_class = \"ovo\"))\n",
    "                auc_str += \"AUC {}: {:6.4f} |\".format(j, aucs[j][len(aucs[j])-1].item())\n",
    "\n",
    "                losses[j].append(loss[j])\n",
    "                #accs[j].append(acc)\n",
    "            #print(full_output)\n",
    "            full_output = np.array(full_output)#.tolist()\n",
    "            #preds = np.zeros_like(full_output)\n",
    "#             for i in range(len(predictions)):\n",
    "#                 preds[i, predictions[i]]=1\n",
    "# #            print(predictions, \"\\n\", preds)\n",
    "\n",
    "            #print(full_output.shape)\n",
    "#             auc = []\n",
    "# #             for i in range(len(labels)):\n",
    "#                           metrics.roc_auc_score(truths, predictions)#, multi_class = \"ovo\")\n",
    "#             aucs.append(auc)\n",
    "            if(aucs[0][len(aucs[0])-1] > best_auc[0]):\n",
    "                best_auc[0] = aucs[0][len(aucs[0])-1]\n",
    "                best_model_wts = model.state_dict()\n",
    "#            cm = metrics.confusion_matrix(truths[0], predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set Label 0 | Epoch: {} |{} |{} |{} | Time elapsed: {:>9} | Sample output: {}'.format(\n",
    "               epoch + 1, acc_str, auc_str, loss_str, elapse, outputs[0][0,:]))\n",
    " #           print(cm)\n",
    "            \n",
    "\n",
    "            #print('Test set | Epoch: {} | Accuracy: {:4.2f} | Time Elapsed: {:>9}'.format(epoch+1, \n",
    "             #   acc, elapse))\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, predictions, truths, accs, losses, aucs ##save this for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=2, bias=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6fb3b08b6dd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnhid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocNum\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maucs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=0\n",
    "BATCH_SIZE = 32\n",
    "torch.manual_seed(111)\n",
    "ntokens = 32000\n",
    "emsize = 24 # embedding dimension\n",
    "nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 2\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout, insize=docNum * 100)\n",
    "print(model.fc2)\n",
    "model = model.float().cuda()\n",
    "model, predictions, truths, accs, losses, aucs = train(model, train_loader=train_loader, test_loader=test_loader, learning_rate = 0.1e-2,num_epoch=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truths = [0, 1, 2, 3, 4]\n",
    "preds = np.ones([5, 5])\n",
    "preds = preds -.81\n",
    "for truth in truths:\n",
    "    preds[truth, truth] = .24\n",
    "    \n",
    "metrics.roc_auc_score(truths, preds, multi_class = \"ovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"rflip4\"\n",
    "def save():\n",
    "    with open(name + '-accs-losses-AUCS-6-25-20.csv', 'w') as f:\n",
    "        f.write(\"EPOCH, ACCURACY, LOSS, AUC\\n\")\n",
    "        for i in range(len(losses)):\n",
    "            f.write(\"{}, {}, {}, {}\\n\".format(i, accs[i], losses[i], aucs[i]))\n",
    "    f.close()\n",
    "    with open(name + '-preds-truths-6-25-20.csv', 'w') as f:\n",
    "        f.write(\"PREDICTION, TRUTH\\n\")\n",
    "        for i in range(len(losses)):\n",
    "            f.write(\"{}, {}\\n\".format(predictions[i], truths[i]))\n",
    "    f.close()\n",
    "    torch.save(model.state_dict(), r'E:/Documents/JohnsonLab/' + name + r'-wts-07-03-20.pth')\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
