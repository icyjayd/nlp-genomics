{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/home/jic286/.local/lib/python3.6/site-packages',\n",
       " '',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python36.zip',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python3.6',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python3.6/lib-dynload',\n",
       " '/gpfs/home/jic286/.local/lib/python3.6/site-packages',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python3.6/site-packages',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python3.6/site-packages/openmmlib-0.0.0-py3.6-linux-x86_64.egg',\n",
       " '/gpfs/share/apps/anaconda3/gpu/5.2.0/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/gpfs/home/jic286/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/gpfs/home/jic286/.local/lib/python3.6/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os import listdir\n",
    "from sys import getsizeof\n",
    "import os\n",
    "import nltk, re, pprint\n",
    "import nltk.tokenize as tkn\n",
    "from collections import Counter\n",
    "import time\n",
    "import ast\n",
    "import sentencepiece as spm\n",
    "import seaborn as sns\n",
    "# import pytorch_lightning as pl\n",
    "from transformers import XLNetModel, XLNetTokenizer\n",
    "#%cd R:\\\\sbj2002lab\\\\sbj2002labspace\\\\kangdata\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.load('H:/Documents/JohnsonLab/NoteTextSentences_32000.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cu102'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model  = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "# model = XLNetModel(XLNetConfig(vocab_size=33000))\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reports' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9bffc9a2f1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TEXT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reports' is not defined"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('kang_data_cleaned_8000.model')\n",
    "lengths = 0\n",
    "max_length = 0\n",
    "min_length = np.inf\n",
    "for report in reports['TEXT']:\n",
    "    length = len(report)\n",
    "    if(length>max_length):\n",
    "        max_length = length\n",
    "    elif(length<min_length):\n",
    "        min_length = length\n",
    "    if(length<20):\n",
    "        print(length)\n",
    "    lengths +=length\n",
    "print(\"Average: \", lengths/len(reports))\n",
    "print(\"Min: \", min_length)\n",
    "print(\"Max: \", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv(r'/mnt/e/Documents/JohnsonLab/wiki_sentences.csv', header=None)\n",
    "test = wiki.iloc[0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.Series(data=cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R:\\\\sbj2002lab\\\\sbj2002labspace\\\\kangdata'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"corpuscleaned.txt\", \"w\")\n",
    "f.write(str(cleaned)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned.to_csv(r'corpuscleaned.txt',sep=\" \", index=False, header=False)\n",
    "\n",
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "    for s in sentences:\n",
    "        word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", s.lower()))\n",
    "    \n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortEmbeddings(path, emb_dim = 50):\n",
    "    \n",
    "\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        glove_embedding = []\n",
    "        words = {}\n",
    "        chars = {}\n",
    "        idx2words = {}\n",
    "        ordered_words = []\n",
    "    \n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            s = line.split()\n",
    "            glove_embedding.append(np.asarray(s[1:]))\n",
    "            \n",
    "            words[s[0]] = len(words)\n",
    "            idx2words[i] = s[0]\n",
    "            ordered_words.append(s[0])\n",
    "\n",
    "        \n",
    "    # add unknown to word and char\n",
    "    glove_embedding.append(np.random.rand(emb_dim))\n",
    "    words[\"<UNK>\"] = len(words)\n",
    "    \n",
    "    # add padding\n",
    "    glove_embedding.append(np.zeros(emb_dim))\n",
    "    words[\"<PAD>\"] = len(words)\n",
    "    \n",
    "    chars[\"<UNK>\"] = len(chars)\n",
    "    chars[\"<PAD>\"] = len(chars)\n",
    "    \n",
    "    glove_embedding = np.array(glove_embedding).astype(float)\n",
    "    return glove_embedding, words, idx2words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 316.8503523229428\n",
      "Min: 25\n",
      "Max: 242\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2048577</th>\n",
       "      <td>NPN 7p-7a\\n\\n\\nResp: Infant remains on nasal p...</td>\n",
       "      <td>3</td>\n",
       "      <td>30387</td>\n",
       "      <td>142196</td>\n",
       "      <td>22809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575887</th>\n",
       "      <td>Please See Carevue for Specifics.\\n\\nNEURO:  A...</td>\n",
       "      <td>3</td>\n",
       "      <td>24018</td>\n",
       "      <td>173153</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947925</th>\n",
       "      <td>[**2141-2-26**] 9:50 AM\\n CHEST (PA &amp; LAT)    ...</td>\n",
       "      <td>13</td>\n",
       "      <td>17134</td>\n",
       "      <td>110613</td>\n",
       "      <td>1830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550255</th>\n",
       "      <td>89F with h/o poorly controlled HTN, and modera...</td>\n",
       "      <td>3</td>\n",
       "      <td>83382</td>\n",
       "      <td>148152</td>\n",
       "      <td>2252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813193</th>\n",
       "      <td>[**2141-5-4**] 7:53 AM\\n FEM/[**Doctor Last Na...</td>\n",
       "      <td>13</td>\n",
       "      <td>3977</td>\n",
       "      <td>111024</td>\n",
       "      <td>2113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262416</th>\n",
       "      <td>ADMIT NOTE\\nBRIEFLY: 69 Y/O MALE ADMITTED TO [...</td>\n",
       "      <td>3</td>\n",
       "      <td>804</td>\n",
       "      <td>138064</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047684</th>\n",
       "      <td>Respiratory Care Note\\nBaby Girl [**Known last...</td>\n",
       "      <td>3</td>\n",
       "      <td>30387</td>\n",
       "      <td>142196</td>\n",
       "      <td>22809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375353</th>\n",
       "      <td>CCU NPN\\n\\nCV: ADEQUATE BP'S OFF ALL PRESSORS....</td>\n",
       "      <td>3</td>\n",
       "      <td>8943</td>\n",
       "      <td>111299</td>\n",
       "      <td>1748</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915949</th>\n",
       "      <td>[**2118-8-4**] 8:12 PM\\n MR [**Name13 (STitle)...</td>\n",
       "      <td>13</td>\n",
       "      <td>10424</td>\n",
       "      <td>186214</td>\n",
       "      <td>1736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795264</th>\n",
       "      <td>[**2116-10-13**] 3:42 AM\\n CHEST (PORTABLE AP)...</td>\n",
       "      <td>13</td>\n",
       "      <td>3205</td>\n",
       "      <td>135362</td>\n",
       "      <td>1985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8373 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      TEXT  CATEGORY  \\\n",
       "2048577  NPN 7p-7a\\n\\n\\nResp: Infant remains on nasal p...         3   \n",
       "1575887  Please See Carevue for Specifics.\\n\\nNEURO:  A...         3   \n",
       "947925   [**2141-2-26**] 9:50 AM\\n CHEST (PA & LAT)    ...        13   \n",
       "550255   89F with h/o poorly controlled HTN, and modera...         3   \n",
       "813193   [**2141-5-4**] 7:53 AM\\n FEM/[**Doctor Last Na...        13   \n",
       "...                                                    ...       ...   \n",
       "1262416  ADMIT NOTE\\nBRIEFLY: 69 Y/O MALE ADMITTED TO [...         3   \n",
       "2047684  Respiratory Care Note\\nBaby Girl [**Known last...         3   \n",
       "1375353  CCU NPN\\n\\nCV: ADEQUATE BP'S OFF ALL PRESSORS....         3   \n",
       "915949   [**2118-8-4**] 8:12 PM\\n MR [**Name13 (STitle)...        13   \n",
       "795264   [**2116-10-13**] 3:42 AM\\n CHEST (PORTABLE AP)...        13   \n",
       "\n",
       "         SUBJECT_ID  HADM_ID  ICD9_CODE  LABEL  \n",
       "2048577       30387   142196      22809      0  \n",
       "1575887       24018   173153       2126      0  \n",
       "947925        17134   110613       1830      1  \n",
       "550255        83382   148152       2252      0  \n",
       "813193         3977   111024       2113      0  \n",
       "...             ...      ...        ...    ...  \n",
       "1262416         804   138064       1991      1  \n",
       "2047684       30387   142196      22809      0  \n",
       "1375353        8943   111299       1748      1  \n",
       "915949        10424   186214       1736      1  \n",
       "795264         3205   135362       1985      1  \n",
       "\n",
       "[8373 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# valid = [0, 1]\n",
    "# reports = pd.read_csv(r'Kang_MRI_Report_Metadata.csv')##8-26-20 - working with stella data\n",
    "# reports = reports[~reports['TEXT'].isna()]\n",
    "# reports = reports[reports['TEXT'].apply(lambda x:True if len(x)>3 else False)]\n",
    "# print(len(reports))\n",
    "# reports = reports[reports.iloc[:, 9].apply(lambda x: True if x in valid else False)]\n",
    "# print('data length: ', len(reports))\n",
    "# # for i in range(8, len(reports.columns)-2 ):\n",
    "# #     print(reports.iloc[:,i].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training, testing = train_test_split(reports, test_size=0.2)\n",
    "# training.iloc[:,9].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "#testnotes = notes[~notes['ISERROR'].notnull() & notes['CATEGORY'].notnull() & notes['CHARTTIME'].notnull() & notes['STORETIME'].notnull()\n",
    "#                  & notes['DESCRIPTION'].notnull() & notes['HADM_ID'].notnull()]\n",
    "#testnotes['ISERROR'] = testnotes['ISERROR'].fillna(0)\n",
    "#neoplasm = testnotes[testnotes['HADM_ID'].isin(sample['HADM_ID'])]\n",
    "#neoplasm.to_csv(r'MIMIC_neoplasm_timestamped.csv')\n",
    "def sentences_to_padded_index_sequences(words, sentences, pad_length=100):\n",
    "    padded_sequences = np.zeros((len(sentences), pad_length))\n",
    "    for i, s in enumerate(sentences):\n",
    "        indices = np.ones(pad_length) * words['<PAD>']\n",
    "        # only take the first pad_length tokens\n",
    "        token_indices = np.array([words[w] if w in words else words['<UNK>'] for w in re.findall(r\"[\\w']+|[.,!?;]\", s.lower())[:pad_length]])\n",
    "        indices[:len(token_indices)] = token_indices\n",
    "        padded_sequences[i] = indices\n",
    "    return padded_sequences\n",
    "\n",
    "def sentence_to_padded_index_sequence(sentence, sp, pad_length = 100, pad_token=0):\n",
    "    sequence = sp.EncodeAsIds(sentence)\n",
    "    if(len(sequence)>pad_length):\n",
    "        sequence = sequence[:pad_length]\n",
    "    else:\n",
    "        while len(sequence) <pad_length:\n",
    "            sequence.append(pad_token)\n",
    "    return sequence\n",
    "\n",
    "neoplasm = pd.read_csv(r'H:/Documents/JohnsonLab/MIMIC_neoplasms.csv', index_col=0)\n",
    "neoplasm = neoplasm[neoplasm['TEXT'].apply(lambda x: False if len(sp.EncodeAsIds(x)) <10 else True)]\n",
    "# neoplasm['TOKENS'] = neoplasm['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "total = 0\n",
    "smallest = np.inf\n",
    "largest = -np.inf\n",
    "for i, note in enumerate(neoplasm['TEXT']):\n",
    "    length = len(sp.EncodeAsIds(note))\n",
    "#     if(length <15):\n",
    "#         print(\"Note Index: \" + str(i) + \"\\n\" + note)\n",
    "    total+=length\n",
    "    if(len(note)<smallest):\n",
    "        smallest = length\n",
    "    if(len(note)>largest):\n",
    "        largest = length\n",
    "print(\"Mean: {}\\nMin: {}\\nMax: {}\".format(total/len(neoplasm['TEXT']), smallest, largest))\n",
    "# testnotes = testnotes.loc[testnotes['HADM_ID'].isin(diagnoses['HADM_ID'])]\n",
    "# ID2ICD9 = dict(zip(diagnoses['HADM_ID'].values, diagnoses['ICD9_CODE'].values))\n",
    "# testnotes['ICD9_CODE'] = testnotes['HADM_ID'].apply(lambda x:ID2ICD9[x] )\n",
    "neoplasm\n",
    "\n",
    "# testnotes.head()\n",
    "#diagnoses[diagnoses['HADM_ID']==100247]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neoplasm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3042caefeb8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mCUDA_LAUNCH_BLOCKING\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneoplasm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neoplasm' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "BATCH_SIZE = 4\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=0\n",
    "training, testing = train_test_split(neoplasm, train_size=0.8, random_state=3)\n",
    "\n",
    "class BioDataset(Dataset):\n",
    "    def __init__(self, data, pad_length = 4510, pad_token=1, label_idx=9):\n",
    "        self.sentences = data['TEXT'].values.tolist()#.astype(int)\n",
    "        self.labels = data.iloc[:,label_idx]\n",
    "        if(\"Nutrition\" in self.labels.values):\n",
    "\n",
    "            self.uniques = dict(zip(self.labels.unique(), np.arange(len(self.labels.unique()))))\n",
    "            print(len(self.uniques))\n",
    "        global sp\n",
    "        self.sp = sp\n",
    "        self.pad_length = pad_length\n",
    "        self.pad_token= pad_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def sentence_to_padded_index_sequence(self, sentence):\n",
    "        #print(type(sentence))\n",
    "        sequence = self.sp.EncodeAsIds(sentence)\n",
    "        if(len(sequence)>self.pad_length):\n",
    "            sequence = sequence[:self.pad_length]\n",
    "        else:\n",
    "            sequence += [self.pad_token] * (self.pad_length-len(sequence))\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "#         print(self.sentences[key])\n",
    "        x = np.array(self.sentence_to_padded_index_sequence(self.sentences[key]))\n",
    "       \n",
    "#         print(self.sp.DecodeIds(list(map(int, x))))\n",
    "#         print(x.shape)\n",
    "        if(np.isnan(x).any()):\n",
    "            print(key)\n",
    "        if(\"Nutrition\" in self.labels.values):\n",
    " \n",
    "            y = self.uniques[self.labels[key]]\n",
    "        else:\n",
    "#             print(\"binary case\")\n",
    "            y =  np.array(self.labels[key])\n",
    "#         print(y.shape)\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "\n",
    "class XLNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, ntokens = 32000, nhid=1024, nclasses=2, dropout=0.5, insize = 100):\n",
    "        super(XLNet, self).__init__()\n",
    "        ##DOCUMENT TRUNCATOR\n",
    "\n",
    "        \n",
    "        #standard transformer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.xlnet = XLNetModel(XLNetConfig(vocab_size=32000, d_model=nhid))\n",
    "        self.xlnet.train()\n",
    "        \n",
    "\n",
    "\n",
    "        self.decoder = nn.Linear(nhid, 64)\n",
    "        self.fc = nn.Linear(64 * insize, nclasses * 2)\n",
    "    \n",
    "        self.fc2 = nn.Linear(nclasses * 2, nclasses)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout =  nn.Dropout(dropout)\n",
    "        self.loss = nn.CrossEntropyLoss()#weight=torch.Tensor([11/396, 385/396]))\n",
    "        self.preds = []\n",
    "        self.truths = []\n",
    "        self.accs = []\n",
    "        self.aucs = []\n",
    "        self.accuracy = pl.metrics.Accuracy()\n",
    "#         self.wts = self.cuda().state_dict()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"input: {}\".format(src.dtype))\n",
    "        #if(src.device == \"cpu\"):\n",
    "#   \n",
    "        output= self.xlnet(x)[0]\n",
    "#         print(output[0].shape)\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "\n",
    "#         print(\"decoder: {}\".format(output.dtype))\n",
    "        output = output.view(output.shape[0], -1)\n",
    "#         print(\"linearized decoder: {}\".format(output.shape))\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        output = self.relu2(output)\n",
    "\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        output = self.sigmoid(output)\n",
    "#         print(output.shape)\n",
    "\n",
    "\n",
    "#         output = self.softmax(output)\n",
    "        return output#, out2\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    def BCEWithLogitsLoss(self, logits, labels):\n",
    "        return self.loss(logits, labels)\n",
    "    \n",
    "    def CEL(self, pred, target):\n",
    "        return self.loss(pred, target)\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        #self.zero_grad()\n",
    "#         print(self.training)\n",
    "        x, y = train_batch\n",
    "#         print(x.shape, y.shape, x, y)\n",
    "        output = self(x)\n",
    "#         print(output)\n",
    "        #print(x.shape, y.shape)\n",
    "        loss = self.CEL(output, y.cuda().long())\n",
    "       \n",
    "        self.log('Cross Entropy Loss', loss)\n",
    "        return loss\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        output = self(x)\n",
    "        _, preds = torch.max(output, dim=1)\n",
    "#         print(output.shape)\n",
    "    \n",
    "        \n",
    "        \n",
    "#        print(preds.device, y.device)\n",
    "#         print(preds.shape, y.shape)\n",
    "        self.log('val_acc_step', self.accuracy(preds, y.cuda().long()), prog_bar=True)\n",
    "        self.log('val_loss',self.CEL(output, y.cuda().long()), prog_bar=True)\n",
    "        self.preds +=list(preds.cpu().numpy().flatten())\n",
    "#         self.preds.append(output.cpu())\n",
    "#         print(self.preds[0].shape)\n",
    "        self.truths +=list(y.cpu().numpy().flatten())\n",
    "        #return output\n",
    "\n",
    "        \n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "#         print(type(self.preds), type(self.truths))\n",
    "#         print(len(self.preds))\n",
    "#         print(self.truths, self.preds)\n",
    "#         print(self.state_dict(), self.wts)\n",
    "#         print(self.training)\n",
    "#         self.preds = torch.cat(self.preds, dim=0)\n",
    "#         if(len(self.preds.shape)<2):\n",
    "#             self.preds = self.preds.unsqueeze(0)\n",
    "#         print(self.truths)\n",
    "        torch.save(self.state_dict(), 'H:/Documents/JohnsonLab/KD-learn-test.pth')\n",
    "\n",
    "        auc = metrics.roc_auc_score( self.truths, self.preds)#, multi_class='ovr')\n",
    "#         self.log('val_auc_epoch', auc)\n",
    "        self.log('val_acc_epoch', self.accuracy.compute(), prog_bar=True)\n",
    "        self.log('val_auc_epoch', auc, prog_bar=True)\n",
    "\n",
    "        self.preds = []\n",
    "        self.truths = []\n",
    "    \n",
    "class BioNLPDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, label_idx=9, insize=2000):\n",
    "        super(BioNLPDataModule, self).__init__()\n",
    "        self.label_idx = label_idx\n",
    "        self.insize = insize\n",
    "      \n",
    "    def setup(self, stage):\n",
    "    # transforms for images\n",
    "#         transform=transforms.Compose([transforms.ToTensor(), \n",
    "#                                       transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        global training\n",
    "        global testing\n",
    "        global BATCH_SIZE\n",
    "        # prepare transforms standard to MNIST\n",
    "        self.train =  training.reset_index(drop=True)\n",
    "        self.test =  testing.reset_index(drop=True)\n",
    "#         print(len(self.test.iloc[:,6].value_counts()))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(BioDataset(self.train, self.insize, label_idx=self.label_idx), batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(BioDataset(self.test, self.insize, label_idx=self.label_idx), batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "nhid =512 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 2\n",
    "ntokens = 32000\n",
    "\n",
    "insize = 250\n",
    "\n",
    "\n",
    "# data_module = BioNLPDataModule(5, insize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = XLNet(ntokens, nhid, nclasses, dropout, insize=insize).cuda()\n",
    "# # emb_dict = model.encoder.state_dict()['weight'].cpu().numpy()\n",
    "# # print(emb_dict.shape)\n",
    "# # new_emb = np.random.randn(1, *emb_dict.shape[1:])\n",
    "# # emb_dict = np.concatenate((emb_dict, new_emb), axis=0)\n",
    "# # print(emb_dict.shape)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "# trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import nltk.data\n",
    "digits = \"([0-9])\" \n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "# def digitrepl(matchobj):\n",
    "#     print(matchobj.group(0))\n",
    "#     if matchobj.group(0)[]\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "data = reports['TEXT'][0]\n",
    "\n",
    "# print(reports['TEXT'][0])\n",
    "split_into_sentences(reports['TEXT'][0])\n",
    "m = re.match(re.compile(r\"\\d+\"), reports['TEXT'][0])\n",
    "data = re.sub(r\"\\d+\\.\\s\",  r\"\", reports['TEXT'][0])\n",
    "data = re.sub(r\"\\n\",r\" \", data)\n",
    "print ('\\n-----\\n'.join(tokenizer.tokenize(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kang_data_cleaned.txt\", \"w\") as f:\n",
    "    for report in reports['TEXT']:\n",
    "        data = re.sub(r\"\\d+\\.\\s\",  r\"\", report)\n",
    "        data = re.sub(r\"\\n\",r\" \", data)\n",
    "\n",
    "        data = tokenizer.tokenize(data)\n",
    "        for line in data:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "####THIS IS THE ONE THAT WORKS FOR CONVERTING THE NOTES'CATEGORIES TO LABELS\n",
    "\n",
    "testnotes = notes[~notes['ISERROR'].notnull() & notes['CATEGORY'].notnull() & notes['CHARTTIME'].notnull() & notes['STORETIME'].notnull\n",
    "                  & notes['DESCRIPTION'].notnull() & notes['HADM_ID'].notnull()]\n",
    "testnotes['ISERROR'] = testnotes['ISERROR'].fillna(0)\n",
    "testnotes = testnotes[['TEXT', 'CATEGORY', 'SUBJECT_ID','HADM_ID']]\n",
    "\n",
    "#BINARY\n",
    "#classes = pd.read_csv('binary_class_labels.csv').iloc[:, 1:3]\n",
    "#FULL\n",
    "classes = pd.read_csv('class_labels.csv')\n",
    "\n",
    "classes = dict(zip(classes['Category'].values, classes['Group'].values))\n",
    "testnotes['CATEGORY'] = testnotes['CATEGORY'].apply(lambda x: classes[x])\n",
    "#14 and 3 are the same, more or less, so let's combine them\n",
    "testnotes['CATEGORY'].replace(14, 3, inplace=True)\n",
    "\n",
    "#testnotes['CATEGORY'].unique()\n",
    "testnotes['HADM_ID'] = testnotes['HADM_ID'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Combining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TOKENS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>319840</th>\n",
       "      <td>319082</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-05</td>\n",
       "      <td>2187-03-05 08:01:00</td>\n",
       "      <td>2187-03-05 08:01:31</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Admission Note</td>\n",
       "      <td>15165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:  n/v, abdominal pain, fever\\n...</td>\n",
       "      <td>[1777, 2028, 31939, 51, 31961, 31957, 31955, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319996</th>\n",
       "      <td>319164</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 05:00:00</td>\n",
       "      <td>2187-03-06 05:00:30</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>21108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fever (Hyperthermia, Pyrexia, not Fever of Unk...</td>\n",
       "      <td>[4059, 55, 14205, 31955, 9590, 31955, 180, 405...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320269</th>\n",
       "      <td>319218</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 12:36:00</td>\n",
       "      <td>2187-03-06 12:50:56</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Transfer Note</td>\n",
       "      <td>21297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59 F h/o breast ca being treated with carbopla...</td>\n",
       "      <td>[3635, 81, 64, 31961, 31922, 3252, 321, 2302, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320275</th>\n",
       "      <td>319224</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 07:54:00</td>\n",
       "      <td>2187-03-06 14:42:31</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Progress Note</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...</td>\n",
       "      <td>[1777, 2028, 31939, 464, 2343, 2075, 31939, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320276</th>\n",
       "      <td>319225</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 07:54:00</td>\n",
       "      <td>2187-03-06 14:47:22</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Progress Note</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...</td>\n",
       "      <td>[1777, 2028, 31939, 464, 2343, 2075, 31939, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082553</th>\n",
       "      <td>2076162</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-21</td>\n",
       "      <td>2165-08-21 06:59:00</td>\n",
       "      <td>2165-08-21 07:00:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>20104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNP On-Call\\nPhysical Exam\\nInfant in open cri...</td>\n",
       "      <td>[7659, 925, 31948, 16711, 1484, 3047, 1269, 48...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082816</th>\n",
       "      <td>2076153</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-18</td>\n",
       "      <td>2165-08-18 14:28:00</td>\n",
       "      <td>2165-08-18 14:33:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NPN 0700-1900\\n\\n\\n#1 RESP: In RA. RR 20-60, s...</td>\n",
       "      <td>[1845, 5768, 2659, 1772, 31939, 203, 1122, 319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082817</th>\n",
       "      <td>2076154</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 00:40:00</td>\n",
       "      <td>2165-08-19 00:42:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNP Physical Exam\\nAwake and alert swaddled in...</td>\n",
       "      <td>[7659, 1484, 3047, 7308, 52, 2018, 3905, 48, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082818</th>\n",
       "      <td>2076155</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 03:20:00</td>\n",
       "      <td>2165-08-19 03:31:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>20315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NPN 1900-0700\\n\\n\\nResp: Remains in RA. RR~20-...</td>\n",
       "      <td>[1845, 4708, 297, 31939, 2245, 48, 1122, 31935...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082950</th>\n",
       "      <td>2076156</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 13:40:00</td>\n",
       "      <td>2165-08-19 13:44:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17593.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neonatology Attending\\n\\nDOL 12 PMA 35 weeks\\n...</td>\n",
       "      <td>[2813, 2279, 5141, 313, 8196, 1438, 1938, 2091...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "319840    319082        2984  124661.0  2187-03-05  2187-03-05 08:01:00   \n",
       "319996    319164        2984  124661.0  2187-03-06  2187-03-06 05:00:00   \n",
       "320269    319218        2984  124661.0  2187-03-06  2187-03-06 12:36:00   \n",
       "320275    319224        2984  124661.0  2187-03-06  2187-03-06 07:54:00   \n",
       "320276    319225        2984  124661.0  2187-03-06  2187-03-06 07:54:00   \n",
       "...          ...         ...       ...         ...                  ...   \n",
       "2082553  2076162       31704  110591.0  2165-08-21  2165-08-21 06:59:00   \n",
       "2082816  2076153       31704  110591.0  2165-08-18  2165-08-18 14:28:00   \n",
       "2082817  2076154       31704  110591.0  2165-08-19  2165-08-19 00:40:00   \n",
       "2082818  2076155       31704  110591.0  2165-08-19  2165-08-19 03:20:00   \n",
       "2082950  2076156       31704  110591.0  2165-08-19  2165-08-19 13:40:00   \n",
       "\n",
       "                   STORETIME       CATEGORY  \\\n",
       "319840   2187-03-05 08:01:31     Physician    \n",
       "319996   2187-03-06 05:00:30        Nursing   \n",
       "320269   2187-03-06 12:50:56        Nursing   \n",
       "320275   2187-03-06 14:42:31     Physician    \n",
       "320276   2187-03-06 14:47:22     Physician    \n",
       "...                      ...            ...   \n",
       "2082553  2165-08-21 07:00:00  Nursing/other   \n",
       "2082816  2165-08-18 14:33:00  Nursing/other   \n",
       "2082817  2165-08-19 00:42:00  Nursing/other   \n",
       "2082818  2165-08-19 03:31:00  Nursing/other   \n",
       "2082950  2165-08-19 13:44:00  Nursing/other   \n",
       "\n",
       "                               DESCRIPTION     CGID  ISERROR  \\\n",
       "319840   Physician Resident Admission Note  15165.0      0.0   \n",
       "319996               Nursing Progress Note  21108.0      0.0   \n",
       "320269               Nursing Transfer Note  21297.0      0.0   \n",
       "320275    Physician Resident Progress Note  14180.0      0.0   \n",
       "320276    Physician Resident Progress Note  14180.0      0.0   \n",
       "...                                    ...      ...      ...   \n",
       "2082553                             Report  20104.0      0.0   \n",
       "2082816                             Report  17147.0      0.0   \n",
       "2082817                             Report  19157.0      0.0   \n",
       "2082818                             Report  20315.0      0.0   \n",
       "2082950                             Report  17593.0      0.0   \n",
       "\n",
       "                                                      TEXT  \\\n",
       "319840   Chief Complaint:  n/v, abdominal pain, fever\\n...   \n",
       "319996   Fever (Hyperthermia, Pyrexia, not Fever of Unk...   \n",
       "320269   59 F h/o breast ca being treated with carbopla...   \n",
       "320275   Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...   \n",
       "320276   Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...   \n",
       "...                                                    ...   \n",
       "2082553  NNP On-Call\\nPhysical Exam\\nInfant in open cri...   \n",
       "2082816  NPN 0700-1900\\n\\n\\n#1 RESP: In RA. RR 20-60, s...   \n",
       "2082817  NNP Physical Exam\\nAwake and alert swaddled in...   \n",
       "2082818  NPN 1900-0700\\n\\n\\nResp: Remains in RA. RR~20-...   \n",
       "2082950  Neonatology Attending\\n\\nDOL 12 PMA 35 weeks\\n...   \n",
       "\n",
       "                                                    TOKENS  \n",
       "319840   [1777, 2028, 31939, 51, 31961, 31957, 31955, 1...  \n",
       "319996   [4059, 55, 14205, 31955, 9590, 31955, 180, 405...  \n",
       "320269   [3635, 81, 64, 31961, 31922, 3252, 321, 2302, ...  \n",
       "320275   [1777, 2028, 31939, 464, 2343, 2075, 31939, 17...  \n",
       "320276   [1777, 2028, 31939, 464, 2343, 2075, 31939, 17...  \n",
       "...                                                    ...  \n",
       "2082553  [7659, 925, 31948, 16711, 1484, 3047, 1269, 48...  \n",
       "2082816  [1845, 5768, 2659, 1772, 31939, 203, 1122, 319...  \n",
       "2082817  [7659, 1484, 3047, 7308, 52, 2018, 3905, 48, 3...  \n",
       "2082818  [1845, 4708, 297, 31939, 2245, 48, 1122, 31935...  \n",
       "2082950  [2813, 2279, 5141, 313, 8196, 1438, 1938, 2091...  \n",
       "\n",
       "[4973 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55672    1420\n",
       "78076    1414\n",
       "77614    1351\n",
       "27427    1294\n",
       "109      1279\n",
       "         ... \n",
       "90566       1\n",
       "82956       1\n",
       "93193       1\n",
       "7119        1\n",
       "11393       1\n",
       "Name: SUBJECT_ID, Length: 46146, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notes.to_csv(\"RawNoteText.txt\", sep=\"\\n\", header=None, index=False)\n",
    "#diagnoses.loc[diagnoses['SUBJECT_ID']==diagnoses['SUBJECT_ID'][0]]\n",
    "notes['SUBJECT_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning ICD-9 codes to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6985"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ICD2binary(code):\n",
    "    '''\n",
    "    Helper function to convert ICD9 codes to binary labels based on whether or not they indicate cancer or not\n",
    "    '''\n",
    "    if(isinstance(code, str)):\n",
    "        #check for non-numeric code starts\n",
    "        if(code[0].isdigit() == False):\n",
    "            return 2\n",
    "        if(code[0]=='0'):\n",
    "            return 2\n",
    "        #only scan first 3 digits of remaining code\n",
    "        if(len(code)>3):\n",
    "            code = ast.literal_eval(code[:3])\n",
    "        else:\n",
    "            code = ast.literal_eval(code)\n",
    "        #check for non-neoplasm codes\n",
    "        if((code<140)|(code>229)):\n",
    "            \n",
    "            return 2\n",
    "        if((code>=140) & (code<=209)):\n",
    "        #return positive value for malignancies\n",
    "            return 1\n",
    "        # all else are benign \n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def patient2label(patient, df):\n",
    "    '''\n",
    "    Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "    '''\n",
    "    patient_codes = df.loc[df['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "    #print(type(patient_codes[0]))\n",
    "    cancer_codes = ['155', '1550', '1551', '1552']\n",
    "    cirrhosis_codes=['5712', '5715', '5716']\n",
    "    #0 = neither liver cancer nor cirrhosis\n",
    "    #1 = liver cancer\n",
    "    #2 = cirrhosis\n",
    "    #3 = both\n",
    "    \n",
    "    label = 0 #assume the null hypothesis is true\n",
    "    for code in patient_codes:\n",
    "        if code in cancer_codes:\n",
    "            if(label == 2):\n",
    "                return 3\n",
    "            label = 1\n",
    "        elif(code in cirrhosis_codes):\n",
    "            if(label == 1):\n",
    "                return 3\n",
    "            label = 2\n",
    "    return label\n",
    "\n",
    "string = '2172-03-14 11:00:00'\n",
    "def date2float(date):\n",
    "    #print(type(date))\n",
    "    return datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "def sortNotes(patient, df):\n",
    "    '''A helper function which marks codes each patient's ICD codes in sequence'''\n",
    "    patientdata = df.loc[df['SUBJECT_ID']==patient]\n",
    "    patientdata['time'] = patientdata['CHARTTIME'].apply(lambda x: date2float(x))\n",
    "    patientdata.sort_values('time', inplace=True)\n",
    "    return patientdata\n",
    "    #for note in patientdata['']\n",
    "    #patientdata['RAW_TIME'] = patientdata[] \n",
    "    #first organize patient codes by \n",
    "\n",
    "def diagnoses2vector(patient, df):\n",
    "    diagnoses = pd.Series(df['ICD9_CODE'].unique())\n",
    "    patientcodes = df[df['SUBJECT_ID']==patient]['ICD9_CODE'].unique()\n",
    "    patientvector = diagnoses.apply(lambda x: 1 if x in patientcodes else 0)\n",
    "    return patientvector\n",
    "\n",
    "patient = diagnoses['SUBJECT_ID'][0]\n",
    "print(diagnoses2vector(patient, diagnoses).sum())\n",
    "len(diagnoses[diagnoses['SUBJECT_ID']==patient]['ICD9_CODE'].unique())\n",
    "len(diagnoses['ICD9_CODE'].unique())\n",
    "# df = pd.DataFrame(data={'ICD9_CODE':diagnoses['ICD9_CODE'].unique(), 'VALUE':np.zeros(len(diagnoses['ICD9_CODE'].unique()))})\n",
    "# testcodes = ['40301', '486', '58281']\n",
    "\n",
    "# df['VALUE'] = df['ICD9_CODE'].apply(lambda x: 1 if str(x) in testcodes else 0)\n",
    "# df\n",
    "#diagnoses['ICD9_CODE'].apply(lambda x:1 if str(x) in testcodes else 0).value_counts()\n",
    "#str(diagnoses['ICD9_CODE'][1]) in testcodes\n",
    "#date2float('2187-03-05 08:01:00')\n",
    "    \n",
    "#neoplasm = pd.read_csv(r'MIMIC_neoplasm_timestamped.csv')\n",
    "#patients = diagnoses['SUBJECT_ID'].unique()\n",
    "#print(patients)\n",
    "# liver_labels =[patient2label(x, diagnoses) for x in patients]\n",
    "# patient_liver_labels = pd.DataFrame(data={\"SUBJECT_ID\":patients, \"LABEL\":liver_labels})\n",
    "# patient_liver_labels.to_csv(r'MIMIC_liver.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testnotes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-40726722d95c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#testnotes['LABEL'] = testnotes['ICD9_CODE'].apply(lambda x: ICD2binary(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtestnotes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestnotes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestnotes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LABEL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testnotes' is not defined"
     ]
    }
   ],
   "source": [
    "#testnotes['LABEL'] = testnotes['ICD9_CODE'].apply(lambda x: ICD2binary(x))\n",
    "#testnotes.head()\n",
    "subset = testnotes.loc[testnotes['LABEL']!=2]\n",
    "sample = subset.sample(frac=1, random_state=0)\n",
    "print(len(sample))\n",
    "sample.groupby('LABEL').count()['TEXT']/len(sample) * 100\n",
    "#sample.to_csv('MIMIC_neoplasms.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "#reloading previously saved .csv\n",
    "#neoplasm\n",
    "#sample = pd.read_csv('MIMIC_neoplasm_timestamped.csv', index_col=1)\n",
    "#prev = pd.read_csv('MIMIC_neoplasms.csv', index_col=0)\n",
    "# sample = sample[sample['CATEGORY']!=13]\n",
    "# sample.head()\n",
    "# sample = sample.sample(frac=1, random_state=0)\n",
    "# sample['LABEL'].value_counts()\n",
    "# test = list(sample['TEXT'].iloc[0])\n",
    "\n",
    "# sample[sample['ICD9_CODE']==1749]\n",
    "# sample\n",
    "\n",
    "#liver\n",
    "sample = pd.read_csv('MIMIC_liver.csv')\n",
    "positives = sample[sample['LABEL']!= 0]\n",
    "#positives.to_csv(r'MIMIC_liver_positives.csv', index=False)\n",
    "print(len(positives))\n",
    "negatives = sample[sample['LABEL']==0].sample(int(len(positives)/3))\n",
    "patients = pd.concat([positives, negatives]).reset_index()\n",
    "#sample.to_csv(r'MIMIC_liver_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2115"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO: Figure out how to properly split the data this time\n",
    "\n",
    "# subset = sample[sample['CHARTTIME'].notnull()]\n",
    "# patient_counts = pd.Series(subset['SUBJECT_ID'].value_counts())\n",
    "# thresholds = patient_counts[(patient_counts>20)]\n",
    "# thresholds = thresholds[thresholds <=50]#.sample(frac=.03)\n",
    "# #print(thresholds.sort_values(ascending=True))\n",
    "# subset = subset[subset['SUBJECT_ID'].isin(thresholds.index.values)]\n",
    "# print(len(thresholds))\n",
    "# print(len(subset))\n",
    "# subset['TOKENS'] = subset['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# train_data, test_data = train_test_split(thresholds.index.values, test_size=0.20, random_state=1)\n",
    "#split by the patients, then add only the subset of notes which are in train data and test data to each respectively\n",
    "#this will allow all notes in the train data to be only those which correspond to the patients\n",
    "#train_data = subset[subset['SUBJECT_ID'].isin(train_data)]\n",
    "#test_data = subset[subset['SUBJECT_ID'].isin(test_data)]\n",
    "# print(len(train_data))\n",
    "\n",
    "\n",
    "sentences = notes[notes['SUBJECT_ID'].isin(sample['SUBJECT_ID'])]\n",
    "#sentences = pd.read_csv('MIMIC_liver_tokenized.csv.gz', compression='gzip', index_col=0)\n",
    "#sentences['SUBJECT_ID'].value_counts()\n",
    "\n",
    "patient_counts = pd.Series(sentences['SUBJECT_ID'].value_counts())\n",
    "#split by the patients, then add only the subset of notes which are in train data and test data to each respectively\n",
    "#this will allow all notes in the train data to be only those which correspond to the patients\n",
    "#sentences['TOKENS'] = sentences['TEXT'].apply(lambda x:(sentence_to_padded_index_sequence(x, sp)))\n",
    "\n",
    "train_data, test_data = train_test_split(patient_counts.index.values, test_size=0.20, random_state=1)\n",
    "len(train_data)# train_data\n",
    "#train_X, train_y = train_data['TEXT'], train_data['LABEL']\n",
    "#test_X, test_y = test_data['TEXT'], test_data['LABEL']\n",
    "\n",
    "#test_data.groupby('LABEL').count()['TEXT']/len(test_data) * 100\n",
    "#print(test_data['LABEL'].value_counts())\n",
    "#train_data['LABEL'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-906d78223e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('MIMIC_liver.csv', index_col=0)\n",
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "#     for s in sentences:\n",
    "#         word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", s.lower()))\n",
    "    sentences.apply(lambda x:  word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", x.lower())))\n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n",
    "    \n",
    "vocabulary, vocab_indices = build_vocab(train_X)\n",
    "print((vocabulary)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155102it [00:02, 65535.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# load embedding\n",
    "emb_dim = 50\n",
    "with open('vectors.txt') as f:\n",
    "    glove_embedding = []\n",
    "    words = {}\n",
    "    chars = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        s = line.split()\n",
    "        glove_embedding.append(np.asarray(s[1:]))\n",
    "        \n",
    "        words[s[0]] = len(words)\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])\n",
    "        \n",
    "# add unknown to word and char\n",
    "glove_embedding.append(np.random.rand(emb_dim))\n",
    "words[\"<UNK>\"] = len(words)\n",
    "\n",
    "# add padding\n",
    "glove_embedding.append(np.zeros(emb_dim))\n",
    "words[\"<PAD>\"] = len(words)\n",
    "\n",
    "chars[\"<UNK>\"] = len(chars)\n",
    "chars[\"<PAD>\"] = len(chars)\n",
    "\n",
    "glove_embedding = np.array(glove_embedding).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_padded_index_sequences(words, sentences, pad_length=100):\n",
    "    padded_sequences = np.zeros((len(sentences), pad_length))\n",
    "    for i, s in enumerate(sentences):\n",
    "        indices = np.ones(pad_length) * words['<PAD>']\n",
    "        # only take the first pad_length tokens\n",
    "        token_indices = np.array([words[w] if w in words else words['<UNK>'] for w in re.findall(r\"[\\w']+|[.,!?;]\", s.lower())[:pad_length]])\n",
    "        indices[:len(token_indices)] = token_indices\n",
    "        padded_sequences[i] = indices\n",
    "    return padded_sequences\n",
    "\n",
    "def sentence_to_padded_index_sequence(sentence, sp, pad_length = 100, pad_token=0):\n",
    "    sequence = sp.EncodeAsIds(sentence)\n",
    "    if(len(sequence)>pad_length):\n",
    "        sequence = sequence[:pad_length]\n",
    "    else:\n",
    "        while len(sequence) <pad_length:\n",
    "            sequence.append(pad_token)\n",
    "    return sequence\n",
    "\n",
    "#print(np.zeros((100, 2)))\n",
    "    \n",
    "#ids = sample['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "#print(sp.DecodeIds([0]))\n",
    "#test = pd.read_csv(\"vectors.txt\", sep = \" \", header=None, index_col = 0)\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-c126f7c660bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# len(s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msentence_to_padded_index_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# for i in range(len(train_X)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "#old version of sentences to IDs\n",
    "#train_X = sentences_to_padded_index_sequences(words, train_data['TEXT'], 100)\n",
    "\n",
    "#test_X = sentences_to_padded_index_sequences(words, test_data['TEXT'], 100)\n",
    "# s = []\n",
    "\n",
    "# print(train_X[0])\n",
    "# print(words[\"<UNK>\"])\n",
    "# print(\"<UNK>\" in idx2words.keys())\n",
    "# print(idx2words[int(train_X[0][-1])])\n",
    "# # for word in train_X[0]:\n",
    "#     if word in idx2words:\n",
    "#         s.append(idx2words[int(word)])\n",
    "# print(s)\n",
    "# len(s)\n",
    "\n",
    "train_X = train_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "# for i in range(len(train_X)):\n",
    "#     if(len(train_X.iloc[i]) != 500):\n",
    "#         print(\"bad sequence detected at {}\".format(train_X.ilove[i]))\n",
    "test_X = test_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "print(type(train_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.78 GiB (GPU 0; 8.00 GiB total capacity; 722.70 MiB already allocated; 5.00 GiB free; 235.30 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-a3c4d1efe7c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0my_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-211-b8a62eee33ba>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m#print(\"transformer encoder: {}\".format(output.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m#print(\"decoder: {}\".format(output.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.78 GiB (GPU 0; 8.00 GiB total capacity; 722.70 MiB already allocated; 5.00 GiB free; 235.30 MiB cached)"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "len(train_X)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i] != truths[i]):\n",
    "        if(predictions ==0):\n",
    "            fn+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    else:\n",
    "        if(predictions == 0):\n",
    "            tn +=1\n",
    "        else:\n",
    "            tp +=1\n",
    "tpr = (tp /(tp + fn))\n",
    "\n",
    "fpr = (fp/(tn + fp))\n",
    "\n",
    "y_true = []\n",
    "y_score =[]  \n",
    "for batch in test_loader:\n",
    "    for j in batch[1]:\n",
    "        y_true.append(j.item())\n",
    "    for j in batch[0]:\n",
    "        y_score.append(model(j).max(1)[1].item())\n",
    "\n",
    "print(y_true)\n",
    "            \n",
    "print(\"True Positives: {} \\n True Negatives: {} \\n False Positive: {} \\n False Negatives:{}\".format(tp, tn, fp, fn))\n",
    "print('AUC: {}'.format(metrics.roc_auc_score(y_true, y_score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395061728395061% accurate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.801790450928382"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('AUC: {}'.format(metrics.auc(fpr, tpr)))\n",
    "from sklearn import metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i]== truths[i]):\n",
    "        correct +=1\n",
    "    total+=1\n",
    "print(\"{}\".format(correct/total) + \"% accurate\")\n",
    "fpr, tpr, thresholds = metrics.roc_curve(truths, predictions)\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDatasetNeoplasm(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences.values.tolist()#.astype(int)\n",
    "        self.labels = labels.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        #print(self.sentences)\n",
    "        #print(np.array(self.sentences.iloc[key]))\n",
    "        return [np.array(self.sentences[key]), np.array(self.labels[key])]\n",
    "\n",
    "class MIMICDatasetMultiLabel(Dataset):\n",
    "    def __init__(self, sentences, sp, diagnoses, maxlength=17):\n",
    "        #print(\"init\")\n",
    "        #sentences['TOKENS'] = sentences['TEXT'].apply(lambda x:self.sentence_to_padded_index_sequence(x, sp))\n",
    "        sentences['TIME'] = sentences['CHARTTIME'].apply(lambda x:self.date2float(x))\n",
    "        #print(sentences['HADM_ID'].iloc[0])\n",
    "        #print(\"sentence time calculated\")\n",
    "        diagnoses = diagnoses[diagnoses['ICD9_CODE'].notnull()]\n",
    "        self.all_diagnoses = pd.Series(diagnoses['ICD9_CODE'].unique())\n",
    "\n",
    "        diagnoses = diagnoses[diagnoses['HADM_ID'].isin(sentences['HADM_ID'])]\n",
    "       # print(\"about to apply diagnoses, length: {}\".format(len(diagnoses)))\n",
    "        self.sentences = sentences[['SUBJECT_ID','HADM_ID', 'TIME', 'TOKENS']] #.values.tolist()#.astype(int)\n",
    "\n",
    "        diagnoses['TIME'] = diagnoses['HADM_ID'].apply(lambda x:self.timestampDiagnoses(x))\n",
    "        #print(\"diagnoses time calculated\")\n",
    "        #print(\"diagnosis check: {}\".format(diagnoses['TIME'].iloc[0]))\n",
    "        #print(diagnoses)\n",
    "\n",
    "        self.diagnoses = diagnoses.sort_values(by=['TIME'])\n",
    "        #print(len(self.all_diagnoses))\n",
    "        self.patients = pd.Series(self.sentences['SUBJECT_ID'].unique())#pd.Series(diagnoses[diagnoses['SEQ_NUM']==maxlength]['SUBJECT_ID'].unique()) ###this should be done before passing in the notes, but just in case it isn't this will pare it down\n",
    "        #self.labels = labels.values\n",
    "        self.maxlength = maxlength\n",
    "        #print(\"ended\")\n",
    "\n",
    "    \n",
    "    def sentence_to_padded_index_sequence(self, sentence, sp, pad_length = 100, pad_token=0):\n",
    "        sequence = sp.EncodeAsIds(sentence)\n",
    "        if(len(sequence)>pad_length):\n",
    "            sequence = sequence[:pad_length]\n",
    "        else:\n",
    "            while len(sequence) <pad_length:\n",
    "                sequence.append(pad_token)\n",
    "        return sequence\n",
    "    \n",
    "    def timestampDiagnoses(self, admission):\n",
    "        #if(len(self.sentences[self.sentences['HADM_ID']==admission]['TIME']) >1):\n",
    "            #print(admission, self.sentences[self.sentences['HADM_ID']==admission]['TIME'])\n",
    "        return self.sentences[self.sentences['HADM_ID']==admission]['TIME'].values[0]\n",
    "    def diagnoses2vector(self, patient):\n",
    "        patientcodes = self.diagnoses[self.diagnoses['SUBJECT_ID']==patient]['ICD9_CODE'].unique()\n",
    "        patientvector = self.all_diagnoses.apply(lambda x: 1 if x in patientcodes else 0)\n",
    "        return patientvector\n",
    "\n",
    "\n",
    "    def patient2label(self, patient):\n",
    "        '''\n",
    "        Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "        '''\n",
    "        patient_codes = self.diagnoses.loc[self.diagnoses['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "        #print(type(patient_codes[0]))\n",
    "        cancer_codes = ['155', '1550', '1551', '1552']\n",
    "        cirrhosis_codes=['5712', '5715', '5716']\n",
    "        #0 = neither liver cancer nor cirrhosis\n",
    "        #1 = liver cancer\n",
    "        #2 = cirrhosis\n",
    "        #3 = both\n",
    "\n",
    "        label = 0 #assume the null hypothesis is true\n",
    "        for code in patient_codes:\n",
    "            if code in cancer_codes:\n",
    "                if(label == 2):\n",
    "                    return 3\n",
    "                label = 1\n",
    "            elif(code in cirrhosis_codes):\n",
    "                if(label == 1):\n",
    "                    return 3\n",
    "                label = 2\n",
    "        return label\n",
    "\n",
    "    def date2float(self, date):\n",
    "    #print(type(date))\n",
    "        \n",
    "    \n",
    "        return datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "    def sortNotes(self, patient):\n",
    "        '''A helper function which marks codes each patient's ICD codes in sequence'''\n",
    "        patientdata = self.sentences.loc[self.sentences['SUBJECT_ID']==patient]\n",
    "  #      patientdata['time'] = patientdata['CHARTTIME'].apply(lambda x: date2float(x))\n",
    "        patientdata.sort_values('TIME', inplace=True)\n",
    "        return patientdata['TOKENS'].values.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        #print(self.sentences)\n",
    "        #print(np.array(self.sentences.iloc[key]))\n",
    "        patient = self.patients[key]\n",
    "        \n",
    "        data = self.sortNotes(patient)\n",
    "        x = random.sample(data, 5) #self.maxlength]\n",
    "        x = [j for i in x for j in i]\n",
    "        #print(\"x type: {}\".format(x))\n",
    "        #y = self.diagnoses[self.diagnoses['SUBJECT_ID']==key]['ICD9_CODE']\n",
    "        y = self.diagnoses2vector(patient)\n",
    "        \n",
    "#        print(\"y type: {}\".format(y.dtypes))\n",
    "        #print(x.values)\n",
    "        return [np.array(x), np.array(y)]\n",
    "class BioDataset(Dataset):\n",
    "    def __init__(self, data, pad_length=500, label_idx=9):\n",
    "#         print((data))\n",
    "        self.sentences = data['TEXT'].values.tolist()#.astype(int)\n",
    "        self.labels = data.iloc[:,label_idx].values.tolist()\n",
    "        global sp\n",
    "        self.sp = sp\n",
    "        self.pad_length =pad_length\n",
    "        self.pad_token=0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def sentence_to_padded_index_sequence(self, sentence):\n",
    "        #print(type(sentence))\n",
    "        sequence = self.sp.EncodeAsIds(sentence)\n",
    "        if(len(sequence)>self.pad_length):\n",
    "            sequence = sequence[:self.pad_length]\n",
    "        else:\n",
    "            sequence += [self.pad_token] * (self.pad_length-len(sequence))\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        #print(self.sentences)\n",
    "        x = np.array(self.sentence_to_padded_index_sequence(self.sentences[key]))\n",
    "        if(np.isnan(x).any()):\n",
    "            print(key)\n",
    "        y =  np.array(self.labels[key])\n",
    "        return [x, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 434\n",
      "torch.Size([44, 1000])\n",
      "torch.Size([44])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0])\n",
      "<class 'int'>\n",
      "tensor([[ 2618,  1298, 31941,  ..., 31936,  1276, 31941],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5918, 11697,    25,  ...,     0,     0,     0],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        [ 7183, 31963,  3918,  ...,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "#BATCH_SIZE =1\n",
    "#TODO: fix to make consistent size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, data, sp, label_column = \"Cirrhosis\"):\n",
    "        super(MIMICDataset, self).__init__()\n",
    "        self.data = data[['TEXT']]\n",
    "        self.labels = data[label_column]\n",
    "        self.sp = sp\n",
    "        #self.sentences = sentences[['SUBJECT_ID', 'TOKENS']]#.astype(int)\n",
    "        #self.labels = patients['LABEL']\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "    \n",
    "    def patient2label(self, patient, df, mode=\"liver\"):\n",
    "        '''\n",
    "        Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "        '''\n",
    "        if(mode==\"liver\" or mode == None):\n",
    "            patient_codes = df.loc[df['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "            #print(type(patient_codes[0]))\n",
    "            cancer_codes = ['155', '1550', '1551', '1552']\n",
    "            cirrhosis_codes=['5712', '5715', '5716']\n",
    "            #0 = neither liver cancer nor cirrhosis\n",
    "            #1 = liver cancer\n",
    "            #2 = cirrhosis\n",
    "            #3 = both\n",
    "\n",
    "            label = 0 #assume the null hypothesis is true\n",
    "            for code in patient_codes:\n",
    "                if code in cancer_codes:\n",
    "                    if(label == 2):\n",
    "                        return 3\n",
    "                    label = 1\n",
    "                elif(code in cirrhosis_codes):\n",
    "                    if(label == 1):\n",
    "                        return 3\n",
    "                    label = 2\n",
    "        elif(mode==\"sex\"):\n",
    "            label = 0 if df.loc[df['SUBJECT_ID']==patient]['GENDER'][0]=='F' else 1\n",
    "        return label\n",
    "\n",
    "    def sentence_to_padded_index_sequence(self, sentence, pad_length = 100, pad_token=0):\n",
    "        if(type(sentence) == type(\"string\")):\n",
    "            sentence = self.sp.EncodeAsIds(sentence)\n",
    "        sequence = np.array(sentence)\n",
    "        if(len(sequence)>pad_length):\n",
    "            #print(\"truncating\")\n",
    "            sequence = sequence[:pad_length]\n",
    "        else:\n",
    "            #print(\"sequence before\" + str(sequence.shape))\n",
    "\n",
    "            #print(\"not truncating\")\n",
    "            padding = np.full_like(np.zeros(pad_length - len(sequence), dtype=np.int32), pad_token)\n",
    "            #print(padding.shape)\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "        #print(type(sequence))\n",
    "#             if(pad_token == 0):\n",
    "#                 sequence = np.append(x, np.zeros((pad_length - len(x)),dtype=np.int32))\n",
    "#             else:\n",
    "#                 while len(sequence) <pad_length:\n",
    "#                     sequence.append(pad_token)\n",
    "            #print(\"sequence after: \" + str(sequence.shape))\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        patient = self.patients['SUBJECT_ID'][key]\n",
    "        data = self.sentences[self.sentences['SUBJECT_ID']== patient]['TEXT'].apply(\n",
    "            lambda x:self.sentence_to_padded_index_sequence(x, pad_length = 100)).values\n",
    "        x = np.concatenate(data).tolist()\n",
    "#         x = np.array(x)\n",
    "#         x = x.repeat(int((98600-(x.shape[0]%98600))/x.shape[0]) + 1, 0)\n",
    "        #print(x.shape)\n",
    "        x = self.sentence_to_padded_index_sequence(x, pad_length = 100 * self.docNum)\n",
    "        if(x.dtype != np.int32):\n",
    "            x=x.astype(np.int32)\n",
    "        #y = np.array(self.patients[self.patients['SUBJECT_ID']==patient]['LABEL'])\n",
    "        y = int(self.patients[(self.patients['SUBJECT_ID']==patient)]['GENDER']=='F')\n",
    "        #y = np.array(y)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "#print(sample['SUBJECT_ID'].value_counts()[sample['SUBJECT_ID'].value_counts() == 5])\n",
    "        #return (np.array(self.sentences.iloc[key]), np.array(self.labels.iloc[key]))\n",
    "#model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout)\n",
    "#model = model.cuda()\n",
    "#\n",
    "#print(labels['LABEL'][1140])\n",
    "#model.train()\n",
    "##general baseline portion\n",
    "#print(notes['SUBJECT_ID'].value_counts()[notes['SUBJECT_ID'].value_counts()==5])\n",
    "#patient = patients\n",
    "#liver = pd.read_csv(\"MIMIC_liver.csv\")\n",
    "#patients = liver\n",
    "patient_doc = pd.read_csv(\"mimic/PATIENTS.csv.gz\")\n",
    "docNum = 10\n",
    "subsample = patients.loc[(patients['SUBJECT_ID'].isin(notes['SUBJECT_ID'].value_counts()[notes['SUBJECT_ID'].value_counts()<=docNum].index.values))]\n",
    "subsample = patient_doc.loc[patient_doc['SUBJECT_ID'].isin(subsample['SUBJECT_ID'].values)]\n",
    "n = 20000\n",
    "n = n if n < len(subsample) else len(subsample)\n",
    "print(\"n: {}\".format(n))\n",
    "#print(len(subsample)) q\n",
    "sample = subsample.sample(frac=n/len(subsample))[['SUBJECT_ID', 'GENDER']]\n",
    "sentences = notes[notes['SUBJECT_ID'].isin(sample['SUBJECT_ID'].values)]\n",
    "\n",
    "##neoplasm portion\n",
    "#neoplasm = pd.read_csv(\"MIMIC_neoplasmys.csv\", index_col = 0)\n",
    "#sample = neoplasm[['TEXT', 'LABEL']]\n",
    "train_data, test_data = train_test_split(sample, train_size=.9, random_state=0)\n",
    "# train_X = train_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# train_y = train_data['LABEL']\n",
    "# test_X = test_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# test_y = test_data['LABEL']\n",
    "\n",
    "train_loader = DataLoader(MIMICDataset(train_data, sentences, sp, docNum=docNum), #, labels),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "test_loader = DataLoader(MIMICDataset(test_data, sentences, sp, docNum=docNum), #,labels ),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "torch.manual_seed(111)\n",
    "\n",
    "#print(x.shape)\n",
    "#print(train_loader.dataset.labels[1934])\n",
    "i=0\n",
    "for batch in test_loader:\n",
    "\n",
    "    #print(batch)\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    #model(x)\n",
    "    #model.zero_grad()\n",
    "    break\n",
    "\n",
    "    if(x.dtype != torch.int32 or y.dtype == torch.int32):\n",
    "        print(x)\n",
    "        break\n",
    "    \n",
    "    #print(i)\n",
    "    i+=1\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "xflat = x.flatten()\n",
    "xlist=[]\n",
    "for i in xflat.cpu().numpy():\n",
    "    xlist.append(i.item())\n",
    "print(type(xlist[0]))\n",
    "print(x[:, -(98600-97200):])\n",
    "#print(sp.DecodeIds(xlist))\n",
    "#print(y)\n",
    "#print(len(glove_embedding))\n",
    "\n",
    "\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "nclasses = 2\n",
    "\n",
    "#out = model(x)\n",
    "#print(out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "BATCH_SIZE = 16\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=0\n",
    "# training, testing = train_test_split(neoplasm, train_size=0.8, random_state=3)\n",
    "import torchmetrics as tm\n",
    "import warnings\n",
    "#warnings.filterwarnings('error')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(dim1, dim2), nn.LeakyReLU())\n",
    "    def forward(self, x):\n",
    "        return einops.rearrange(self.net(x[0]), 'b s e -> b (s e)')\n",
    "\n",
    "class XLNet(nn.Sequential):\n",
    "    def __init__(self, ntokens = 32000, nhid=1024, nclasses=2, dropout=0.5, insize = 100):\n",
    "        # super(XLNet, self).__init__()\n",
    "        net = []\n",
    "        xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "        xlnet.train()\n",
    "        net.append(xlnet)\n",
    "\n",
    "        net.append(DecoderBlock(768, 64))\n",
    "        net.append(nn.Dropout(dropout))\n",
    "        net.append(nn.Linear(64 * insize, 64))\n",
    "        net.append(nn.LeakyReLU())\n",
    "        net.append(nn.Linear(64, nclasses))\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        super(XLNet, self).__init__(*net)\n",
    "\n",
    "\n",
    "\n",
    "class BioDataset(Dataset):\n",
    "    def __init__(self, data, pad_length = 4510, pad_token=5, label_idx=9, text_column = 'TEXT'):\n",
    "        super(BioDataset, self).__init__()\n",
    "        data = data.reset_index(drop=True)\n",
    "        self.sentences = data[text_column].values.tolist()#.astype(int)\n",
    "        self.labels = data.iloc[:,label_idx]\n",
    "        if(\"Nutrition\" in self.labels.values):\n",
    "\n",
    "            self.uniques = dict(zip(self.labels.unique(), np.arange(len(self.labels.unique()))))\n",
    "            print(len(self.uniques))\n",
    "        # global sp\n",
    "        # self.sp = sp\n",
    "        self.sp= XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "        self.pad_length = pad_length\n",
    "        self.pad_token= pad_token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def sentence_to_padded_index_sequence(self, sentence):\n",
    "        #print(type(sentence))\n",
    "        sequence = self.sp.encode(sentence)\n",
    "        if(len(sequence)>self.pad_length):\n",
    "            sequence = sequence[:self.pad_length]\n",
    "        else:\n",
    "            sequence += [self.pad_token] * (self.pad_length-len(sequence))\n",
    "        # print(self.sp.DecodeIds(sequence))\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # print(self.sentences[key])\n",
    "        x = np.array(self.sentence_to_padded_index_sequence(self.sentences[key]))\n",
    "       \n",
    "#         print(x.shape)\n",
    "        if(np.isnan(x).any()):\n",
    "            print(key)\n",
    "        if(\"Nutrition\" in self.labels.values):\n",
    " \n",
    "            y = self.uniques[self.labels[key]]\n",
    "        else:\n",
    "#             print(\"binary case\")\n",
    "            y =  np.array(self.labels[key])\n",
    "#         print(y.shape)\n",
    "        return [x, y]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d718f0ae86a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;31m#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "#x = torch.Tensor(32, 100).normal_() * 155103/3\n",
    "#x = x.long()\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"A single attention head\"\"\"\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    " \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The full multihead attention block\"\"\"\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    " \n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "     \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "         \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM', k=2):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.conv = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=k)\n",
    "        self.pool = nn.MaxPool1d(1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN', 'ATTN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        #self.attn = \n",
    "        self.linear = nn.Linear(hidden_dim, 1000)\n",
    "        self.fc = nn.Linear(1000, output_dim)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state values\n",
    "        \"\"\"\n",
    "        hidden = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        #print(hidden)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            c = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "            \n",
    "            return hidden, c\n",
    "        #if(self.rnn_fn =='ATTN')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #if([x.shape[0], x.shape[1]] != [32, 100]):\n",
    "            #print(x.shape)\n",
    "        x = x.to(device)\n",
    "\n",
    "        #print(x.shape, \" input\")\n",
    "        x = self.emb(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(x.shape, \" permute 1\")\n",
    "        #x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        #print(x.shape, \" conv\")\n",
    "\n",
    "        #x = self.pool(x)\n",
    "        #print(x.shape, \" pool\")\n",
    "\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        #x = x.squeeze()\n",
    "        #print(x.shape, \" permute 2\") \n",
    "\n",
    "\n",
    "       # print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1, self.embedding_dim)\n",
    "        #self.hidden_dim = 50\n",
    "        #self.rnn.hidden_dim = self.hidden_dim\n",
    "        #self.rnn.input_dim = self.hidden_dim\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(x.shape, \" embedding\")\n",
    "        _, last_hidden = self.rnn(x, self.init_hidden(x.shape[0]))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            last_hidden = last_hidden[0]\n",
    "       # print(last_hidden.shape, \" memory\")\n",
    "        last_hidden = self.linear(last_hidden)\n",
    "        last_hidden = self.relu(last_hidden)\n",
    "        out = self.fc(last_hidden)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if(len(out.shape)==1):\n",
    "            out = out.unsqueeze()\n",
    "\n",
    "       # print(out.shape, \" output\")\n",
    "        return out\n",
    "    \n",
    "#print(x)\n",
    "model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#print(model(x))\n",
    "# torch.Size([32, 100])  input\n",
    "# torch.Size([32, 100, 50])  embedding\n",
    "# torch.Size([1, 32, 40])  memory\n",
    "# torch.Size([1, 32, 2])  output\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cecd6bc72d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNNEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "#### DUPLICATE WITHOUT CNN\n",
    "### RETOOLED FOR ATTENTION\n",
    "# x = torch.Tensor(32, 100).normal_() * 155103/3\n",
    "# x = x.long()\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM'):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.e,ed\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN', 'ATTN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        #self.rnn = nnnlp.Attention()\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state values\n",
    "        \"\"\"\n",
    "        hidden = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim)).to(device)\n",
    "        #print(hidden)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            c = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim)).to(device)\n",
    "            \n",
    "            return hidden, c\n",
    "        return hidden\n",
    "        #if(self.rnn_fn =='ATTN')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #if([x.shape[0], x.shape[1]] != [32, 100]):\n",
    "            #print(x.shape)\n",
    "        x = x.cuda().long()\n",
    "\n",
    "        #print(x.shape, \" input\")\n",
    "        x = self.emb(x)\n",
    "        #print(x.shape, \" emb\")\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x= self.conv(x)\n",
    "        #x = self.pool(x)\n",
    "        #x= self.relu(x)\n",
    "        #x = x.squeeze()\n",
    "        #print(x.shape)\n",
    "        \n",
    "\n",
    "       # print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1, 32)\n",
    "        #self.hidden_dim = 32\n",
    "        #self.rnn.hidden_dim = self.hidden_dim\n",
    "        #self.rnn.input_dim = self.hidden_dim\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(x.shape, \" embedding\")\n",
    "        out, last_hidden = self.rnn(x, self.init_hidden(x.shape[0]))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            last_hidden = last_hidden[0]\n",
    "        #print(last_hidden.shape, \" memory\")\n",
    "        if(len(last_hidden.shape)>2):\n",
    "            last_hidden = last_hidden.squeeze(0)\n",
    "        \n",
    "    \n",
    "        out = self.fc(last_hidden)\n",
    "        #print(out.shape, \" output\")\n",
    "        return out#, last_hidden\n",
    "    \n",
    "#print(x)\n",
    "model = RNNEncoder(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#print(model(x))\n",
    "# torch.Size([32, 100])  input\n",
    "# torch.Size([32, 100, 50])  embedding\n",
    "# torch.Size([1, 32, 40])  memory\n",
    "# torch.Size([1, 32, 2])  output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test = 0\n",
    "def train(model, train_loader, test_loader, \n",
    "          learning_rate=0.005, num_epoch=10):\n",
    "    print_every = int(len(train_loader)/5)\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    best_auc = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    #loss_fn2 = nn.CrossEntropyLoss()\n",
    "    acc_metric = tm.Accuracy()\n",
    "    auc_metric = tm.AUROC(num_classes=2)\n",
    "    accs =[]\n",
    "    aucs = []\n",
    "    losses = []\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    print(device)\n",
    "    best_model_epoch = 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print('Beginning training for {} epochs'.format(num_epoch))\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "\n",
    "            labels = labels.to(device).long()\n",
    "            data = data.long().to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(data).squeeze()\n",
    "            loss = loss_fn(output.float(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d}/{} | {:6d}/{:6d} batches | Loss: {:6.4f} |Elapsed Time: {:>9}'.format(\n",
    "                    epoch + 1, num_epoch, i + 1, len(train_loader), loss.item(), time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))))     \n",
    "#                 print('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4}, Validation Acc:{5}, AUC:{6}'.format(\n",
    "#                     epoch + 1, EPOCHS, i + 1, len(train_loader), loss.data[0], test_acc, test_auc))\n",
    "        \n",
    "    # Evaluate after every epoch\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        full_output = []\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, labels) in enumerate(test_loader):\n",
    "                #print(data.shape)\n",
    "                data = data.cuda()\n",
    "                truths.append(labels.squeeze(0).long().cpu())\n",
    "                labels = labels.squeeze(0).cuda().long()\n",
    "                #random.sample(list(train_data), int(len(train_data)/400))\n",
    "                output = softmax(model(data))\n",
    "                full_output.append(output.squeeze(0).cpu())\n",
    "\n",
    "                acc = acc_metric(softmax(output).cpu(), labels.cpu())\n",
    "                \n",
    "                # auc = auc_metric(softmax(output).cpu(), labels.cpu())\n",
    "                pred = torch.round(output)\n",
    "                #print(pred.shape)\n",
    "# #                 full_output += list(output.cpu().numpy())\n",
    "#                 predictions += list(pred.cpu().numpy())\n",
    "#                 truths += list(labels.cpu().numpy().flatten())\n",
    "                # total += labels.size(0)\n",
    "                # correct += (pred.cpu() == labels.squeeze().long().cpu()).sum()\n",
    "#                 print(total, correct)\n",
    "#                 print(pred.cpu(), labels.long().cpu())\n",
    "                \n",
    "            acc = acc_metric.compute()\n",
    "            # print(truths[0].shape, full_output[0].shape)\n",
    "\n",
    "            auc = auc_metric(torch.cat(full_output),torch.cat(truths))\n",
    "            auc = auc_metric.compute()\n",
    "            accs.append(acc)\n",
    "            losses.append(loss)\n",
    "            ###not compatible with multiclass\n",
    "            #print(truths)\n",
    "            #output_total = np.array(output_total)\n",
    "            #print(output_total)\n",
    "            \n",
    "            #output_total = np.concatenate((output_total))\n",
    "            #print(output_total.shape)\n",
    "            #fpr, tpr, thresholds\n",
    "            #print(truths, predictions)\n",
    "#             full_output = np.concatenate(full_output).tolist()\n",
    "            #print(full_output, truths)\n",
    "            \n",
    "#             full_output = np.array(full_output)\n",
    "#             print(full_output.shape, truths)\n",
    "\n",
    "            #             full_output = np.concatenate(full_output, axis=1)\n",
    "#             print(full_output)\n",
    "#             print(full_output.shape)\n",
    "            # auc = metrics.roc_auc_score(truths, predictions)#, multi_class = \"ovo\")\n",
    "            aucs.append(auc)\n",
    "#             #print(test)\n",
    "#             #auc = metrics.auc(fpr, tpr)\n",
    "            if(auc > best_auc):\n",
    "                best_auc = auc\n",
    "                best_model_wts = model.state_dict()\n",
    "                best_model_epoch = epoch + 1 \n",
    "#             if(acc > best_acc):\n",
    "#                 best_acc = acc\n",
    "#                 best_model_wts = model.state_dict()\n",
    "\n",
    "            cm = \"\"#metrics.confusion_matrix(truths, predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "#             print('Test set | Epoch: {} | Accuracy: {:6.4f} | AUC: {:4.2f} | time elapse: {:>9}'.format(\n",
    "#                epoch + 1, acc, auc, elapse))\n",
    "            print('Test set | Epoch: {} | Accuracy: {:6.4f} | AUC: {} | Last Loss: {} | Time elapsed: {:>9} | Sample output: {}'.format(\n",
    "               epoch + 1, acc, auc, loss.item(), elapse, output))\n",
    "            #print(cm)\n",
    "            torch.save(model.state_dict(), os.path.join(os.getcwd(), 'HF-debug-test-{}'.format(epoch + 1)))\n",
    "            \n",
    "\n",
    "            #print('Test set | Epoch: {} | Accuracy: {:4.2f} | Time Elapsed: {:>9}'.format(epoch+1, \n",
    "             #   acc, elapse))\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Best model epoch: {}\".format(best_model_epoch))\n",
    "    return model, predictions, truths, accs, losses, aucs ##save this for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76327    796\n",
       "70516    772\n",
       "32425    574\n",
       "50822    448\n",
       "74701    369\n",
       "        ... \n",
       "4513       2\n",
       "5773       2\n",
       "10986      2\n",
       "21852      1\n",
       "92324      1\n",
       "Name: SUBJECT_ID, Length: 529, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes[notes['SUBJECT_ID'].isin(test_data)]['SUBJECT_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-137-5eb18229d570>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-137-5eb18229d570>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    i+=1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "d = {}\n",
    "token2word = {}\n",
    "print(sp.DecodeIds([9016]))\n",
    "for miss in misses:\n",
    "#     if(i>0):\n",
    "#         break\n",
    "\n",
    "    for token in miss[0]:\n",
    "        if(sp.DecodeIds([token.item()]) not in d.keys()):\n",
    "            d[sp.DecodeIds([token.item()])] = 1\n",
    "        else:\n",
    "            d[sp.DecodeIds([token.item()])]+=1\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of inaccurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_X = sample['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "sample_loader = DataLoader(MIMICDataset(sample_X, sample['LABEL']),\n",
    "                          batch_size=1,\n",
    "                          shuffle=True)\n",
    "\n",
    "corrects = []\n",
    "misses = []\n",
    "missed_labels = []\n",
    "correct_labels = []\n",
    "for batch in sample_loader:\n",
    "    data = batch[0]\n",
    "    label = batch[1]\n",
    "    output = model(data)\n",
    "    pred = output.max(1)[1]\n",
    "    if(pred.cpu() != label.cpu()):\n",
    "        misses.append(data)\n",
    "        missed_labels.append(label)\n",
    "    else:\n",
    "        corrects.append(data)\n",
    "        correct_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-728e8aadd972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LABEL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#df.to_csv(r'misses_2-24-20-100_epochs_32k.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecodeIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     print(sp.DecodeIds(list(misses[i].tolist()[0])))\n",
    "#     print(missed_labels[i])\n",
    "d = {}\n",
    "misses_str = []\n",
    "#print(sp.DecodeIds(misses[0].tolist()[0]))\n",
    "for i in range(len(misses)):\n",
    "    d[sp.DecodeIds(misses[i].tolist()[0])] = missed_labels[i].cpu().item()\n",
    "    misses_str.append(sp.DecodeIds(misses[i].tolist()[0]))\n",
    "df = pd.DataFrame(columns = (\"TEXT\", \"LABEL\"), data=zip(d.keys(), d.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating similarity of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "def pad(s, maxlen):\n",
    "    length = maxlen - len(s)\n",
    "    return np.concatenate([s, [0] * length], axis=None)\n",
    "\n",
    "print(len(sentences[0]))\n",
    "print(len(pad(sentences[0], maxlen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f054326383a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "misses = pd.read_csv(r'misses_2-24-20-100_epochs_32k.csv')\n",
    "#print(misses)\n",
    "sentences = misses['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "maxlen = 0\n",
    "for sentence in sentences:\n",
    "    if(len(sentence)> maxlen):\n",
    "        maxlen = len(sentence)\n",
    "#vectors = sentences.apply(lambda x:pad(x, maxlen))\n",
    "base = []\n",
    "i = 0\n",
    "for vector in vectors:\n",
    "    base.append(vector)\n",
    "    \n",
    "base = np.array(base)\n",
    "cosine = pairwise.cosine_similarity(base)\n",
    "\n",
    "\n",
    "sns.clustermap(cosine, col_cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "a = [1, 2]\n",
    "b = [[1, 2], 1]\n",
    "sample['IDs'] = ids\n",
    "sample['MISSED'] = np.zeros(len(sample))\n",
    "#ids = sample['TEXT']#.apply(lambda x: sp.EncodeAsIds(x))\n",
    "#overlap = pd.Series(list(set(sentences) & set(ids)))\n",
    "\n",
    "#for sentence in sentences.values.tolist():\n",
    "for i in range(len(sample)):\n",
    "    if(sample['IDs'].values[i] in sentences.values.tolist()):\n",
    "        sample['MISSED'][i] = 1\n",
    "missed = sample.loc[sample['MISSED']==1]\n",
    "missed.to_csv(r'missed_expanded.csv', index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15\n",
      "Name: LABEL, dtype: int64\n",
      "0    14\n",
      "Name: LABEL, dtype: int64\n",
      "1    14\n",
      "Name: LABEL, dtype: int64\n",
      "0    11\n",
      "Name: LABEL, dtype: int64\n",
      "0    10\n",
      "Name: LABEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(missed['SUBJECT_ID'].value_counts())\n",
    "missed[missed['SUBJECT_ID']>="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1275\n",
       "0     523\n",
       "3     196\n",
       "1     121\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-479a3901d399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Binarize the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "n_classes = 6984\n",
    "# Binarize the output\n",
    "y_score = label_binarize(predictions, classes=np.arange(n_classes))\n",
    "y_test = label_binarize(truths, classes=np.arange(n_classes))\n",
    "\n",
    "print(y_test.shape)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEWCAYAAADLkvgyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XmcjfX7+PHXZTDWrPERsiXDbJaxhZBCqU+WRMnSYhtLUkIlfUQkbbK2USmSbPlVIkSLb8ggQrYsyW7szHL9/rjPnM6MM+NMOXNmxvV8PM5j5tz3fe77utfrfr/v5S2qijHGGBNIOQIdgDHGGGPJyBhjTMBZMjLGGBNwloyMMcYEnCUjY4wxAWfJyBhjTMBZMsogIvKMiLwbgOm2EZF9InJGRGpk9PS9CdSyyGxEpJGIbMvgaTYRkf0ZOU1/cm3XFf/B77L8NigiL4jIjDT6dxKRb/7BeMuLiIpIzn8XYTqnm97njESkITAWCAUSgN+AAaq65uqH538iMh3Yr6rPBToWfxCRncBAVV2QSn8FzgEKxAKfAoNUNSHjorw2uJZ1ZVXdEcAYmgAzVLVMoGJwxVEe2A3kUtV4P0+rCVdpnkVkhWtcAU9kIvICcJOqPnQ1l2dGrhtP6SoZich1wCLgLaAoUBr4H3Dx6odmrpJywOYrDBOpqgWAxkAH4BG/R+UHIhIUwGln6FlkZhDIeb4Wl3e2p6o+f4Ao4GQa/XMAzwF/AIeBD4FCrn7lcc6+Hwb2ASeAXkBtYCNwEpiQYnyP4JS8TgCLgXJpTLse8KNrPBuAJq7uRYH9wD2u7wWAHUAXoAcQB1wCzgBfuIa5AfgcOIJzhtDfYzovALNd83Ya50Af5dF/MHDA1W8b0MzjdzM8hvuv67cngRVAVY9+e4CnXMslqbSSJz3LHAh2zZMCZ4Gdqfxecc6ukr7PBiZ6fC8EvAccdM3XSCDIo3931zo6DWwBavq4DGe4/v8a6Jsipg1AW9f/IcAS4Lhred7vMdx0YDLwpWseb/cyfzcAC12/3wF0TxHHHNfyPQ38gpOY8XEe5gAzgFPAY0Ad4CfXOj0ITAByu4Zf6bEuzuAk/SY4pXKf1jvwtGu8f7qml2zdpZjvosA017AngPmu7k1w9ocnXdvLQeBhj9+1Ata75mkf8IJHv/KuaT4K7AVWurp/BvzlinklEOrxm7zAqzjbZyzwvavbXte4zrg+9a+0z7uG7wP8DuxOuf0Cd+Fsg6dxttWngPzAeSDRY1o3cPn+2JC/jx/7gG5elukonNqgC67xTHB1vwVY45q/NcAtaRyn9gCDXOv4LM6+VRL4yhX3UqCI57ry8vvbvexHly1PoBvwfRqxpLZuktZzTtdwD/P3Pr4L6OkxjuI4BZSTOPvYKiBHWsfCVONJq6eX4K8DjgEfAHcmLTSP/o/g7PAVcQ76c4GPUmzIU4A8QHPXSp0PlMApZR0GGruGb+0aV1UgJ84B98dU4irtiusunIPzHa7v17v6N8fZWUoA7wBzUhzQRqY4uK8Dngdyu+ZlF9DCYwO44JpWEDAaWO3qVwVnQ77BY54redlwbsbZEO8AcuEcZHbw94FrD/Azzk5T1LUh9Epl3lNd5t6SjZffe+7MITgHpyc8+s8HpuLs1CVccfV09Wvv2thqAwLchFMS82UZJi2LLsAPHtOrhrNhB7umuQ9nZ8gJ1ASO4jrYudZdLNDANc3LEjbwHTAJZ5urjpNYPE8Q4oD7XOvhKVzVEz7OQxzOdpoDZyeuhXNSlNO17pOqsFNL/E24PBl5Xe9AS5xtOBTIB3yU1roF/h9OMivimp/GHtOMB0a4ut+FU03reQAMd81TBHAIaJ1iH/7QtW7yemyDBV3r7A0gxiOOiTgnW6Vx9pdbXMMljSunx7Bp7vOu4Ze4lk3elMsUZ9tt5Pq/CH+fGCVbzl62wRtxDpgPuJZJMaB6Kst1BfCYx/eiOImzsyvmB1zfi6WRjFbjJKCkY94vQA3XclkGDP8Hycjb8uxG2snIp3WDc4JSCWcfb4yzvSQt29E4x/Rcrk8j13CpHgtTjcfXROQxA1VxDgL7cTbqhUBJV79vgWiPYavg7LBJO6cCpT36HwM6eHz/HNfOi3Om8GiKJHEOL6UjnAz8UYpui4GuHt/fAjbhnCkW8+g+neTJqC6wN8W4hgLTPDaApSkOnudd/9/k2rhux6lvTW3jHwbMTjFvB/i7NLcHeMij/1hgSirrI9Vl7u0A6OX3inMWfNb1/0wg2NWvJE4VbF6P4R8Alnss48e9jNOXZZi0LAq6pl3O9X0U8L7r/w7AqhTjmcrfO+t04MM05q0szplsQY9uo4HpHnGsTrEeDuLsUL7Mw8or7CsDgHkplvWVkpHX9Q68D4z26HdTausWKIVTEijipV8TnJKC50HrMFAvlXl4A3jd9X951zQrpjHPhV3DFHItz/N4lDY9hksal2ccae7zruFv87L9JiWjvUBP4Dov85xWMhrquZ6usE5XkDwZdQZ+TjHMT3gpWXms404e3z8HJnt870eKUqyX3//rZJTedZOi/3xc+z3OSc2ClNshaRwLU/uk+246Vf1NVbupczEwDOcs7g1X7xtwinxJ/sBJRCU9uh3y+P+8l+8FXP+XA94UkZMiklQEFJwsnlI5oH3SsK7hG+LslEnedsU7TVWPpTGL5YAbUozrmRTz8JfH/+eAPCKSU50L0wNwNpLDIjJLRG7wMo1ky0lVE3HOIjznLeU0CuCdL8v8Smq6xt8B5yCc39W9HM7ZzkGPZTEVp4QEzsF+p5fx+bIMAVDV0zhn8R1dnToCH3uMp26K8XQC/uMxin1pzNcNwHHXNJL8QfLl7P69az3sd/3Ol3lINm0RuVlEFonIXyJyCngJpxojPVJb7zekmF5a810WZ75PpNL/mCa/MO2ejojUFZHlInJERGJxqtJTzoN72iISJCJjRGSna573uHoVd33y4H0b8caXfT6t+W6HU9L7Q0S+E5H6Pk43te3YFyn3P7h8G0vJ12PgVeO6e/CM6zOFdKwbEblTRFaLyHHXermLv7eJV3BKs9+IyC4RGQKQjmOh27+6tVtVt+KcnYa5Ov2Js0EluRGn9HSI9NuHUx1U2OOTV1V/TGXYj1IMm19Vx4D7wvZUnOqF3iJyk+dseBnX7hTjKqiqd/kStKp+oqoNcZaDAi97GSzZchIRwdkhDvgyjbTGxT9c5uqYjXNW97yr8z6cklFxj2VxnaqGevSv5GV06V2GM4EHXAePvMByj/F8l2I8BVS1t2foaczWn0BRESno0e1Gki/nskn/iEgOoIzrd77MQ8ppTwa24twxdx1O8pI04kuPg67YLovbi3048134H0znE5zajrKqWginCiblPHjO94PAvThnwIVwzqpx/eYoTpW2t23E23rzZZ9PdX2r6hpVvRfnZGk+zvXPNH/jMV1vMXqdTIrvKfc/uHwb+6fO4lTJAu7j2PU+xpW8p+pLrn2ngKr2Iu114yYiwTilt3E4NWCFca7Rimu8p1X1SVWtCNwDDBSRZq5+vhwL3dJ7N12IiDwpImVc38viVNusdg0yE3hCRCqISAGcM8NP9Z/dHjgFGCoioa5pFRKR9qkMOwO4R0RauM7U8riep0jaeZ9x/X0EZ6F+6HHn1SGc6wFJfgZOichgEcnrGl+YiNS+UsAiUkVEbnOtwAs4ZznebpGeDbQSkWYikgvnYvJFnAuo6XU1lznAGKCHiPxHVQ8C3wCvish1IpJDRCqJSGPXsO8CT4lILXHcJCLlSP8y/BJngx3hij3R1X0RcLOIdBaRXK5PbRGp6suMqOo+nGU62rVNROBcfP/YY7BaItLWdXfWAJz1sPofzAM4VY6ngDMiEgL0TtE/5baWHrOBh0Wkqojk4+8Thsu41ttXwCQRKeJabrf6OJ2COKWqCyJSByfZXGn4izhV7vlwtr+kOBJxqhdfE5EbXMuwvmv/OIJTlei5PNKzzycjIrnFea6mkKrG4ayHpH3vEFBMRAql8vOPgdtF5H4RySkixUSkeirDplyHX+Jsow+6ftsBp+p+kS9xX8F2nFqXVq7jxHM413S88bY8U3WFdeMpt2uaR4B4EbkT5xo8ACJyt2u/F/5e5gnpOBa6pbdkdBqnGuf/ROQszk77K87BFNfMfYRzR81uVxD90jkNAFR1Hk4mnSVO8f9XnJsmvA27D+fs7BmchbYP546VHCJSCxgIdFHn2ZmXcbL0ENfP3wOqiVM1MN81zD04F7t345xBvItz1nclwTgH86P8fcPEMykHUtVtwEM417GOuqZ3j6pe8mEaKV21Ze6KbRPORf9Brk5dcDbILTgXZufgqv5U1c9wrvF8grNtzAeKpncZqupFnBsvbneNK6n7aZwNvyPOGehfOOsvtR3Smwdwztb/BObhXG9a4tF/AU71ZNJF6LaqGvcPt4OncA7ep3FulPk0Rf8XgA9c29r96ZgHVPUrYDxOqXEHTgkWUn+sojPOtcOtOHX3A3ycVDQwQkRO4yS82VcY/kOcaqkDONvI6hT9n8K5VrsGp9rtZZy7rc7hbDs/uJZHvfTs86noDOxx/bYXzj6WVIMzE9jlmlay6iJV3YtT9fSkK8YYIDKVabwJ3CciJ0RkvKvK/27Xb4/h3Ix0t6oeTUfcXqlqLM76eBdn+Z7FqUb2Nuxly9OHSXhdNynGexroj7MdnMDZvhd6DFIZ5w7AMzjb5CRVXYGPx0JP6X7o1ZjsQjweGgx0LOnlKh3+inOzSYY9mGiMv9jrgIzJIsR5tVNuESmCcxb7hSUik11YMjIm6+iJUw29E6f+PeU1KWOyLKumM8YYE3BWMjLGGBNwWe5lg8WLF9fy5csHOgxjjMlS1q1bd1RVU3tOKeCyXDIqX748a9euDXQYxhiTpYhIyjdFZCpWTWeMMSbgLBkZY4wJOEtGxhhjAs6SkTHGmICzZGSMMSbgLBkZY4wJOL8lIxF5X0QOi8ivqfQXERkvIjtEZKOI1PRXLMYYYzI3f5aMpgMt0+h/J87rxysDPXAaJjPGGHOVXbr0T1qnyVh+e+hVVVeKSPk0BrkX+FCdl+OtFpHCIlLK1TCYMcaYq2DQoEGsX78+0GFcUSCvGZUmeXv2+0ml3XgR6SEia0Vk7ZEjRzIkOGOMyQ7CwsJYtWpVoMO4okAmI/HSzesrxFX1bVWNUtWo66/PtK9WMsaYgNuyZQszZsxwf+/SpQvbtm0LYES+CWQy2g+U9fheBqdpaGOMMel07tw5nnnmGSIjI3nsscfYsWMHACJCVni5dCCT0UKgi+uuunpArF0vMsaY9Pvqq68ICwtj9OjRxMfH061bN4oVKxbosNLFbzcwiMhMoAlQXET2A8OBXACqOgX4ErgL2AGcAx72VyzGGJMdHThwgAEDBjBnzhwAIiIimDJlCvXr1w9wZOnnz7vpHrhCfwX6+Gv6xhiT3fXp04cFCxaQL18+RowYweOPP07OnFmuZSAgC7ZnZIwx17L4+Hh3wnn55ZfJlSsXr776KjfeeGOAI/t37HVAxhiTBcTGxtKvXz9atWqFU7EEVapU4bPPPsvyiQisZGSMMZmaqvLZZ58xYMAADh48SFBQEDExMdSoUSPQoV1VVjIyxphMaufOndx111106NCBgwcPUr9+fX755Zdsl4jAkpExxmRK48aNIywsjK+//prChQszdepUvv/+eyIiIgIdml9YNZ0xxmRC586d48KFC3Tu3Jlx48ZRokSJQIfkV5aMjDEmEzhy5Ajbtm2jYcOGAAwePJgmTZpw6623BjiyjGHVdMYYE0CJiYm8++67VKlShbZt23L8+HEAgoODr5lEBJaMjDEmYH799VduvfVWunfvzokTJ6hevTrnzp0LdFgBYcnIGGMy2NmzZxk8eDA1atTghx9+oGTJksycOZPFixdTpkyZQIcXEHbNyBhjMth9993H119/jYgQHR3NqFGjKFy4cKDDCihLRsYYk8EGDx7MoUOHmDx5MnXr1g10OJmCJSNjjPGj+Ph43nrrLfbs2cObb74JQJMmTVi7di05ctiVkiSWjIwxxk9+/vlnevbsSUxMDAA9evQgNDQUwBJRCrY0jDHmKjt58iTR0dHUq1ePmJgYypUrxxdffOFOROZyloyMMeYqmjVrFiEhIUyePJmgoCAGDx7M5s2bufvuuwMdWqZm1XTGGHMVffPNNxw6dIgGDRowefJkwsPDAx1SlmDJyBhj/oWLFy9y4MABKlasCMDYsWNp1KgRXbt2tetC6WBLyhhj/qFly5YRERFBq1atuHTpEgDFixfn4YcftkSUTra0jDEmnQ4dOkTnzp1p1qwZ27dvB2D//v0Bjiprs2RkjDE+SkxMZOrUqYSEhDBjxgzy5MnDyJEj2bBhg7uazvwzds3IGGN81KZNGxYuXAhAixYtmDhxIpUqVQpwVNmDlYyMMcZHbdu25T//+Q+ffvopX331lSWiq0hUNdAxpEtUVJSuXbs20GEYY64BCxcuZP/+/URHRwOgqpw5c4aCBQsGOLL0E5F1qhoV6DhSY9V0xhiTwt69e+nfvz8LFiwgODiYli1bUrFiRUQkSyairMCq6YwxxiUuLo5XX32VatWqsWDBAgoWLMjYsWMpV65coEPL9qxkZIwxwOrVq+nZsycbN24EoH379rz++uuULl06wJFdGywZGWMMMGzYMDZu3EiFChWYMGECd911V6BDuqZYNZ0x5pqkqpw6dcr9fcKECTzzzDP8+uuvlogCwO6mM8Zcc7Zt20Z0dDQiwpIlSxCRQIfkd5n9bjorGRljrhkXLlxg+PDhREREsGzZMmJiYtizZ0+gwzJYMjLGXCOWLFlCeHg4I0aM4NKlSzzyyCNs27aNChUqBDo0g5+TkYi0FJFtIrJDRIZ46X+jiCwXkfUislFErKLWGHNVqSqPPPIIzZs3Z8eOHVSrVo2VK1fy3nvvUaxYsUCHZ1z8loxEJAiYCNwJVAMeEJFqKQZ7DpitqjWAjsAkf8VjjLk2iQjly5cnb968jB49mvXr19OoUaNAh2VS8Oet3XWAHaq6C0BEZgH3Als8hlHgOtf/hYA//RiPMeYaERMTw8GDB7nzzjsBGDx4MJ07d7YquUzMn9V0pYF9Ht/3u7p5egF4SET2A18C/byNSER6iMhaEVl75MgRf8RqjMkGTp8+zcCBA6lVqxZdu3bl+PHjAAQHB1siyuT8mYy83SuZ8j7yB4DpqloGuAv4SEQui0lV31bVKFWNuv766/0QqjEmK1NV5s2bR7Vq1Xj99dcBePDBB8mVK1eAIzO+8mc13X6grMf3MlxeDfco0BJAVX8SkTxAceCwH+MyxmQjf/zxB3379mXRokUAREVFMXXqVGrWrBngyEx6+LNktAaoLCIVRCQ3zg0KC1MMsxdoBiAiVYE8gNXDGWN8oqq0a9eORYsWcd111zFhwgRWr15tiSgL8lsyUtV4oC+wGPgN5665zSIyQkT+6xrsSaC7iGwAZgLdNKu9EsIYk+ESExMB5065cePG0aFDB7Zu3UqfPn0ICgoKcHTmn7DXARljsoxjx44xZIjzyOI777wT4GiyFnsdkDHG/EuqygcffEBISAjvvvsuH374Ifv37w90WOYqsmRkjMnUfvvtN5o2bUq3bt04evQoTZo0YcOGDZQpUybQoZmryJKRMSZTUlWGDRtGZGQk3333HcWLF+eDDz5g2bJlhISEBDo8c5VZMjLGZEoiwoEDB4iLi6N79+5s27aNLl26XBPNPVyL7AYGY0ym8eeff3L06FEiIiIAOHr0KNu2baNBgwYBjizrsxsYjDHmChISEpgwYQJVq1alY8eOXLp0CYDixYtbIrpGWDIyxgTUL7/8Qr169ejXrx+nTp2iUqVKyZoDN9cGn5KRiOQWkZv8HYwx5tpx6tQpHn/8cWrXrs3atWspU6YMc+fOZeHChRQvXjzQ4ZkMdsVkJCKtgE3AEtf36iIyz9+BGWOyL1Xl1ltvZfz48YgIAwcOZMuWLbRp08ZuULhG+VIyGgHUBU4CqGoMYKUkY8w/JiI88cQT1KlTh7Vr1/Lqq69SsGDBQIdlAsiXt3bHqerJFGcrWesWPGNMQF26dInXXnuNoKAgBg0aBECXLl146KGH7F1yBvAtGf0mIvcDOUSkAvA4sNq/YRljsotVq1bRq1cvtmzZQnBwMF26dKFkyZKIiCUi4+ZLNV1foBaQCMwFLuAkJGOMSdXRo0d55JFHuPXWW9myZQuVK1dm0aJFlCxZMtChmUzIl2TUQlUHq2oN12cIcKe/AzPGZE2qyrRp0wgJCWHatGnkzp2b4cOHs3HjRm6//fZAh2cyKV+S0XNeuj17tQMxxmQfM2bM4NixY9x2221s3LiRF154gTx58gQ6LJOJpXrNSERa4DQJXlpEXvPodR1OlZ0xxgBw7tw5YmNjKVWqFCLCpEmTWLNmDZ06dbJbtY1P0rqB4TDwK841os0e3U8DQ/wZlDEm6/jqq6/o06cPFStWZMmSJYgIVapUoUqVKoEOzWQhqSYjVV0PrBeRj1X1QgbGZIzJAg4cOMCAAQOYM2cOAAULFuTYsWP29gTzj/hyzai0iMwSkY0isj3p4/fIjDGZUkJCAuPHj6dq1arMmTOH/Pnz8+qrr7Ju3TpLROYf8+U5o+nASGAczl10D2PXjIy5JiUmJtK4cWN++OEHAFq3bs2bb77JjTfeGODITFbnS8kon6ouBlDVnar6HNDUv2EZYzKjHDly0Lx5c8qWLcuCBQuYN2+eJSJzVfhSMroozu0wO0WkF3AAKOHfsIwxmYGqMnv2bHLmzEm7du0AGDx4MAMHDqRAgQIBjs5kJ74koyeAAkB/YBRQCHjEn0EZYwJv586dREdH880333D99ddz2223UaRIEYKDgwkODg50eCabuWIyUtX/c/17GugMICJl/BmUMSZwLl68yCuvvMKoUaO4cOECRYoUYdSoURQqVCjQoZlsLM1kJCK1gdLA96p6VERCgcHAbYAlJGOymRUrVtC7d2+2bt0KQOfOnRk3bhwlSljNvPGvVG9gEJHRwMdAJ+BrEXkWWA5sAG7OmPCMMRklISGB6Ohotm7dSpUqVVi2bBkffvihJSKTIdIqGd0LRKrqeREpCvzp+r4tY0IzxvhbYmIiFy5cIF++fAQFBTF58mRWrlzJ008/bdeFTIZK69buC6p6HkBVjwNbLREZk31s2rSJRo0a0a9fP3e3xo0bM2zYMEtEJsOlVTKqKCJzXf8LUN7jO6ra1q+RGWP84uzZs4wYMYLXXnuN+Ph4du/ezYkTJyhSpEigQzPXsLSSUbsU3yf4MxBjjP998cUX9O3bl7179yIiREdHM2rUKAoXLhzo0Mw1Lq0XpX6bkYEYY/wnPj6eDh06MHeuU7lRvXp1pk6dSp06dQIcmTEOX14HZIzJ4nLmzEmhQoUoUKAAr7/+OmvWrLFEZDIVUVX/jVykJfAmEAS8q6pjvAxzP/ACoMAGVX0wrXFGRUXp2rVr/RCtMdnL//2f87x63bp1ATh27Bjnz5+nTBl7RPBaJCLrVDUq0HGkxpfXAQEgIsGqejEdwwcBE4E7gP3AGhFZqKpbPIapDAwFGqjqCRGxBxqM+ZdOnjzJ0KFDmTp1KiEhIcTExJA7d26KFSsW6NCMSdUVq+lEpI6IbAJ+d32PFJG3fBh3HWCHqu5S1UvALJxnlzx1Byaq6gkAVT2cruiNMW6qyieffEJISAhTpkwhKCiI//73vyQkJAQ6NGOuyJdrRuOBu4FjAKq6Ad+akCgN7PP4vt/VzdPNwM0i8oOIrHZV6xlj0un333+nefPmdOrUiUOHDtGgQQPWr1/PmDFjyJs3b6DDM+aKfKmmy6GqfzitSLj5cqolXrqlvECVE6gMNMF5190qEQlT1ZPJRiTSA+gBWNspxqQQFxfHbbfdxv79+ylatChjx47l4YcfJkcOuz/JZB2+bK37RKQOoCISJCIDAF+aHd8PlPX4XgbnlUIph1mgqnGquhvYhpOcklHVt1U1SlWjrr/+eh8mbUz2l3TzUa5cuRg1ahTdunVj69atPProo5aITJbjyxbbGxgI3AgcAuq5ul3JGqCyiFQQkdxAR2BhimHm46ryE5HiONV2u3wL3Zhr06FDh+jcuTMjR450d+vSpQvTpk3DTtZMVuVLNV28qnZM74hVNV5E+gKLcW7tfl9VN4vICGCtqi509WsuIltwqv4Gqeqx9E7LmGtBYmIi77zzDkOGDOHkyZMULlyYAQMGULBgwUCHZsy/dsXnjERkJ0712afAXFU9nRGBpcaeMzLXog0bNtCrVy9Wr14NQMuWLZk4cSIVK1YMcGQmq8jszxldsZpOVSsBI4FawCYRmS8i6S4pGWPSLy4ujqeeeopatWqxevVqSpUqxezZs/nyyy8tEZlsxaernKr6o6r2B2oCp3Aa3TPG+FnOnDlZv349iYmJ9OvXj99++4327duT4u5WY7K8K14zEpECOA+rdgSqAguAW/wclzHXrL1795KQkECFChUQEaZMmUJsbCxRUZm2hsWYf82XktGvOHfQjVXVm1T1SVX9Pz/HZcw1Jy4ujnHjxlG1alW6d+/uvnW7cuXKlohMtufL3XQVVTXR75EYcw376aef6NWrFxs3bgSgaNGinDt3jvz58wc4MmMyRqrJSEReVdUngc9F5LJb7qylV2P+vRMnTjBkyBDefvttACpUqMDEiRO58847AxyZMRkrrZLRp66/1sKrMX5w8eJFqlevzt69e8mVKxeDBg3i2WefJV++fIEOzZgMl1ZLrz+7/q2qqskSkuthVmsJ1ph/ITg4mEcffZRvv/2WyZMnU61atUCHZEzA+PLQ6y+qWjNFt/WqWsOvkaXCHno1WdWFCxcYPXo0VapU4cEHnTYk4+PjCQoKslu1jd9l9ode07pm1AHndu4KIjLXo1dB4KT3XxljvFmyZAnR0dHs2LGDEiVK0KZNG/LmzUvOnD63b2lMtpbWnvAzThtGZXBabE1yGljvz6CMyS7++usvBg4cyMyZMwEIDQ1lypQp1saQMSmkdc1oN7AbWJpx4RiTPSQkJDB16lSeeeYZYmNjyZs3L8OHD+eJJ55qxDi/AAAgAElEQVQgd+7cgQ7PmEwnrWq671S1sYicIHmjeAKoqhb1e3TGZFEJCQm89dZbxMbGctdddzFhwgQqVKgQ6LCMybTSqqZLalq8eEYEYkxWd/r0aRISEihcuDC5c+fmnXfe4dChQ7Rt29ZuUDDmClJ9HZDHWxfKAkGqmgDUB3oC9li4MS6qyty5c6latSpPPvmku3vDhg1p166dJSJjfODLu+nm4zQ5Xgn4EOdlqZ/4NSpjsog9e/bw3//+l3bt2nHgwAF+/fVXLly4EOiwjMlyfElGiaoaB7QF3lDVfkBp/4ZlTOYWFxfHyy+/TLVq1Vi0aBHXXXcdEyZM4McffyRPnjyBDs+YLMenZsdFpD3QGWjt6pbLfyEZk7mdO3eOevXqsWnTJgA6duzIa6+9RqlSpQIcmTFZly/J6BEgGqcJiV0iUgGY6d+wjMm88uXLR1RUFOfOnWPSpEk0b9480CEZk+Vd8XVAACKSE7jJ9XWHqsb7Nao02OuATEZTVT788EMqVapEw4YNAYiNjSV37tz28KrJMrLs64CSiEgj4CPgAM4zRv8Rkc6q+oO/gzMm0H777Td69+7Nd999R9WqVYmJiSF37twUKlQo0KEZk634Uk33OnCXqm4BEJGqOMkp02ZYY/6t8+fPM2rUKMaOHUtcXBzXX389Q4cOJVcuu1xqjD/4koxyJyUiAFX9TUTsfSYm2/r666/p06cPu3btAqB79+6MGTOGokXtpSPG+IsvyegXEZmKUxoC6IS9KNVkU2fOnKFz584cPXqUsLAwpkyZQoMGDQIdljHZni/JqBfQH3ga55rRSuAtfwZlTEZKSEggMTGRXLlyUaBAAd58803279/PE088YdVyxmSQNJORiIQDlYB5qjo2Y0IyJuOsW7eOnj17cu+99zJs2DAAd8N3xpiMk+obGETkGZxXAXUClojIIxkWlTF+durUKR5//HHq1KnDunXr+Oijj4iLiwt0WMZcs9J6HVAnIEJV2wO1gd4ZE5Ix/qOqfPbZZ4SEhDB+/HhEhIEDB/LLL79YlZwxAZRWNd1FVT0LoKpHRMSX99gZk2mdPn2aDh068NVXXwFQt25dpkyZQvXq1QMcmTEmrWRUUUTmuv4XoJLHd1S1rV8jM+YqK1CgABcvXqRQoUKMGTOGHj16kCOHnWMZkxmklYzapfg+wZ+BGOMPK1eupFSpUlSuXBkR4f333ydPnjyULFky0KEZYzykmoxU9duMDMSYq+no0aM8/fTTTJs2jWbNmrFkyRJEhHLlygU6NGOMF1ZHYbKVxMRE3n//fapUqcK0adPInTs3jRo1IiEhIdChGWPS4NdkJCItRWSbiOwQkSFpDHefiKiI2PvuzD+2efNmmjRpwqOPPsrx48dp1qwZmzZtYvjw4eTM6cvz3caYQPF5DxWRYFW9mI7hg4CJwB3AfmCNiCz0fM+da7iCOG94+D9fx21MSrGxsdSrV48zZ85QokQJXnvtNR588EFEJNChGWN8cMWSkYjUEZFNwO+u75Ei4svrgOrgtH20S1UvAbOAe70M9yIwFrjge9jGOJLa4ypUqBCDBw+mV69ebN26lU6dOlkiMiYL8aWabjxwN3AMQFU3AE19+F1pYJ/H9/2ubm4iUgMoq6qL0hqRiPQQkbUisvbIkSM+TNpkdwcOHOC+++5jxowZ7m7PPvsskydPpkiRIgGMzBjzT/iSjHKo6h8puvlyNdjbaam7WVnXQ7SvA09eaUSq+raqRqlq1PXXX+/DpE12FR8fz5tvvklISAiff/45w4cPd9+cYCUhY7IuX5LRPhGpA6iIBInIAGC7D7/bD5T1+F4G+NPje0EgDFghInuAesBCu4nBpGbNmjXUrVuXAQMGcObMGVq3bs13331HUFBQoEMzxvxLviSj3sBA4EbgEE7S8OU9dWuAyiJSwdUYX0dgYVJPVY1V1eKqWl5VywOrgf+q6tp0zoPJ5s6ePUvfvn2pW7cuv/zyCzfeeCMLFixg3rx5lC1b9sojMMZkele8m05VD+MkknRR1XgR6QssBoKA91V1s4iMANaq6sK0x2CMI2fOnCxdupQcOXIwcOBAhg8fTv78+QMdljHmKpKku5FSHUDkHTyu9SRR1R7+CiotUVFRunatFZ6yu507d1K4cGGKFSsGOFV0efLkITw8PMCRGZM1icg6Vc20l0F8qaZbCnzr+vwAlAB8ft7ImPS4ePEiI0eOJCwsjMGDB7u7165d2xKRMdmYL9V0n3p+F5GPgCV+i8hcs1asWEHv3r3ZunUr4Nw5l5CQYDcoGHMN+CevA6oA2NsmzVVz+PBhunbtStOmTdm6dStVqlRh2bJlTJ8+3RKRMdeIK5aMROQEf18zygEcB1J9z5wx6XH06FGqVq3K8ePHCQ4O5tlnn+Xpp58mODg40KEZYzJQmslInKcII4EDrk6JeqU7HoxJh+LFi3Pvvfeyf/9+Jk2axE033RTokIwxAZBmMlJVFZF5qlorowIy2dvZs2cZMWIErVq14tZbbwVg0qRJBAcH2xsUjLmG+XLN6GcRqen3SEy298UXX1CtWjXGjh1LdHQ0iYmJAOTJk8cSkTHXuFRLRiKSU1XjgYZAdxHZCZzFeeecqqolKOOTffv28fjjjzNv3jwAatSowdSpU8mRw9p2NMY40qqm+xmoCbTOoFhMNhMfH8/48eN5/vnnOXv2LAUKFGDkyJH06dPHGrszxiST1hFBAFR1ZwbFYrKZU6dOMXr0aM6ePUu7du144403KFOmTKDDMsZkQmklo+tFZGBqPVX1NT/EY7K4kydPkjdvXoKDgylatChTp04lODiYVq1aBTo0Y0wmllalfRBQAKepB28fY9xUlU8++YQqVaowduxYd/e2bdtaIjLGXFFaJaODqjoiwyIxWdb27duJjo7m22+/BWDlypWoqt0hZ4zxWVolIzuSmDRduHCB//3vf4SHh/Ptt99StGhR3nvvPRYvXmyJyBiTLmmVjJplWBQmy/nrr7+49dZb+f333wHo1q0br7zyCsWLFw9wZMaYrCjVZKSqxzMyEJO1lCxZkrJly5IzZ04mT55M48aNAx2SMSYLs4c9jE8SExN55513aNq0KTfffDMiwieffEKRIkXInTt3oMMzxmRx9gi8uaINGzbQoEEDevXqRXR0NEnvyi1ZsqQlImPMVWHJyKTqzJkzPPXUU9SqVYvVq1dzww030KtXr0CHZYzJhqyazng1f/58+vXrx/79+8mRIwf9+vVj5MiRXHfddYEOzRiTDVkyMpc5cOAAHTt25OLFi9SqVYspU6YQFRUV6LCMMdmYJSMDQFxcHDlz5kREKF26NKNGjSJ37txER0db09/GGL+za0aGH3/8kVq1ajFjxgx3tyeffJJ+/fpZIjLGZAhLRtew48eP07NnTxo0aMCmTZuYNGkS1qq8MSYQrJruGqSqzJgxgyeffJIjR46QK1cunn76aZ599tk0X+MTFxfH/v37uXDhQgZGa4xJjzx58lCmTBly5coV6FDSxZLRNebQoUM88MADLF++HIDGjRszefJkqlatesXf7t+/n4IFC1K+fHl795wxmZCqcuzYMfbv30+FChUCHU66WDXdNaZw4cIcPHiQ4sWLM336dJYvX+5TIgLnxajFihWzRGRMJiUiFCtWLEvWXljJ6BqwZMkSatasSbFixQgODuazzz6jVKlSFCtWLN3jskRkTOaWVfdRKxllYwcPHuSBBx6gefPmDB482N09LCzsHyUiY4zxF0tG2VBCQgKTJk0iJCSEWbNmkTdvXqpUqXLN3Cm3cOFCxowZE+gwAm7FihUUKlSIGjVqEBISwlNPPZWs//z584mIiCAkJITw8HDmz5+frP+4ceMICQkhLCyMyMhIPvzww4wM3ydvvPFGpozLH5566imWLVsW6DD8R1Wz1KdWrVpqUrdu3TqtXbu2Agpoq1atdPfu3Vdl3Fu2bEneofhbyT+p+WBT8uGe+PaqxHM1JCYmakJCQsCmHx8f77dxL1++XFu1aqWqqufOndMqVaro999/r6qqMTExWqlSJd21a5eqqu7atUsrVaqkGzZsUFXVyZMna/PmzTU2NlZVVU+ePKnTp0+/qvH923mPi4vT8PBwjYuLS9dvsqo9e/boHXfc4dOwl+2rqgqs1UxwDE/tYyWjbGTPnj3UqVOHNWvWULp0aT7//HO++OILypcvH+jQroo9e/YQEhLCY489RlhYGJ06dWLp0qU0aNCAypUr8/PPPwMwffp0+vbtCzh3D7Zp04bIyEgiIyP58ccf2bNnD1WrViU6OpqaNWuyb98+Zs6cSXh4OGFhYcmqNFNOv1GjRtSsWZOaNWvy448/AtChQwe+/PJL93DdunXj888/JyEhgUGDBlG7dm0iIiKYOnUq4JRYmjZtyoMPPkh4eDgArVu3platWoSGhvL222+7x/Xee+9x880306RJE7p37+6eryNHjtCuXTtq165N7dq1+eGHH9Jcdnnz5qV69eocOHAAcEo9zzzzjPuOqwoVKjB06FBeeeUVAF566SUmTZrkfhdhoUKF6Nq162Xj3bFjB7fffjuRkZHUrFmTnTt3smLFCu6++273MH379mX69OkAlC9fnhEjRtCwYUPGjh1LnTp1ki3fiIgIANatW0fjxo2pVasWLVq04ODBg5dNe9myZdSsWZOcOZ1L3++88w61a9cmMjKSdu3ace7cOff6GDhwIE2bNmXw4MGcPXuWRx55hNq1a1OjRg0WLFiQ5vr9N7p160b//v255ZZbqFixInPmzAGclxA3a9aMmjVrEh4eniyGqlWr0r17d0JDQ2nevDnnz58HoFy5chw7doy//vrrX8eVKfkz0wEtgW3ADmCIl/4DgS3ARuBboNyVxmklo7Q99thj+sQTT+ipU6eu+rgDXTLavXu3BgUF6caNGzUhIUFr1qypDz/8sCYmJur8+fP13nvvVVXVadOmaZ8+fVRV9f7779fXX39dVZ0z8ZMnT+ru3btVRPSnn35SVdUDBw5o2bJl9fDhwxoXF6dNmzbVefPmXTb9s2fP6vnz51VVdfv27Zq0Lc6dO1e7dOmiqqoXL17UMmXK6Llz53Tq1Kn64osvqqrqhQsXtFatWrpr1y5dvny55suXz10qUVU9duyYqjolmNDQUD169KgeOHBAy5Urp8eOHdNLly5pw4YN3fP1wAMP6KpVq1RV9Y8//tCQkJDL4vUsGR0/flxr1qypBw8eVFXVGjVqaExMTLLhY2JitEaNGnrq1CktXLiwT+ukTp06OnfuXFVVPX/+vJ49ezbZdFVV+/Tpo9OmTVNV1XLlyunLL7/s7hcZGak7d+5UVdUxY8boiy++qJcuXdL69evr4cOHVVV11qxZ+vDDD1827eeff17Hjx/v/n706FH3/88++6y7X9euXbVVq1buktjQoUP1o48+UlXVEydOaOXKlfXMmTOprt+UGjZsqJGRkZd9lixZctmwXbt21fvuu08TEhJ08+bNWqlSJVV1SmhJpc4jR45opUqVNDEx0b2Nr1+/XlVV27dv745V1dm/58yZ4zUuT1mxZOS3u+lEJAiYCNwB7AfWiMhCVd3iMdh6IEpVz4lIb2As0MFfMWU3e/bsoV+/fjz11FPullbffvvtLHs3jS8qVKjgLk2EhobSrFkzRITw8HD27Nlz2fDLli1zX1MICgqiUKFCnDhxgnLlylGvXj0A1qxZQ5MmTbj++usB6NSpEytXrqR169bJxhUXF0ffvn2JiYkhKCiI7du3A3DnnXfSv39/Ll68yNdff82tt95K3rx5+eabb9i4caP7bDg2Npbff/+d3LlzU6dOnWTPgYwfP5558+YBsG/fPn7//Xf++usvGjduTNGiRQFo3769e5pLly5ly5a/d6VTp05x+vRpChYsmCzmVatWERERwbZt2xgyZAj/+c9/AOckNOV2ktTNWz9vTp8+zYEDB2jTpg3gPGzpiw4d/t7F77//fmbPns2QIUP49NNP+fTTT9m2bRu//vord9xxB+BcAy1VqtRl4zl48GCyxxJ+/fVXnnvuOU6ePMmZM2do0aKFu1/79u3dr7b65ptvWLhwIePGjQOcRxb27t3LDTfc4HX9prRq1Sqf5jNJ69atyZEjB9WqVePQoUOAs6yfeeYZVq5cSY4cOThw4IC7X4UKFahevToAtWrVSrZdlyhRgj///DNd088q/Hlrdx1gh6ruAhCRWcC9OCUhAFR1ucfwq4GH/BhPthEXF8drr73G//73P86fP8/Ro0f56aefgAy+rfNIX9+G6xLmfK6C4OBg9/85cuRwf8+RIwfx8fE+jyd//vzu/52TxsvNmzeP//3vfwC8++67LFq0iJIlS7JhwwYSExPdB988efLQpEkTFi9ezKeffsoDDzzgHu9bb72V7KAITjWd5/RXrFjB0qVL+emnn8iXLx9NmjThwoULqcYFTsu7P/30E3nz5k1zPhs1asSiRYvYvn07DRs2pE2bNlSvXp3Q0FDWrl3rrhYD+OWXX6hWrRrXXXcd+fPnZ9euXVSsWDHVcacWX86cOUlMTHR/T/nMi+e8d+jQgfbt29O2bVtEhMqVK7Np0yZCQ0Pd23Rq8ubNm2zc3bp1Y/78+URGRjJ9+nRWrFjhdZqqyueff06VKlWSje+FF17wun5TatSoEadPn76s+7hx47j99tsv6+65zSYts48//pgjR46wbt06cuXKRfny5d3z4jl8UFCQu5oOnGV5pXWeVfnzmlFpYJ/H9/2ubql5FPjKWw8R6SEia0Vk7ZEjR65iiFnP999/T40aNRgyZAjnz5+nY8eOzJ07N9BhZVrNmjVj8uTJgHOGferUqcuGqVu3Lt999x1Hjx4lISGBmTNn0rhxY9q0aUNMTAwxMTFERUURGxtLqVKlyJEjBx999BEJCQnucXTs2JFp06axatUqd/Jp0aIFkydPJi4uDoDt27dz9uzZy6YfGxtLkSJFyJcvH1u3bmX16tUA1KlTh++++44TJ04QHx/P559/7v5N8+bNmTBhgvt7TExMmsvh5ptvZujQobz88suAc2fW6NGj3Wfde/bs4aWXXuLJJ58EYOjQofTp08e9vE6dOpXsWhbAddddR5kyZdx34V28eJFz585Rrlw5tmzZwsWLF4mNjeXbb79NNa5KlSoRFBTEiy++6C4xValShSNHjriTUVxcHJs3b77st1WrVmXHjh3u76dPn6ZUqVLExcXx8ccfpzrNFi1a8NZbb7kTw/r16wHSXL+eVq1a5d4uPD/eElFqYmNjKVGiBLly5WL58uX88ccfPv1u+/bthIVdnRO7zMafycjbKbrXUykReQiIAl7x1l9V31bVKFWNSqpKudacOHGCxx57jEaNGrF582YqVarE4sWLmTlzptcqDON48803Wb58OeHh4dSqVcvrQa1UqVKMHj2apk2bui/E33vvvZcNFx0dzQcffEC9evXYvn17srPt5s2bs3LlSm6//XZ3U+yPPfYY1apVo2bNmoSFhdGzZ0+vpbeWLVsSHx9PREQEw4YNc1cfli5dmmeeeYa6dety++23U61aNQoVKgQ41XpJJZtq1aoxZcqUKy6LXr16sXLlSnbv3k316tV5+eWXueeeewgJCeGee+5h7Nix7uqh3r1707RpU2rXrk1YWBiNGzcmX758l43zo48+Yvz48URERHDLLbfw119/UbZsWe6//34iIiLo1KkTNWrUSDOuDh06MGPGDO6//34AcufOzZw5cxg8eDCRkZFUr17d680Ed955JytXrnR/f/HFF6lbty533HEHISEhqU5v2LBhxMXFERERQVhYGMOGDQPSXr9XW6dOnVi7di1RUVF8/PHHacabJC4ujh07dmTftsX8dTEKqA8s9vg+FBjqZbjbgd+AEr6M91q9geHo0aNavHhxzZUrlw4bNkzPnTuX4TF4uyhq/Ov06dOq6lzwvvvuu903CxhH69atdfv27YEOI0PMnTtXn3vuOZ+GtRsYklsDVBaRCsABoCPwoOcAIlIDmAq0VNXDfowlS9q6dSsVKlQgODiYYsWK8fHHH3PjjTf6dBZlsocXXniBpUuXcuHCBZo3b37ZTRXXujFjxnDw4EEqV64c6FD8Lj4+3l2Nmh2JpnGR9F+PXOQu4A0gCHhfVUeJyAicDL1QRJYC4UDSQwR7VfW/aY0zKipK165d67eYM4Nz584xatQoXnnlFYYNG+auRgi03377zeeXqhpjAsfbvioi61Q109bx+fVFqar6JfBlim7Pe/zv+xW/a8TXX39NdHQ0u3fvBuDo0aMBjsgYY/zP3tqdSfz5558MGDCAzz77DIDw8HCmTJnCLbfcEuDIjDHG/ywZZQLbt28nKiqK06dPky9fPl544QUGDBiQ5VpqNMaYf8qSUSZQuXJlateuTf78+XnrrbcoV65coEMyxpgMZS9KDYBTp04xYMAA9+tGRISFCxeycOFCS0TGb4KCgqhevTphYWHcc889nDx50t1v8+bN3Hbbbdx8881UrlyZF198MdkbFr766iuioqKoWrWq1+YoMoP169fz2GOPBTqMDLFo0SKGDx8e6DCurkDfW57eT1Z+zigxMVFnz56tpUqVUkBbtGgR6JDSJeWzC/BCsk9qpk5dm2y47t0X+jvUf8yfTToEevr58+d3/9+lSxcdOXKkqjovZ61YsaIuXrxYVZ0XwrZs2VInTJigqqqbNm3SihUr6m+//aaqzjNPEydOvKqxXY2mHe67777LXv7q72kGSmJiolavXl3Pnj3rtX9WfM7ISkYZZNeuXbRq1Yr777+fgwcPUq9ePferWYxvfG1C4ueff+aWW26hRo0a3HLLLWzbtg1wXgf01FNPER4eTkREBG+99RaQvFmDzz77jJiYGOrVq0dERARt2rThxIkTXuPx1uzD5MmTefrpp93DTJ8+nX79+gEwY8YM6tSpQ/Xq1enZs6f7dTMFChTg+eefp27duvz000+MGDHC/eaDHj16uEsoa9asISIigvr16zNo0CD3a2FSa6oiLfXr13c3J/HJJ5/QoEEDmjdvDkC+fPmYMGGCu4HCsWPH8uyzz7qfb8uZMyfR0dGXjfPMmTM8/PDD7uWb9PqiAgUKuIeZM2cO3bp1A5I37TBo0CDKly+frLR20003cejQIZ+ayzh9+jQbN24kMjISSH0bmD59Ou3bt+eee+5xz+8rr7ziXnaepY3UmvX4p6ZPn07btm1p2bIllStXTrad9O7dm6ioKEJDQ5PFUL58eYYPH+5uamLr1q2AU5vSpEkTFi1a9K/jyjQCnQ3T+8lqJaOLFy/qqFGjNE+ePApo4cKFdcqUKQFt0O2fCnTJyNcmJGJjY91nvUuWLNG2bduqquqkSZO0bdu27n5JzTakbNYgPDxcV6xYoaqqw4YN08cff9xrPN6afTh8+LC7mQBV1ZYtW+qqVat0y5Ytevfdd+ulS5dUVbV37976wQcfqKoqoJ9++ull41VVfeihh3ThQmd5hYaG6g8//KCqqoMHD9bQ0FBV1VSbqkgpqWQUHx+v9913n3711VeqqvrEE0/oG2+8cdnwhQsX1tjYWK/NTXjz9NNPJ1tWx48fTzZdVdXPPvtMu3btqqqXN+3Qv39/ff/991VVdfXq1dqsWTNV9a25jGXLlrnXs2rq28C0adO0dOnS7mW8ePFi7d69u7uRxVatWul3332nqt7Xb0oDBgzw2pzE6NGjLxt22rRpWqFCBT158qSeP39eb7zxRt27d2+yacXHx2vjxo3djRyWK1fO3RTGxIkT9dFHH3WPb8aMGdq3b9/LpqOaNUtGdgODn+3bt48RI0Zw8eJFOnXqxKuvvkrJkiUDHVaW5UsTErGxsXTt2pXff/8dEXG/qHTp0qX06tXL3RhbUtMM8HezBrGxsZw8edLdJEfXrl1p376911i8NftQr149KlasyOrVq6lcuTLbtm2jQYMGTJw4kXXr1lG7dm0Azp8/T4kSJQDnWk67du3c412+fDljx47l3LlzHD9+nNDQUPebopNu9X/wwQfdZ8WpNVXh2URF0jSrV6/Onj17qFWrlruJBtXUm4xIz1vgly5dyqxZs9zfixQpcsXfeDbt0KFDB0aMGMHDDz/MrFmz3OvEl+YyDh48iOd7K1PbBgDuuOMO97r/5ptv+Oabb9zvzztz5gy///47t956q9f1W6xYsWTxv/76674tHJdmzZq53y9YrVo1/vjjD8qWLcvs2bN5++23iY+P5+DBg2zZssX9RvW2bdsCTnMSni9Fzm7NSVgy8oMTJ05QuHBhRIRKlSrx5ptvctNNN9GsWbNAh3ZVqfp2AbVHj1r06FHrqkzTlyYkhg0bRtOmTZk3bx579uyhSZMmrnhTP+he6aWY+/bt45577gGcF46GhIR4bfYBnIPq7NmzCQkJoU2bNu42grp27cro0aMvG3eePHncB+QLFy4QHR3N2rVrKVu2LC+88MIVm5NQ9d5URUp58+YlJiaG2NhY7r77biZOnEj//v0JDQ1N9sJRcKqVCxQoQMGCBQkNDWXdunXuKrC04vC2fD27pdWcRP369dmxYwdHjhxh/vz5PPfcc4BvzWWkbE4itW0g5TRVlaFDh9KzZ89k40utWY+UnnjiCZYvX35Z944dOzJkyJDLuqdsHiI+Pp7du3czbtw41qxZQ5EiRejWrVuyaSX9Jmn4JNmtOQm7ZnQVJSYm8v7773PTTTcxY8YMd/eePXtmu0SUmcXGxlK6tNNaSVJz1+C8WXvKlCnuHfr48eOX/bZQoUIUKVLE3YDaRx99ROPGjSlbtqy7qYBevXql2uwDOGey8+fPZ+bMme6z+2bNmjFnzhwOHz7snra3ZgOSDkLFixfnzJkz7tJOkSJFKFiwoHs6niUQX5uq8JzH8ePHM27cOOLi4ujUqRPff/89S5cuBZwSVP/+/d3XNAYNGjIwtrgAABTPSURBVMRLL73kvvszMTGR11577bLxpmzWIulaW8mSJfntt99ITEx0lzS8ERHatGnDwIEDqVq1qrsU4ktzGSmbk0htG0ipRYsWvP/++5w5cwaAAwcOcPjw4TTXr6fXX3/da3MS3hJRak6dOkX+/PkpVKgQhw4d4quvvLakc5ns1pyEJaOrZPPmzTRp0oRHH32U48eP+7xBmavv6aefZujQoTRo0CBZmzSPPfYYN954IxEREURGRvLJJ594/f0HH3zAoEGDiIiIICYmhueff/6yYVJr9gGcxJFUBVOnTh3AqZIZOXIkzZs3JyIigjvuuIODBw9eNt7ChQvTvXt3wsPDad26tbtaD+C9996jR48e1K9fH1V1V/f42lSFpxo1ahAZGcmsWbPImzcvCxYsYOTIkVSpUoXw8HBq165N375O44kRERG88cYbPPDAA1StWpWwsDCvsT/33HOcOHGCsLAwIiMj3SWGMWPGcPfdd3PbbbddsbmTpOYkPFuD9aW5jJCQEGJjY92N3qW2DaTUvHlzHnzwQerXr094eDj33Xcfp0+fTnP9Xm2RkZHUqFGD0NBQHnnkERo0aODT75YvX06rVq38FleGC/RFq/R+MtsNDGfPntUhQ4Zozpw5FdASJUroxx9/rImJiYEO7aqzJiQCK6k5CVXV0aNHa//+/QMYTebz2muv6TvvvBPoMDLEX3/9pbfddluq/e0GhmvM9u3badGiBXv27EFE6NWrFy+99JJPF26NSa//9//+H6NHjyY+Pp5y5cqlWf10Lerdu7f73Y7Z3d69e/9/e/ceHVV9LXD8u0MpKdfqvYVK4VIhmPAMZMjlrcVbURDtVdQAKhICYhd4gSUKRQtULtiWKmIVqCkPi2AMryrS0qIuiIVaQUASGikkiAhpkaYpUoEYE9j3j3MymSQTMkFmziTZn7VmrcyZM+fs+ZHM5jx+e/PMM894HcZlFdYWEuEQTS0kSkpK8Pl8NGvWjPT09LAeykcDayFhTP1QH1tI2DWjOigrK2Px4sUUFRUBzl0uW7ZsYc+ePQ0+ERljTDhZMgrRe++9R58+fZg8eTIzZszwL2/Xrp1/3ooxxphLY8moFqdPn2bSpEn069ePffv2cc0113DHHXd4HZYxxjQoloxqoKqsWbOGzp07s2TJEpo0acIPfvADDhw44J/8aIwx5vKwZFSDnJwc7r33Xj755BMGDBjA+++/z89+9rNaZ+qb8HvttdcQEX/RSHBmzH/ve9+rtF5aWpp/0mhpaSmPPfYYCQkJJCYm0qdPnzrNBduxYwfdunXD5/NRXFxc6/pz5sxhwYIFIW+/Lvbu3Uv37t2Jj49nypQpNVZn+PnPf86qVavCEkO0mTZtGtu2bfM6DPMlWDIKEDg5zufzMXXqVJYtW8aOHTv89dCM9zIzM7n++usrVSGozezZszlx4gS5ubnk5ubym9/8xj9BMhQZGRlMmzaN7Oxsz0uwTJw4kaVLl5Kfn09+fj5btmyptk5ZWRkvvvgi9913X8jbrW2ibDSbPHmyv8q4qZ8sGbmysrJITEysVKNr4cKFjB8/npgYG6aqJEyP2pw5c4Z33nmHFStWhJyMzp07x7Jly1i0aJG/zlerVq0YMWJEtXW3bt1Kz5496d69O+PGjaOkpITly5ezbt065s6dy6hRo6q9Z9WqVf6qDqNHj672+rJly+jduzdJSUncfffdnDt3DoD169f7qxUMHDgQcCp5lLeZ6NGjB/n5+ZW2deLECf71r3/Rv39/RITU1FQ2btxYbZ/btm0jOTnZf3NNTTEEtnGYMWMGZ8+eZdy4cfTu3ZuePXvy+uuvA077ju985zskJyeTnJzMn/70p5DG/mLS0tKYMmUKAwYMoEOHDv6j2DNnzjBo0CB/24TAGLp06cKDDz5It27dGDx4sP8otV27dhQVFfHJJ5986biMR7yedVvXx+WuwHDy5ElNTU1VQAF/GwJTXeCs7nD9A9dm9erVOm7cOFVV7d+/v+7du1dVVbOysvS2226rtO6YMWN0/fr1mpOToz6fr9ZtFxcXa9u2bfXQoUOqqjp69Gh99tlnK22rqtzcXO3YsaMWFhaqakUrgCeeeEKffvppVdVKrQdmzpzpbwmQmJioBQUFqqp66tQpVVWdNGmSvvzyy6rqtB85d+5cpf3t3r3b31pBVXX79u3VPreq6o9+9CP/fi4WQ9U2Do8//riuXr3aH1NCQoKeOXNGz549q8XFxaqqmpeXpzX9HV5//fVBWyq89dZb1dYdM2aMpqSk6Pnz5/WDDz7wt94oLS3V06dPq6pqYWGhXnvttXrhwgV/C5F9+/apqurw4cP9saqqjh8/Xjds2BA0rsbGKjDUIxcuXGDFihXMmDGDU6dO0axZM2bNmsX06dO9Dq1e8GqqdGZmJg8//DDgVEbOzMwkOTn5srRAOHToEHFxcXTs2BFw2kcsWbLEv79gtm3bRkpKCi1btgQqt6Uol5uby6xZs/j00085c+aMv7r2ddddR1paGiNGjPC3Cejfvz8//vGPKSgo4K677iIhIaHStjTI9aFgn/HEiROVJj3WFANUbuPw5ptvsmnTJv/1rs8//5xjx47Rpk0bJk2aRHZ2Nk2aNPEXTa2qvMBsqIYNG0ZMTAxdu3bl5MmT/s/4wx/+kO3btxMTE8Nf//pX/2txcXH4fD7AaalQ3jYEGl5LhcamUSajjz76iPvvv99/qmHw4MEsWbKE+Ph4jyMzF1NUVMS2bdvIzc1FRDh//jwiwlNPPUWLFi2qdWT95z//ScuWLYmPj+fYsWPVeuBUFeyLvjaqNbelKJeWlsbGjRtJSkpi5cqVvP322wCkp6eza9cuNm/ejM/nIzs7m/vuu4++ffuyefNmhgwZwvLly7nxxhv922rbti0FBQX+5wUFBbRp06baPqu2VKgpBqjeUuHXv/41nTp1qrS9OXPm0KpVK3Jycrhw4QKxsbFBP2t536WqFixYwE033VRteWBLhfLxz8jIoLCwkL1799K0aVPat2/v/yxVWzAE3kzS0FoqNDaN8mLIlVdeSV5eHt/61rdYs2YNW7ZssURUD2zYsIHU1FQ+/vhjjh49yvHjx4mLi+OPf/wjCQkJ/O1vf+Mvf/kLAB9//DE5OTn4fD6aN2/OAw88wJQpU/jiiy8A58ghsM0HOJWfjx496m9FUN4+4mIGDRrEunXr/FU5grWl+Oyzz2jdujWlpaVkZGT4l3/44Yf07duXuXPn0rJlS44fP86RI0fo0KEDU6ZM4fbbb2f//v2VttW6dWt/KwlVZdWqVUHnvVVtqVBTDFUNGTKERYsW+RPDvn37AGe+XevWrYmJiWH16tU1VsLesWNH0JYKwRJRTU6fPs3VV19N06ZNycrKCtpqI5iG1lKhsWk0yeiNN96gpKQEgBYtWrBp0yYOHjzIyJEj63Qqx3gnMzOTO++8s9Kyu+++m1deeYVmzZrx8ssvM3bsWHw+HykpKSxfvtzfZuHJJ5/km9/8Jl27diUxMZFhw4ZV6gwKTpO7X/3qVwwfPpzu3bsTExPDhAkTLhpTt27dmDlzJjfccANJSUk88sgj1daZN28effv25eabb6Zz587+5dOnT6d79+4kJiYycOBAkpKSWLt2LYmJifh8Pg4ePEhqamq17b3wwguMHz+e+Ph4rr32WoYOHVptnaFDh1a6GaemGKqaPXs2paWl9OjRg8TERGbPng3AQw89xEsvvUS/fv3Iy8sL6xSHUaNGsWfPHnr16kVGRsZF4y1XWlrK4cOH6dUrakuvmdp4fdGqro+63sBw7NgxHTZsmAI6b968Or3XVGYtJOqXYcOGaV5entdhRMSrr76qs2bN8jqMqFEfb2BosEdGZWVlLFy4kC5durBx40auuOKKoBeXjWmo5s+fH7QJXkNUVlbGo48+6nUY5ktokDcw7Ny5kwkTJpCTkwM4p3Kee+45fxtiYxqDTp06VbsRoaEaPny41yGYL6nBJaNdu3YxYMAAVJX27duzePHihtWa12Mawt1jxhjv6CXcFRoNGlwy6tOnD0OGDKFnz57MmjWL5s2bex1SgxEbG0tRUREtWrSwhGRMFFJVioqKarz1PprV+2SUn5/P1KlTWbhwIR07dkRE2Lx5s5XwCYPyOS6FhYVeh2KMqUFsbCxt27b1Oow6q7fJqKSkhPnz5/PTn/6UkpISYmNj/bWtLBGFR9OmTYmLi/M6DGNMAxTWb20RuUVEDonIYRF5LMjrzURkrfv6LhFpH8p2t27dSo8ePZgzZw4lJSWMHTuW9PT0yx2+McaYCJFwXewSkSZAHnAzUADsBu5V1QMB6zwE9FDVCSJyD3Cnqo682HZbtGih5bPcu3TpQnp6ur/isTHGmOBEZK+qRu2s4HAeGfUBDqvqEVX9AlgDVK1bcgfwkvvzBmCQ1HJl/NSpU8TGxvKTn/yE7OxsS0TGGNMAhPPIKAW4RVXHu89HA31VdVLAOrnuOgXu8w/ddf5RZVvfB77vPk0EcsMSdP3TEvhHrWs1DjYWFWwsKthYVOikqjVXCvZYOG9gCHaEUzXzhbIOqroUWAogInui+VAzkmwsKthYVLCxqGBjUUFE9ngdw8WE8zRdAfDtgOdtgarNRvzriMhXgKuA6mWPjTHGNGjhTEa7gQQRiRORrwL3AJuqrLMJGOP+nAJs0/o6fdgYY8wlC9tpOlUtE5FJwBtAE+BFVf1ARObiVI/dBKwAVovIYZwjontC2PTScMVcD9lYVLCxqGBjUcHGokJUj0XYbmAwxhhjQmWlCowxxnjOkpExxhjPRW0yClcpofoohLF4REQOiMh+EdkqIu28iDMSahuLgPVSRERFpMHe1hvKWIjICPd34wMReSXSMUZKCH8j14hIlojsc/9ObvUiznATkRdF5O/uHM5gr4uIPO+O034RSY50jDXyutVssAfODQ8fAh2ArwI5QNcq6zwEpLs/3wOs9TpuD8fiu0Bz9+eJjXks3PW+DmwHdgK9vI7bw9+LBGAf8B/u86u9jtvDsVgKTHR/7goc9TruMI3FQCAZyK3h9VuB3+PM8ewH7PI65vJHtB4ZhaWUUD1V61ioapaqnnOf7sSZ09UQhfJ7ATAPeAr4PJLBRVgoY/EgsERVTwGo6t8jHGOkhDIWClzp/nwV1ec8Ngiqup2Lz9W8A1iljp3Av4tI68hEd3HRmoz+Ezge8LzAXRZ0HVUtA04DLSISXWSFMhaBHsD5n09DVOtYiEhP4Nuq+ttIBuaBUH4vOgIdReQdEdkpIrdELLrICmUs5gD3i0gB8DtgcmRCizp1/T6JmGjtZ3TZSgk1ACF/ThG5H+gF3BDWiLxz0bEQkRjgWSAtUgF5KJTfi6/gnKr7b5yj5R0ikqiqn4Y5tkgLZSzuBVaq6jMi0h9nfmOiql4If3hRJWq/N6P1yMhKCVUIZSwQkZuAmcDtqloSodgirbax+DpOId23ReQozjnxTQ30JoZQ/0ZeV9VSVf0IOISTnBqaUMbiAWAdgKq+C8TiFFFtbEL6PvFCtCYjKyVUodaxcE9N/RInETXU6wJQy1io6mlVbamq7VW1Pc71s9tVNaoLRF6iUP5GNuLc3IKItMQ5bXckolFGRihjcQwYBCAiXXCSUWFEo4wOm4BU9666fsBpVT3hdVAQpafpNHylhOqdEMfiaeAKYL17D8cxVb3ds6DDJMSxaBRCHIs3gMEicgA4D0xX1SLvog6PEMfiUWCZiEzFOS2V1hD/8yoimTinZVu618eeAJoCqGo6zvWyW4HDwDlgrDeRVmflgIwxxnguWk/TGWOMaUQsGRljjPGcJSNjjDGes2RkjDHGc5aMjDHGeM6SkYk6InJeRLIDHu0vsm77mioU13Gfb7tVn3Pc8jmdLmEbE0Qk1f05TUTaBLy2XES6XuY4d4uIL4T3PCwizb/svo0JJ0tGJhoVq6ov4HE0QvsdpapJOAV4n67rm1U1XVVXuU/TgDYBr41X1QOXJcqKOH9BaHE+DFgyMlHNkpGpF9wjoB0i8r77GBBknW4i8p57NLVfRBLc5fcHLP+liDSpZXfbgXj3vYPcHjh/dnvFNHOXz5eKHlIL3GVzRGSaiKTg1AjMcPf5NfeIppeITBSRpwJiThORRZcY57sEFLkUkRdEZI84vYv+z102BScpZolIlrtssIi8647jehG5opb9GBN2loxMNPpawCm619xlfwduVtVkYCTwfJD3TQCeU1UfTjIocEu/jASuc5efB0bVsv//Af4sIrHASmCkqnbHqVgyUUS+AdwJdFPVHsCTgW9W1Q3AHpwjGJ+qFge8vAG4K+D5SGDtJcZ5C07Jn3IzVbUX0AO4QUR6qOrzOLXHvquq33XLAs0CbnLHcg/wSC37MSbsorIckGn0it0v5EBNgcXuNZLzOHXWqnoXmCkibYFXVTVfRAYB/wXsdkslfQ0nsQWTISLFwFGcFgOdgI9UNc99/SXgf4HFOL2SlovIZiDkdhWqWigiR9y6YPnuPt5xt1uXOP8Np/RNYKfOESLyfZy/69Y4TeT2V3lvP3f5O+5+voozbsZ4ypKRqS+mAieBJJwj+mqN81T1FRHZBdwGvCEi43FK5r+kqo+HsI9RgUVVRSRofyy3FlofnMKb9wCTgBvr8FnWAiOAg8BrqqriZIaQ48TpZjofWALcJSJxwDSgt6qeEpGVOMVAqxLgLVW9tw7xGhN2dprO1BdXASfc/jOjcY4KKhGRDsAR99TUJpzTVVuBFBG52l3nGyLSLsR9HgTai0i8+3w08Af3GstVqvo7nJsDgt3R9hlOS4tgXgWG4fTYWesuq1OcqlqKc7qtn3uK70rgLHBaRFoBQ2uIZSdwXflnEpHmIhLsKNOYiLJkZOqLXwBjRGQnzim6s0HWGQnkikg20BmnvfIBnC/tN0VkP/AWzimsWqnq5zhVjdeLyJ+BC0A6zhf7b93t/QHnqK2qlUB6+Q0MVbZ7CjgAtFPV99xldY7TvRb1DDBNVXOAfcAHwIs4p/7KLQV+LyJZqlqIc6dfprufnThjZYynrGq3McYYz9mRkTHGGM9ZMjLGGOM5S0bGGGM8Z8nIGGOM5ywZGWOM8ZwlI2OMMZ6zZGSMMcZz/w+W+AF1ipEO8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "lw = 2\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', ])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='AUC of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0      2.864725\n",
       "1      2.199209\n",
       "2     10.039456\n",
       "3     50.217645\n",
       "4      6.784873\n",
       "5      0.259714\n",
       "6      0.045767\n",
       "7      1.522407\n",
       "8      0.451425\n",
       "9      0.395525\n",
       "10     0.127792\n",
       "11     0.004850\n",
       "12     0.004706\n",
       "13    25.081905\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testnotes= testnotes.sample(frac=0.05, random_state=0)\n",
    "#testnotes['CATEGORY'].unique()\n",
    "\n",
    "testnotes.groupby('CATEGORY').count()['TEXT']/len(testnotes) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training for 100 epochs\n",
      "torch.Size([32, 25000]) Linear(in_features=25000, out_features=2, bias=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-3cadffcbcdd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding).to(device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maucs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-161-a396e4b6da7a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, learning_rate, num_epoch, print_every)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#             loss2 = loss_fn2(output2, data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;31m#loss2.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "##TODO: FIX\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=0\n",
    "BATCH_SIZE = 32\n",
    "#random.sample(list(train_data), int(len(train_data) * 0.05))\n",
    "#random.sample(list(train_data), int(len(train_data)/400))\n",
    "train_loader = DataLoader(BioDataset(training), batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(BioDataset(testing), batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# test_loader = DataLoader(MIMICDataset(test_data,  sentences, labels, sp),\n",
    "#                           batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "torch.manual_seed(111)\n",
    "# glove_model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM', k=2)\n",
    "# glove_model = glove_model.to(device)\n",
    "\n",
    "#ntokens = len(words)# the size of vocabulary\n",
    "ntokens = 8000\n",
    "#print(ntokens/nhead)\n",
    "emsize = 80 # embedding dimension\n",
    "nhid = 80 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 1\n",
    "\n",
    "    \n",
    "ntokens = 8000\n",
    "nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 1\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout, insize=500)\n",
    "#model=RNNEncoder(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "model = model.float().cuda()\n",
    "\n",
    "#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding).to(device))\n",
    "\n",
    "model, predictions, truths, accs, losses, aucs = train(model, train_loader=train_loader, test_loader=test_loader, learning_rate = 0.1e-2,num_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 4\n",
      "Parameter: GPU pinned 4 × 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32 × 16000000\n",
      "Parameter: GPU pinned 32000\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 120 × 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Series.data is deprecated and will be removed in a future version\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Index.data is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: GPU pinned 5000 × 1 × 40\n",
      "Tensor: GPU pinned 8 × 8\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000\n",
      "Tensor: GPU pinned 32 × 16000000\n",
      "Tensor: GPU pinned 8 × 500\n",
      "Tensor: GPU pinned 8\n",
      "Tensor: GPU pinned 8 × 4\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 32 × 16000000\n",
      "Tensor: GPU pinned 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: RangeIndex.data is deprecated and will be removed in a future version\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Int64Index.data is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Tensor: GPU pinned 5000 × 1 × 40\n",
      "Total size: 1546987949\n"
     ]
    }
   ],
   "source": [
    "#subset, k=3\n",
    "# Test set | Epoch: 0 | Accuracy: 59.0000 | time elapse:  00:00:11\n",
    "# Test set | Epoch: 1 | Accuracy: 52.0000 | time elapse:  00:00:22\n",
    "# Test set | Epoch: 2 | Accuracy: 52.0000 | time elapse:  00:00:34\n",
    "# Test set | Epoch: 3 | Accuracy: 66.0000 | time elapse:  00:00:45\n",
    "# Test set | Epoch: 4 | Accuracy: 69.0000 | time elapse:  00:00:55\n",
    "# Test set | Epoch: 5 | Accuracy: 58.0000 | time elapse:  00:01:04\n",
    "# Test set | Epoch: 6 | Accuracy: 71.0000 | time elapse:  00:01:14\n",
    "# Test set | Epoch: 7 | Accuracy: 72.0000 | time elapse:  00:01:23\n",
    "# Test set | Epoch: 8 | Accuracy: 70.0000 | time elapse:  00:01:33\n",
    "# Test set | Epoch: 9 | Accuracy: 64.0000 | time elapse:  00:01:43\n",
    "torch.cuda.is_available()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "#k=4\n",
    "# Train set | epoch:   1/10 |    100/   237 batches | Loss: 0.7166\n",
    "# Train set | epoch:   1/10 |    200/   237 batches | Loss: 0.3979\n",
    "# Test set | Epoch: 1 | Accuracy: 74.0000 | time elapse:  00:00:08\n",
    "# Train set | epoch:   2/10 |    100/   237 batches | Loss: 0.3456\n",
    "# Train set | epoch:   2/10 |    200/   237 batches | Loss: 0.4418\n",
    "# Test set | Epoch: 2 | Accuracy: 80.0000 | time elapse:  00:00:17\n",
    "# Train set | epoch:   3/10 |    100/   237 batches | Loss: 0.0871\n",
    "# Train set | epoch:   3/10 |    200/   237 batches | Loss: 0.1620\n",
    "# Test set | Epoch: 3 | Accuracy: 80.0000 | time elapse:  00:00:26\n",
    "# Train set | epoch:   4/10 |    100/   237 batches | Loss: 0.0428\n",
    "# Train set | epoch:   4/10 |    200/   237 batches | Loss: 0.0070\n",
    "# Test set | Epoch: 4 | Accuracy: 77.0000 | time elapse:  00:00:35\n",
    "# Train set | epoch:   5/10 |    100/   237 batches | Loss: 0.0042\n",
    "# Train set | epoch:   5/10 |    200/   237 batches | Loss: 0.0432\n",
    "# Test set | Epoch: 5 | Accuracy: 78.0000 | time elapse:  00:00:44\n",
    "# Train set | epoch:   6/10 |    100/   237 batches | Loss: 0.0078\n",
    "# Train set | epoch:   6/10 |    200/   237 batches | Loss: 0.0751\n",
    "# Test set | Epoch: 6 | Accuracy: 80.0000 | time elapse:  00:00:54\n",
    "# Train set | epoch:   7/10 |    100/   237 batches | Loss: 0.0047\n",
    "# Train set | epoch:   7/10 |    200/   237 batches | Loss: 0.0414\n",
    "# Test set | Epoch: 7 | Accuracy: 79.0000 | time elapse:  00:01:03\n",
    "# Train set | epoch:   8/10 |    100/   237 batches | Loss: 0.0024\n",
    "# Train set | epoch:   8/10 |    200/   237 batches | Loss: 0.0015\n",
    "# Test set | Epoch: 8 | Accuracy: 78.0000 | time elapse:  00:01:12\n",
    "# Train set | epoch:   9/10 |    100/   237 batches | Loss: 0.0012\n",
    "# Train set | epoch:   9/10 |    200/   237 batches | Loss: 0.0069\n",
    "# Test set | Epoch: 9 | Accuracy: 78.0000 | time elapse:  00:01:20\n",
    "# Train set | epoch:  10/10 |    100/   237 batches | Loss: 0.0131\n",
    "# Train set | epoch:  10/10 |    200/   237 batches | Loss: 0.0016\n",
    "# Test set | Epoch: 10 | Accuracy: 79.0000 | time elapse:  00:01:29\n",
    "\n",
    "#torch.cuda.max_memory_allocated(0)#-torch.cuda.memory_allocated(0)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()\n",
    "\n",
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)\n",
    "\n",
    "dump_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "# import os\n",
    "# import torch\n",
    "# import time\n",
    "# import sys\n",
    "exp_truths, exp_preds = [0, 3, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 3, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 3, 2, 0, 2, 3, 2, 2, 2, 0, 3, 0, 0, 2, 1, 3, 2, 2, 2, 2, 3, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 3, 2, 2, 0, 2, 2, 3, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 0, 3, 2, 2, 2, 2, 3, 1, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 1, 2, 0, 2, 3, 2, 2, 2, 2, 1, 0, 0, 2, 2, 3, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 3, 0, 1, 3, 1, 1, 2, 2, 1, 2, 0, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 3, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 3, 0, 0, 2, 2, 1, 2, 2, 2, 0, 3, 3, 0, 0, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 0, 2, 3, 3, 3, 2, 3, 3, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 3, 2, 2, 3, 3, 2, 0, 0, 0, 2, 3, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 3, 3, 2, 1, 2, 1, 1, 2, 2, 3, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 0, 3, 2, 1, 0, 0, 0, 2, 3, 2, 2, 0, 2, 2, 2, 2, 3, 2, 0, 0, 3, 0, 0, 2, 0, 3, 2, 3, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 1, 0, 2, 0, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 3, 0, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 3, 2, 2, 2, 2, 2, 0, 0, 2, 3, 2, 3, 1, 0, 1, 3, 0, 0, 0, 3, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 3, 2, 2, 0, 0, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 3, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 1, 0, 3, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 1, 0, 2, 0, 2, 0, 0, 3, 2, 0, 3, 2, 2, 1, 1, 0, 2, 0, 2, 2, 2, 3, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 3, 0, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 3, 0, 2, 2, 0, 2, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2, 3, 2, 0, 0, 3, 2, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 3, 3, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 0, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 1, 2, 3, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 3, 0, 2, 2, 0, 2, 0, 2, 2, 2, 3, 3, 0, 0, 0, 3, 2, 1, 2, 2, 2, 0, 2, 3, 0, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 3, 0, 2, 2, 2, 2, 0, 2, 1, 0, 0, 2, 3, 3, 3, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 3, 3, 2, 3, 2, 0, 2, 2, 2, 0, 2, 1, 3, 3, 2, 1, 2, 2, 3, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 2, 0, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 3, 3, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 0, 0, 3, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 2, 3, 0, 2, 0, 3, 0, 3, 2, 2, 2, 3, 2, 2, 0, 3, 1, 2, 3, 3, 0, 3, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 3, 2, 2, 0, 2, 0, 2, 2, 2, 0, 3, 2, 2, 0, 2, 1, 3, 1, 2, 2, 3, 0, 3, 0, 3, 2, 2, 3, 2, 2, 2, 3, 0, 2, 2, 2, 2, 1, 3, 3, 2, 0, 0, 2, 3, 2, 2, 2, 1, 2, 2, 2, 3, 3, 0, 2, 2, 0, 0, 2, 1, 3, 2, 3, 0, 2, 3, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 2, 0, 2, 0, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 0, 1, 3, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 3, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 3, 2, 0, 1, 3, 3, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 3, 1, 2, 2, 2, 0, 2, 0, 0, 3, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 1, 0, 2, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 0, 3, 3, 1, 0, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 2, 2, 2, 3, 0, 0, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 2, 2, 2, 2, 0, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 1, 2, 2, 0, 2, 0, 0, 2, 2, 1, 0, 3, 2, 2, 2, 1, 2, 0, 0, 0, 2, 3, 2, 1, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 0, 2, 2, 3, 0, 2, 0, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 2, 3, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 2, 3, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 3, 2, 2, 2, 2, 0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 3, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 0, 2, 2, 2, 3, 2, 0, 2, 0, 2, 2, 3, 0, 2, 3, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 3, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 3, 2, 2, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 3, 0, 2, 2, 3, 2, 3, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 3, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 0, 0, 2, 1, 1, 0, 3, 0, 2, 2, 2, 2, 0, 0, 0, 3, 0, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 0, 2, 2, 2, 0, 3, 0, 2, 0, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 3, 2, 2, 0, 2, 2, 1, 1, 3, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 3, 1, 0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 0, 2, 0, 3, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 3, 2, 2, 3, 0, 2, 2, 3, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 2, 2, 3, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 3, 2, 2, 2, 3, 0, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 3, 3, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 3, 0, 2, 0, 2, 2, 1, 0, 2, 0, 2, 3, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 3, 0, 2, 3, 0, 2, 2, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 3, 0, 2, 1, 0, 0, 1, 3, 3, 2, 2, 2, 2, 0, 2, 0, 3, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 1, 0, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "# print(os.environ)\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "# print(os.getpid())\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# a = torch.randn(10, 10, device=device)\n",
    "# c\n",
    "# os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_preds_array = np.zeros((len(exp_preds), 4))\n",
    "exp_preds_array[:,1] = 1\n",
    "\n",
    "auc = metrics.roc_auc_score(exp_truths,exp_preds_array, multi_class = \"ovo\")\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]], dtype=torch.float64)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ndarray((100, 4))\n",
    "a = torch.from_numpy(a)\n",
    "a = nn.Softmax(dim=1)(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
