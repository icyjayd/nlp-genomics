{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Patient #', 'Last Name', 'First Name', 'MRN', 'DOB', 'Sex',\n",
      "       'Date of study', 'Accession #', 'Cirrhosis \\n(2=possible; 1=yes; 0=no)',\n",
      "       'If yes cirrhosis, is there progression? (1=yes; 0=no; 2=n/a)',\n",
      "       'If yes cirrhosis progression, notes on why?',\n",
      "       'Presence of stigmata of portal HTN? (1=yes; 0=no)',\n",
      "       'If yes portal HTN, splenomegaly? \\n(1=yes; 0=no; 2=N/A; 3=splenectomy)',\n",
      "       'If yes portal HTN, varices? \\n(1=yes; 0=no; 2=N/A)',\n",
      "       'If yes varices, 1 site or >1 site (1=>1 site; 0=1 site; 2=N/A; 3=unknown)',\n",
      "       'If yes portal HTN, ascites? \\n(1=yes; 0=no; 2=N/A)',\n",
      "       'If yes ascites, what is the volume? (0=small/trace; 1=moderate; 2=large; 3=unknown; 4=N/A)',\n",
      "       'Presence of portal vein thrombosis? (1=yes; 0=no)',\n",
      "       'If yes PV thrombosis, occlusive or nonocclusive? (1=occlusive; 0=nonocclusive; 2=N/A; 3=unknown)',\n",
      "       'Presence of SMV thrombosis? (1=yes; 0=no)',\n",
      "       'If yes SMV thrombosis, occlusive or nonocclusive? (1=occlusive; 0=nonocclusive; 2=N/A)',\n",
      "       'Presence of fibrosis? (1=yes; 0=no)',\n",
      "       'If MRE present, what is the measured kPa?', '# of liver lesions',\n",
      "       'Lesion 1: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
      "       'Lesion 1: If yes HCC, what LIRADS?', 'Lesion 1: If HCC, what OPTN?',\n",
      "       'Lesion 1: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
      "       'Lesion 1: If yes HCC s/p tx, what treatment?',\n",
      "       'Lesion 1: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
      "       'Lesion 1: If indeterminate liver lesions, \\nwhat characteristics?',\n",
      "       'Lesion 2: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
      "       'Lesion 2: If yes HCC, what LIRADS?', 'Lesion 2: If HCC, what OPTN?',\n",
      "       'Lesion 2: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
      "       'Lesion 2: If yes HCC s/p tx, what treatment?',\n",
      "       'Lesion 2: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
      "       'Lesion 2: If indeterminate liver lesions, \\nwhat characteristics?',\n",
      "       'Lesion 3: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
      "       'Lesion 3: If yes HCC, what LIRADS?', 'Lesion 3: If HCC, what OPTN?',\n",
      "       'Lesion 3: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
      "       'Lesion 3: If yes HCC s/p tx, what treatment?',\n",
      "       'Lesion 3: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
      "       'Lesion 3: If indeterminate liver lesions, \\nwhat characteristics?',\n",
      "       'Lesion 4: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
      "       'Lesion 4: If yes HCC, what LIRADS?',\n",
      "       'Lesion 4: If yes HCC, what OPTN?',\n",
      "       'Lesion 4: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
      "       'Lesion 4: If yes HCC s/p tx, what treatment?',\n",
      "       'Lesion 4: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
      "       'Lesion 4: If indeterminate liver lesions, \\nwhat characteristics?',\n",
      "       'Other', 'TEXT', 'RAW TEXT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#import the data\n",
    "df = pd.read_csv('KangData/Kang_MRI_Report_Metadata.csv')\n",
    "df = df[df['TEXT'].notnull()]#clearing null values\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    576\n",
       "0     49\n",
       "Name: Cirrhosis \\n(2=possible; 1=yes; 0=no), dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only take binary values\n",
    "\n",
    "df =  df[df['Cirrhosis \\n(2=possible; 1=yes; 0=no)'].astype(str).str.isdigit()]\n",
    "df = df[df['Cirrhosis \\n(2=possible; 1=yes; 0=no)'].astype(int)<2]\n",
    "df['Cirrhosis \\n(2=possible; 1=yes; 0=no)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming and splitting the data\n",
    "# np.random.seed(0)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['TEXT'])\n",
    "y = df[['Cirrhosis \\n(2=possible; 1=yes; 0=no)']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.reshape([y_train.shape[0],])\n",
    "y_test = y_test.reshape([y_test.shape[0],])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring function for clean presentation\n",
    "\n",
    "def score_model(y_true, y_pred):\n",
    "    scores = {}\n",
    "    scores['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    scores['precision'] = precision_score(y_true, y_pred,average='macro' )\n",
    "    scores['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    scores['f1'] = f1_score(y_true, y_pred, average='micro')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.952,\n",
       " 'precision': 0.8791666666666667,\n",
       " 'recall': 0.717911877394636,\n",
       " 'f1': 0.952}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#fitting and evaluating the model\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "scores = score_model(y_test, y_pred)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.92,\n",
       " 'precision': 0.7173763736263736,\n",
       " 'recall': 0.8031609195402298,\n",
       " 'f1': 0.92}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat but with SMOTE\n",
    "np.random.seed(42)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['TEXT'])\n",
    "y = df[['Cirrhosis \\n(2=possible; 1=yes; 0=no)']].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "y_train = y_train.reshape([y_train.shape[0],])\n",
    "y_test = y_test.reshape([y_test.shape[0],])\n",
    "\n",
    "np.random.seed(0)\n",
    "#fitting and evaluating the model\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "scores = score_model(y_test, y_pred)\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    525\n",
       "0.0    111\n",
       "Name: Presence of stigmata of portal HTN? (1=yes; 0=no), dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat all of the above with 'Presence of stigmata of portal HTN? (1=yes; 0=no)'\n",
    "df = pd.read_csv('KangData/Kang_MRI_Report_Metadata.csv')\n",
    "df = df[df['TEXT'].notnull()]#clearing null values\n",
    "df = df[df['Presence of stigmata of portal HTN? (1=yes; 0=no)'].notnull()]\n",
    "# df =  df[df['Presence of stigmata of portal HTN? (1=yes; 0=no)'].astype(str).str.isdigit()]\n",
    "df = df[df['Presence of stigmata of portal HTN? (1=yes; 0=no)'].astype(int)<2]\n",
    "df['Presence of stigmata of portal HTN? (1=yes; 0=no)'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#transforming and splitting the data\n",
    "np.random.seed(0)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['TEXT'])\n",
    "y = df[['Presence of stigmata of portal HTN? (1=yes; 0=no)']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.reshape([y_train.shape[0],])\n",
    "y_test = y_test.reshape([y_test.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.92,\n",
       " 'precision': 0.7173763736263736,\n",
       " 'recall': 0.8031609195402298,\n",
       " 'f1': 0.92}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "#fitting and evaluating the model\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "scores = score_model(y_test, y_pred)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/home/jic286/.conda/envs/imblearn_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8984375,\n",
       " 'precision': 0.4698872785829307,\n",
       " 'recall': 0.5331465919701214,\n",
       " 'f1': 0.8984375}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat but with SMOTE\n",
    "np.random.seed(0)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['TEXT'])\n",
    "y = df[['Cirrhosis \\n(2=possible; 1=yes; 0=no)']].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2)\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "y_train = y_train.reshape([y_train.shape[0],])\n",
    "y_test = y_test.reshape([y_test.shape[0],])\n",
    "\n",
    "np.random.seed(0)\n",
    "#fitting and evaluating the model\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "scores = score_model(y_test, y_pred)\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (2.0.1)\n",
      "611\n",
      "2141.3486088379705\n",
      "4503 1098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Patient #', 'Last Name', 'First Name', 'MRN', 'DOB', 'Sex',\n",
       "       'Date of study', 'Accession #', 'Cirrhosis \\n(2=possible; 1=yes; 0=no)',\n",
       "       'If yes cirrhosis, is there progression? (1=yes; 0=no; 2=n/a)',\n",
       "       'If yes cirrhosis progression, notes on why?',\n",
       "       'Presence of stigmata of portal HTN? (1=yes; 0=no)',\n",
       "       'If yes portal HTN, splenomegaly? \\n(1=yes; 0=no; 2=N/A; 3=splenectomy)',\n",
       "       'If yes portal HTN, varices? \\n(1=yes; 0=no; 2=N/A)',\n",
       "       'If yes varices, 1 site or >1 site (1=>1 site; 0=1 site; 2=N/A; 3=unknown)',\n",
       "       'If yes portal HTN, ascites? \\n(1=yes; 0=no; 2=N/A)',\n",
       "       'If yes ascites, what is the volume? (0=small/trace; 1=moderate; 2=large; 3=unknown; 4=N/A)',\n",
       "       'Presence of portal vein thrombosis? (1=yes; 0=no)',\n",
       "       'If yes PV thrombosis, occlusive or nonocclusive? (1=occlusive; 0=nonocclusive; 2=N/A; 3=unknown)',\n",
       "       'Presence of SMV thrombosis? (1=yes; 0=no)',\n",
       "       'If yes SMV thrombosis, occlusive or nonocclusive? (1=occlusive; 0=nonocclusive; 2=N/A)',\n",
       "       'Presence of fibrosis? (1=yes; 0=no)',\n",
       "       'If MRE present, what is the measured kPa?', '# of liver lesions',\n",
       "       'Lesion 1: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
       "       'Lesion 1: If yes HCC, what LIRADS?', 'Lesion 1: If HCC, what OPTN?',\n",
       "       'Lesion 1: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
       "       'Lesion 1: If yes HCC s/p tx, what treatment?',\n",
       "       'Lesion 1: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
       "       'Lesion 1: If indeterminate liver lesions, \\nwhat characteristics?',\n",
       "       'Lesion 2: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
       "       'Lesion 2: If yes HCC, what LIRADS?', 'Lesion 2: If HCC, what OPTN?',\n",
       "       'Lesion 2: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
       "       'Lesion 2: If yes HCC s/p tx, what treatment?',\n",
       "       'Lesion 2: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
       "       'Lesion 2: If indeterminate liver lesions, \\nwhat characteristics?',\n",
       "       'Lesion 3: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
       "       'Lesion 3: If yes HCC, what LIRADS?', 'Lesion 3: If HCC, what OPTN?',\n",
       "       'Lesion 3: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
       "       'Lesion 3: If yes HCC s/p tx, what treatment?',\n",
       "       'Lesion 3: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
       "       'Lesion 3: If indeterminate liver lesions, \\nwhat characteristics?',\n",
       "       'Lesion 4: HCC? \\n(2=indeterminate;\\n1=yes; 0=no)',\n",
       "       'Lesion 4: If yes HCC, what LIRADS?',\n",
       "       'Lesion 4: If yes HCC, what OPTN?',\n",
       "       'Lesion 4: If yes HCC s/p tx?\\n(1=yes; 0=no; 2=N/A)',\n",
       "       'Lesion 4: If yes HCC s/p tx, what treatment?',\n",
       "       'Lesion 4: If indeterminate liver lesion, \\nwhat LIRADS?',\n",
       "       'Lesion 4: If indeterminate liver lesions, \\nwhat characteristics?',\n",
       "       'Other', 'TEXT', 'RAW TEXT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('KangData/kang_data_cleaned_8000.model')\n",
    "!pip install xlrd\n",
    "# cirrhosis = pd.read_excel('KangData/NLP Cirrhosis Final.xlsx')\n",
    "meta = pd.read_csv('KangData/Kang_MRI_Report_Metadata.csv')\n",
    "count = 0\n",
    "lengths = 0\n",
    "min_len = np.inf\n",
    "max_len = 0\n",
    "for text in meta['TEXT']:\n",
    "    if(isinstance(text, str)):\n",
    "        length = len(text)\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "        if length < min_len and length > 3:\n",
    "            min_len = length\n",
    "        if(length > 3):\n",
    "            count +=1\n",
    "        lengths += length\n",
    "print(count)\n",
    "print(lengths/count)\n",
    "print(max_len, min_len)\n",
    "#average length: 2051\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize(examples):\n",
    "    return tokenizer(list(examples['TEXT']), padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        #x = self.sentence_to_padded_index_sequence(self.features[key], pad_length = self.pad_length)\n",
    "        x = torch.tensor(self.features[key])\n",
    "        y = int(self.labels[key])\n",
    "        return x, y\n",
    "        \n",
    "meta = meta[~meta['TEXT'].isna()]\n",
    "mask = (meta['TEXT'].str.len()>3)\n",
    "meta = meta.loc[mask]\n",
    "\n",
    "ones = meta.loc[meta['Cirrhosis \\n(2=possible; 1=yes; 0=no)']=='1']\n",
    "zeros = meta[(meta['Cirrhosis \\n(2=possible; 1=yes; 0=no)']=='0')]\n",
    "meta = pd.concat([ones, zeros])\n",
    "X = (tokenize(meta)['input_ids'])\n",
    "y = meta['Cirrhosis \\n(2=possible; 1=yes; 0=no)']\n",
    "#SMOTE with Imabalanced Learn\n",
    "# oversample = SMOTE()\n",
    "# X, y = oversample.fit_resample(X, y)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# print(y.shape)\n",
    "\n",
    "dataset = MRIDataset(X, y)\n",
    "train, test = train_test_split(dataset, train_size = .8)\n",
    "train_loader = DataLoader(train, batch_size = 2)\n",
    "test_loader = DataLoader(test, batch_size = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, d_embs, d_conv, ):\n",
    "        self.embs = nn.Embedding(d_embs)\n",
    "        self.conv = nn.Conv1d(1, d_conv)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-34034aaecfc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "help(tokenizer)\n",
    "sp = tokenizer\n",
    "\n",
    "meta = meta[~meta['TEXT'].isna()]\n",
    "mask = (meta['TEXT'].str.len()>3)\n",
    "meta = meta.loc[mask]\n",
    "train, test = train_test_split(meta, train_size = .8)\n",
    "count = 0\n",
    "lengths = 0\n",
    "max_len = 0\n",
    "min_len = np.inf\n",
    "for tokens in tokenize(meta)['input_ids']:\n",
    "    count +=1\n",
    "    lengths += len(tokens)\n",
    "    if(lengths>max_len):\n",
    "        max_len = len(tokens)\n",
    "    if(lengths <min_len):\n",
    "        min_len = len(tokens)\n",
    "print(lengths/count)\n",
    "print(max_len)\n",
    "print(min_len)\n",
    "\n",
    "meta= meta[['TEXT','Cirrhosis \\n(2=possible; 1=yes; 0=no)']]\n",
    "print(next(iter(train_dataset))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.5\n",
      "Requirement already satisfied: transformers in /gpfs/home/jic286/.local/lib/python3.6/site-packages (4.15.0)\n",
      "Requirement already satisfied: dataclasses in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (1.17.4)\n",
      "Requirement already satisfied: sacremoses in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages/sacremoses-0.0.43-py3.8.egg (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /gpfs/home/jic286/.local/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: six in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in /gpfs/data/proteomics/projects/Juan/.conda/envs/py36/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_type_to_module_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f6e986b41578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -U --user transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2707\u001b[0m             \u001b[0mBigBirdPegasusForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m             \u001b[0mBigBirdPegasusForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m             \u001b[0mBigBirdPegasusForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m             \u001b[0mBigBirdPegasusForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m             \u001b[0mBigBirdPegasusModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mtar_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path_extracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0mtar_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Archive format of {output_path} could not be identified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                 \u001b[0mtar_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mtar_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path_extracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                 \u001b[0mtar_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Archive format of {output_path} could not be identified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/auto/__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mTF_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mTF_MODEL_WITH_LM_HEAD_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mTFAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mTFAutoModelForImageClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_BaseAutoModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_class_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type_to_module_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_list_option_in_docstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_type_to_module_name'"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy, Precision, Recall, F1\n",
    "print (pd.__version__)\n",
    "!pip install -U --user transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "acc_m = Accuracy().to(device)\n",
    "prec_m = Precision(average='macro', num_classes=2).to(device)\n",
    "rec_m = Recall(average='macro', num_classes=2).to(device)\n",
    "f1_m = F1( num_classes=2).to(device)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "num_epochs = 100\n",
    "#train_loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        model.zero_grad()\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device).long()\n",
    "        out = model(x)['logits']\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: {} | Last Loss: {}\".format(epoch, loss.item()))\n",
    "#test loop        \n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device).long()\n",
    "            out = model(x)['logits']\n",
    "            acc = acc_m(softmax(out), y)\n",
    "            prec = prec_m(softmax(out), y)\n",
    "            rec = rec_m(softmax(out), y)\n",
    "            f1 = f1_m(softmax(out), y)\n",
    "    print(\"Epoch: {} | Accuracy: {} | Precision: {} | Recall: {} | F1: {}\".format(epoch, acc_m.compute(),\n",
    "                                                                                 prec_m.compute(), rec_m.compute(),\n",
    "                                                                                 f1_m.compute()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### f = open(\"corpuscleaned.txt\", \"w\")\n",
    "f.write(str(cleaned)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned.to_csv(r'corpuscleaned.txt',sep=\" \", index=False, header=False)\n",
    "\n",
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "    for s in sentences:\n",
    "        word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", s.lower()))\n",
    "    \n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortEmbeddings(path, emb_dim = 50):\n",
    "    \n",
    "\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        glove_embedding = []\n",
    "        words = {}\n",
    "        chars = {}\n",
    "        idx2words = {}\n",
    "        ordered_words = []\n",
    "    \n",
    "        for i, line in tqdm(enumerate(f)):\n",
    "            s = line.split()\n",
    "            glove_embedding.append(np.asarray(s[1:]))\n",
    "            \n",
    "            words[s[0]] = len(words)\n",
    "            idx2words[i] = s[0]\n",
    "            ordered_words.append(s[0])\n",
    "\n",
    "        \n",
    "    # add unknown to word and char\n",
    "    glove_embedding.append(np.random.rand(emb_dim))\n",
    "    words[\"<UNK>\"] = len(words)\n",
    "    \n",
    "    # add padding\n",
    "    glove_embedding.append(np.zeros(emb_dim))\n",
    "    words[\"<PAD>\"] = len(words)\n",
    "    \n",
    "    chars[\"<UNK>\"] = len(chars)\n",
    "    chars[\"<PAD>\"] = len(chars)\n",
    "    \n",
    "    glove_embedding = np.array(glove_embedding).astype(float)\n",
    "    return glove_embedding, words, idx2words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notes = pd.read_csv('mimic/NOTEEVENTS.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"',low_memory=False, encoding='utf-8')\n",
    "\n",
    "#patients = notes['SUBJECT_ID']\n",
    "#patient_counts = patients.value_counts()\n",
    "# len(patient_counts.loc[patient_counts>=1e3])\n",
    "# patients = pd.read_csv('mimic/PATIENTS.csv.gz', compression='gzip', header=0, sep=',', quotechar='\"', low_memory=False, encoding='utf-8')\n",
    "#del(patients)\n",
    "#len(notes.loc[notes['CHARTTIME']!= np.NaN])\n",
    "#(len(notes)- len(notes.loc[pd.isna(notes['CHARTTIME']) == False]))/len(notes)\n",
    "# notes.head()\n",
    "# patients\n",
    "reports = pd.read_csv(r'KangData/Kang_MRI_Report_Metadata.csv')##8-26-20 - working with stella data\n",
    "reports = reports[~reports['TEXT'].isna()]\n",
    "reports\n",
    "\n",
    "print(max(reports[\"Patient #\"].value_counts()))\n",
    "lengths = []\n",
    "zeros = 0\n",
    "for i, report in enumerate(reports['TEXT']):\n",
    "    if(len(sp.encode_as_ids(report))==0):\n",
    "        zeros +=1\n",
    "    lengths.append(len(sp.encode_as_ids(report)))\n",
    "min(lengths)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPRESSION:  Cirrhosis and portal hypertension.\n",
      "-----\n",
      "No evidence of HCC.\n",
      "-----\n",
      "Liver stiffness measures 5.2 kPa, consistent with stage IV fibrosis.\n",
      "-----\n",
      "Interpretation of MRE results:  <2.5 kPa : Normal 2.5 to 3.0 kPa : Normal or inflammation  Increased liver stiffness under appropriate clinical and laboratory findings is compatible with fibrosis as below:  3 to 3.5 kPa = Stage 1-2 fibrosis 3.5 to 4 kPa = Stage 2-3 fibrosis 4 to 5 kPa = Stage 3-4 fibrosis >5kPa = Stage 4 fibrosis  3D image post processing was performed, supporting the findings stated above.\n",
      "-----\n",
      "MRI ELASTOGRAPHY LIVER WITH AND WITHOUT IV CONTRAST  Clinical Indication: Hepatitis C , HIV.\n",
      "-----\n",
      "Technique: Multiplanar T1-weighted, T2-weighted, and diffusion-weighted sequences were obtained of the abdomen, including dynamic post-contrast T1-weighted sequences.\n",
      "-----\n",
      "Contrast: 7.2 cc Gadavist    COMPARISON: None.\n",
      "-----\n",
      "FINDINGS:  LIVER:  Morphology:  Morphologic changes of cirrhosis.\n",
      "-----\n",
      "Signal intensity: Normal.\n",
      "-----\n",
      "No lesions concerning for HCC.\n",
      "-----\n",
      "Other lesions: None.\n",
      "-----\n",
      "Other findings: None.\n",
      "-----\n",
      "Portal venous system: Patent SMV: Patent Hepatic veins: Patent Hepatic arterial anatomy: Replaced right hepatic artery arising from the SMA.\n",
      "-----\n",
      "Varices: Small upper abdomen varices.\n",
      "-----\n",
      "Measured liver stiffness: 5.2 kPa Range: 4.7 - 6.4 kPa  SPLEEN: Enlarged at 18.1 cm.\n",
      "-----\n",
      "PANCREAS: Normal.\n",
      "-----\n",
      "BILIARY TREE: Small gallstone.\n",
      "-----\n",
      "Nonspecific gallbladder wall edema, most likely reactive.\n",
      "-----\n",
      "No biliary ductal dilatation.\n",
      "-----\n",
      "ADRENALS: Normal.\n",
      "-----\n",
      "KIDNEYS: Complete and symmetric enhancement without hydronephrosis or mass.\n",
      "-----\n",
      "LYMPHADENOPATHY/RETROPERITONEUM: Mildly prominent periportal lymph nodes are most likely reactive.\n",
      "-----\n",
      "BOWEL: No bowel obstruction.\n",
      "-----\n",
      "PERITONEUM/ABDOMINAL WALL: Trace upper abdominal ascites.\n",
      "-----\n",
      "VASCULATURE: Normal caliber abdominal aorta.\n",
      "-----\n",
      "SKELETAL: No aggressive osseous lesion.\n",
      "-----\n",
      "LUNG BASES: No pleural effusion or consolidation.\n",
      "-----\n",
      "Electronic Signature: I personally reviewed the images and agree with this report.\n",
      "-----\n",
      "Final Report: Dictated by Fellow Ilana Kafer and Signed by Attending Andrew Rosenkrantz MD 2/14/2018 8:26 AM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import nltk.data\n",
    "digits = \"([0-9])\" \n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "# def digitrepl(matchobj):\n",
    "#     print(matchobj.group(0))\n",
    "#     if matchobj.group(0)[]\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "data = reports['TEXT'][0]\n",
    "\n",
    "# print(reports['TEXT'][0])\n",
    "split_into_sentences(reports['TEXT'][0])\n",
    "m = re.match(re.compile(r\"\\d+\"), reports['TEXT'][0])\n",
    "data = re.sub(r\"\\d+\\.\\s\",  r\"\", reports['TEXT'][0])\n",
    "data = re.sub(r\"\\n\",r\" \", data)\n",
    "print ('\\n-----\\n'.join(tokenizer.tokenize(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kang_data_cleaned.txt\", \"w\") as f:\n",
    "    for report in reports['TEXT']:\n",
    "        data = re.sub(r\"\\d+\\.\\s\",  r\"\", report)\n",
    "        data = re.sub(r\"\\n\",r\" \", data)\n",
    "\n",
    "        data = tokenizer.tokenize(data)\n",
    "        for line in data:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "####THIS IS THE ONE THAT WORKS FOR CONVERTING THE NOTES'CATEGORIES TO LABELS\n",
    "\n",
    "testnotes = notes[~notes['ISERROR'].notnull() & notes['CATEGORY'].notnull() & notes['CHARTTIME'].notnull() & notes['STORETIME'].notnull\n",
    "                  & notes['DESCRIPTION'].notnull() & notes['HADM_ID'].notnull()]\n",
    "testnotes['ISERROR'] = testnotes['ISERROR'].fillna(0)\n",
    "testnotes = testnotes[['TEXT', 'CATEGORY', 'SUBJECT_ID','HADM_ID']]\n",
    "\n",
    "#BINARY\n",
    "#classes = pd.read_csv('binary_class_labels.csv').iloc[:, 1:3]\n",
    "#FULL\n",
    "classes = pd.read_csv('class_labels.csv')\n",
    "\n",
    "classes = dict(zip(classes['Category'].values, classes['Group'].values))\n",
    "testnotes['CATEGORY'] = testnotes['CATEGORY'].apply(lambda x: classes[x])\n",
    "#14 and 3 are the same, more or less, so let's combine them\n",
    "testnotes['CATEGORY'].replace(14, 3, inplace=True)\n",
    "\n",
    "#testnotes['CATEGORY'].unique()\n",
    "testnotes['HADM_ID'] = testnotes['HADM_ID'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Combining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>TOKENS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>319840</td>\n",
       "      <td>319082</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-05</td>\n",
       "      <td>2187-03-05 08:01:00</td>\n",
       "      <td>2187-03-05 08:01:31</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Admission Note</td>\n",
       "      <td>15165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:  n/v, abdominal pain, fever\\n...</td>\n",
       "      <td>[2948, 2154, 31941, 55, 31963, 31957, 31956, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319996</td>\n",
       "      <td>319164</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 05:00:00</td>\n",
       "      <td>2187-03-06 05:00:30</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Progress Note</td>\n",
       "      <td>21108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fever (Hyperthermia, Pyrexia, not Fever of Unk...</td>\n",
       "      <td>[4310, 57, 15054, 31956, 10198, 31956, 186, 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320269</td>\n",
       "      <td>319218</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 12:36:00</td>\n",
       "      <td>2187-03-06 12:50:56</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Nursing Transfer Note</td>\n",
       "      <td>21297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59 F h/o breast ca being treated with carbopla...</td>\n",
       "      <td>[4046, 86, 68, 31963, 31922, 3384, 350, 2420, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320275</td>\n",
       "      <td>319224</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 07:54:00</td>\n",
       "      <td>2187-03-06 14:42:31</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Progress Note</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...</td>\n",
       "      <td>[2948, 2154, 31941, 458, 2461, 2215, 31941, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320276</td>\n",
       "      <td>319225</td>\n",
       "      <td>2984</td>\n",
       "      <td>124661.0</td>\n",
       "      <td>2187-03-06</td>\n",
       "      <td>2187-03-06 07:54:00</td>\n",
       "      <td>2187-03-06 14:47:22</td>\n",
       "      <td>Physician</td>\n",
       "      <td>Physician Resident Progress Note</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...</td>\n",
       "      <td>[2948, 2154, 31941, 458, 2461, 2215, 31941, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082553</td>\n",
       "      <td>2076162</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-21</td>\n",
       "      <td>2165-08-21 06:59:00</td>\n",
       "      <td>2165-08-21 07:00:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>20104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNP On-Call\\nPhysical Exam\\nInfant in open cri...</td>\n",
       "      <td>[10073, 986, 31942, 17843, 1552, 3186, 1351, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082816</td>\n",
       "      <td>2076153</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-18</td>\n",
       "      <td>2165-08-18 14:28:00</td>\n",
       "      <td>2165-08-18 14:33:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NPN 0700-1900\\n\\n\\n#1 RESP: In RA. RR 20-60, s...</td>\n",
       "      <td>[4721, 6237, 3026, 1553, 31941, 214, 1157, 319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082817</td>\n",
       "      <td>2076154</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 00:40:00</td>\n",
       "      <td>2165-08-19 00:42:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>19157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNP Physical Exam\\nAwake and alert swaddled in...</td>\n",
       "      <td>[10073, 1552, 3186, 7648, 56, 2137, 4078, 52, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082818</td>\n",
       "      <td>2076155</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 03:20:00</td>\n",
       "      <td>2165-08-19 03:31:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>20315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NPN 1900-0700\\n\\n\\nResp: Remains in RA. RR~20-...</td>\n",
       "      <td>[4721, 5274, 341, 31941, 2365, 52, 1157, 31936...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2082950</td>\n",
       "      <td>2076156</td>\n",
       "      <td>31704</td>\n",
       "      <td>110591.0</td>\n",
       "      <td>2165-08-19</td>\n",
       "      <td>2165-08-19 13:40:00</td>\n",
       "      <td>2165-08-19 13:44:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>17593.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neonatology Attending\\n\\nDOL 12 PMA 35 weeks\\n...</td>\n",
       "      <td>[23123, 2166, 5430, 248, 8526, 1462, 2055, 220...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4973 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "319840    319082        2984  124661.0  2187-03-05  2187-03-05 08:01:00   \n",
       "319996    319164        2984  124661.0  2187-03-06  2187-03-06 05:00:00   \n",
       "320269    319218        2984  124661.0  2187-03-06  2187-03-06 12:36:00   \n",
       "320275    319224        2984  124661.0  2187-03-06  2187-03-06 07:54:00   \n",
       "320276    319225        2984  124661.0  2187-03-06  2187-03-06 07:54:00   \n",
       "...          ...         ...       ...         ...                  ...   \n",
       "2082553  2076162       31704  110591.0  2165-08-21  2165-08-21 06:59:00   \n",
       "2082816  2076153       31704  110591.0  2165-08-18  2165-08-18 14:28:00   \n",
       "2082817  2076154       31704  110591.0  2165-08-19  2165-08-19 00:40:00   \n",
       "2082818  2076155       31704  110591.0  2165-08-19  2165-08-19 03:20:00   \n",
       "2082950  2076156       31704  110591.0  2165-08-19  2165-08-19 13:40:00   \n",
       "\n",
       "                   STORETIME       CATEGORY  \\\n",
       "319840   2187-03-05 08:01:31     Physician    \n",
       "319996   2187-03-06 05:00:30        Nursing   \n",
       "320269   2187-03-06 12:50:56        Nursing   \n",
       "320275   2187-03-06 14:42:31     Physician    \n",
       "320276   2187-03-06 14:47:22     Physician    \n",
       "...                      ...            ...   \n",
       "2082553  2165-08-21 07:00:00  Nursing/other   \n",
       "2082816  2165-08-18 14:33:00  Nursing/other   \n",
       "2082817  2165-08-19 00:42:00  Nursing/other   \n",
       "2082818  2165-08-19 03:31:00  Nursing/other   \n",
       "2082950  2165-08-19 13:44:00  Nursing/other   \n",
       "\n",
       "                               DESCRIPTION     CGID  ISERROR  \\\n",
       "319840   Physician Resident Admission Note  15165.0      0.0   \n",
       "319996               Nursing Progress Note  21108.0      0.0   \n",
       "320269               Nursing Transfer Note  21297.0      0.0   \n",
       "320275    Physician Resident Progress Note  14180.0      0.0   \n",
       "320276    Physician Resident Progress Note  14180.0      0.0   \n",
       "...                                    ...      ...      ...   \n",
       "2082553                             Report  20104.0      0.0   \n",
       "2082816                             Report  17147.0      0.0   \n",
       "2082817                             Report  19157.0      0.0   \n",
       "2082818                             Report  20315.0      0.0   \n",
       "2082950                             Report  17593.0      0.0   \n",
       "\n",
       "                                                      TEXT  \\\n",
       "319840   Chief Complaint:  n/v, abdominal pain, fever\\n...   \n",
       "319996   Fever (Hyperthermia, Pyrexia, not Fever of Unk...   \n",
       "320269   59 F h/o breast ca being treated with carbopla...   \n",
       "320275   Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...   \n",
       "320276   Chief Complaint:\\n   24 Hour Events:\\n TRANSTH...   \n",
       "...                                                    ...   \n",
       "2082553  NNP On-Call\\nPhysical Exam\\nInfant in open cri...   \n",
       "2082816  NPN 0700-1900\\n\\n\\n#1 RESP: In RA. RR 20-60, s...   \n",
       "2082817  NNP Physical Exam\\nAwake and alert swaddled in...   \n",
       "2082818  NPN 1900-0700\\n\\n\\nResp: Remains in RA. RR~20-...   \n",
       "2082950  Neonatology Attending\\n\\nDOL 12 PMA 35 weeks\\n...   \n",
       "\n",
       "                                                    TOKENS  \n",
       "319840   [2948, 2154, 31941, 55, 31963, 31957, 31956, 1...  \n",
       "319996   [4310, 57, 15054, 31956, 10198, 31956, 186, 43...  \n",
       "320269   [4046, 86, 68, 31963, 31922, 3384, 350, 2420, ...  \n",
       "320275   [2948, 2154, 31941, 458, 2461, 2215, 31941, 19...  \n",
       "320276   [2948, 2154, 31941, 458, 2461, 2215, 31941, 19...  \n",
       "...                                                    ...  \n",
       "2082553  [10073, 986, 31942, 17843, 1552, 3186, 1351, 5...  \n",
       "2082816  [4721, 6237, 3026, 1553, 31941, 214, 1157, 319...  \n",
       "2082817  [10073, 1552, 3186, 7648, 56, 2137, 4078, 52, ...  \n",
       "2082818  [4721, 5274, 341, 31941, 2365, 52, 1157, 31936...  \n",
       "2082950  [23123, 2166, 5430, 248, 8526, 1462, 2055, 220...  \n",
       "\n",
       "[4973 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testnotes = notes[~notes['ISERROR'].notnull() & notes['CATEGORY'].notnull() & notes['CHARTTIME'].notnull() & notes['STORETIME'].notnull()\n",
    "#                  & notes['DESCRIPTION'].notnull() & notes['HADM_ID'].notnull()]\n",
    "#testnotes['ISERROR'] = testnotes['ISERROR'].fillna(0)\n",
    "#neoplasm = testnotes[testnotes['HADM_ID'].isin(sample['HADM_ID'])]\n",
    "#neoplasm.to_csv(r'MIMIC_neoplasm_timestamped.csv')\n",
    "neoplasm = pd.read_csv(r'MIMIC_neoplasm_timestamped.csv', index_col=0)\n",
    "neoplasm['TOKENS'] = neoplasm['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "neoplasm\n",
    "# testnotes = testnotes.loc[testnotes['HADM_ID'].isin(diagnoses['HADM_ID'])]\n",
    "# ID2ICD9 = dict(zip(diagnoses['HADM_ID'].values, diagnoses['ICD9_CODE'].values))\n",
    "# testnotes['ICD9_CODE'] = testnotes['HADM_ID'].apply(lambda x:ID2ICD9[x] )\n",
    "\n",
    "# testnotes.head()\n",
    "#diagnoses[diagnoses['HADM_ID']==100247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55672    1420\n",
       "78076    1414\n",
       "77614    1351\n",
       "27427    1294\n",
       "109      1279\n",
       "         ... \n",
       "90566       1\n",
       "82956       1\n",
       "93193       1\n",
       "7119        1\n",
       "11393       1\n",
       "Name: SUBJECT_ID, Length: 46146, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notes.to_csv(\"RawNoteText.txt\", sep=\"\\n\", header=None, index=False)\n",
    "#diagnoses.loc[diagnoses['SUBJECT_ID']==diagnoses['SUBJECT_ID'][0]]\n",
    "notes['SUBJECT_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning ICD-9 codes to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6985"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ICD2binary(code):\n",
    "    '''\n",
    "    Helper function to convert ICD9 codes to binary labels based on whether or not they indicate cancer or not\n",
    "    '''\n",
    "    if(isinstance(code, str)):\n",
    "        #check for non-numeric code starts\n",
    "        if(code[0].isdigit() == False):\n",
    "            return 2\n",
    "        if(code[0]=='0'):\n",
    "            return 2\n",
    "        #only scan first 3 digits of remaining code\n",
    "        if(len(code)>3):\n",
    "            code = ast.literal_eval(code[:3])\n",
    "        else:\n",
    "            code = ast.literal_eval(code)\n",
    "        #check for non-neoplasm codes\n",
    "        if((code<140)|(code>229)):\n",
    "            \n",
    "            return 2\n",
    "        if((code>=140) & (code<=209)):\n",
    "        #return positive value for malignancies\n",
    "            return 1\n",
    "        # all else are benign \n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def patient2label(patient, df):\n",
    "    '''\n",
    "    Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "    '''\n",
    "    patient_codes = df.loc[df['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "    #print(type(patient_codes[0]))\n",
    "    cancer_codes = ['155', '1550', '1551', '1552']\n",
    "    cirrhosis_codes=['5712', '5715', '5716']\n",
    "    #0 = neither liver cancer nor cirrhosis\n",
    "    #1 = liver cancer\n",
    "    #2 = cirrhosis\n",
    "    #3 = both\n",
    "    \n",
    "    label = 0 #assume the null hypothesis is true\n",
    "    for code in patient_codes:\n",
    "        if code in cancer_codes:\n",
    "            if(label == 2):\n",
    "                return 3\n",
    "            label = 1\n",
    "        elif(code in cirrhosis_codes):\n",
    "            if(label == 1):\n",
    "                return 3\n",
    "            label = 2\n",
    "    return label\n",
    "\n",
    "string = '2172-03-14 11:00:00'\n",
    "def date2float(date):\n",
    "    #print(type(date))\n",
    "    return datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "def sortNotes(patient, df):\n",
    "    '''A helper function which marks codes each patient's ICD codes in sequence'''\n",
    "    patientdata = df.loc[df['SUBJECT_ID']==patient]\n",
    "    patientdata['time'] = patientdata['CHARTTIME'].apply(lambda x: date2float(x))\n",
    "    patientdata.sort_values('time', inplace=True)\n",
    "    return patientdata\n",
    "    #for note in patientdata['']\n",
    "    #patientdata['RAW_TIME'] = patientdata[] \n",
    "    #first organize patient codes by \n",
    "\n",
    "def diagnoses2vector(patient, df):\n",
    "    diagnoses = pd.Series(df['ICD9_CODE'].unique())\n",
    "    patientcodes = df[df['SUBJECT_ID']==patient]['ICD9_CODE'].unique()\n",
    "    patientvector = diagnoses.apply(lambda x: 1 if x in patientcodes else 0)\n",
    "    return patientvector\n",
    "\n",
    "patient = diagnoses['SUBJECT_ID'][0]\n",
    "print(diagnoses2vector(patient, diagnoses).sum())\n",
    "len(diagnoses[diagnoses['SUBJECT_ID']==patient]['ICD9_CODE'].unique())\n",
    "len(diagnoses['ICD9_CODE'].unique())\n",
    "# df = pd.DataFrame(data={'ICD9_CODE':diagnoses['ICD9_CODE'].unique(), 'VALUE':np.zeros(len(diagnoses['ICD9_CODE'].unique()))})\n",
    "# testcodes = ['40301', '486', '58281']\n",
    "\n",
    "# df['VALUE'] = df['ICD9_CODE'].apply(lambda x: 1 if str(x) in testcodes else 0)\n",
    "# df\n",
    "#diagnoses['ICD9_CODE'].apply(lambda x:1 if str(x) in testcodes else 0).value_counts()\n",
    "#str(diagnoses['ICD9_CODE'][1]) in testcodes\n",
    "#date2float('2187-03-05 08:01:00')\n",
    "    \n",
    "#neoplasm = pd.read_csv(r'MIMIC_neoplasm_timestamped.csv')\n",
    "#patients = diagnoses['SUBJECT_ID'].unique()\n",
    "#print(patients)\n",
    "# liver_labels =[patient2label(x, diagnoses) for x in patients]\n",
    "# patient_liver_labels = pd.DataFrame(data={\"SUBJECT_ID\":patients, \"LABEL\":liver_labels})\n",
    "# patient_liver_labels.to_csv(r'MIMIC_liver.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testnotes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-40726722d95c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#testnotes['LABEL'] = testnotes['ICD9_CODE'].apply(lambda x: ICD2binary(x))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtestnotes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestnotes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestnotes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LABEL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testnotes' is not defined"
     ]
    }
   ],
   "source": [
    "#testnotes['LABEL'] = testnotes['ICD9_CODE'].apply(lambda x: ICD2binary(x))\n",
    "#testnotes.head()\n",
    "subset = testnotes.loc[testnotes['LABEL']!=2]\n",
    "sample = subset.sample(frac=1, random_state=0)\n",
    "print(len(sample))\n",
    "sample.groupby('LABEL').count()['TEXT']/len(sample) * 100\n",
    "#sample.to_csv('MIMIC_neoplasms.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "#reloading previously saved .csv\n",
    "#neoplasm\n",
    "#sample = pd.read_csv('MIMIC_neoplasm_timestamped.csv', index_col=1)\n",
    "#prev = pd.read_csv('MIMIC_neoplasms.csv', index_col=0)\n",
    "# sample = sample[sample['CATEGORY']!=13]\n",
    "# sample.head()\n",
    "# sample = sample.sample(frac=1, random_state=0)\n",
    "# sample['LABEL'].value_counts()\n",
    "# test = list(sample['TEXT'].iloc[0])\n",
    "\n",
    "# sample[sample['ICD9_CODE']==1749]\n",
    "# sample\n",
    "\n",
    "#liver\n",
    "sample = pd.read_csv('MIMIC_liver.csv')\n",
    "positives = sample[sample['LABEL']!= 0]\n",
    "#positives.to_csv(r'MIMIC_liver_positives.csv', index=False)\n",
    "print(len(positives))\n",
    "negatives = sample[sample['LABEL']==0].sample(int(len(positives)/3))\n",
    "patients = pd.concat([positives, negatives]).reset_index()\n",
    "#sample.to_csv(r'MIMIC_liver_balanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2115"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TODO: Figure out how to properly split the data this time\n",
    "\n",
    "# subset = sample[sample['CHARTTIME'].notnull()]\n",
    "# patient_counts = pd.Series(subset['SUBJECT_ID'].value_counts())\n",
    "# thresholds = patient_counts[(patient_counts>20)]\n",
    "# thresholds = thresholds[thresholds <=50]#.sample(frac=.03)\n",
    "# #print(thresholds.sort_values(ascending=True))\n",
    "# subset = subset[subset['SUBJECT_ID'].isin(thresholds.index.values)]\n",
    "# print(len(thresholds))\n",
    "# print(len(subset))\n",
    "# subset['TOKENS'] = subset['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# train_data, test_data = train_test_split(thresholds.index.values, test_size=0.20, random_state=1)\n",
    "#split by the patients, then add only the subset of notes which are in train data and test data to each respectively\n",
    "#this will allow all notes in the train data to be only those which correspond to the patients\n",
    "#train_data = subset[subset['SUBJECT_ID'].isin(train_data)]\n",
    "#test_data = subset[subset['SUBJECT_ID'].isin(test_data)]\n",
    "# print(len(train_data))\n",
    "\n",
    "\n",
    "sentences = notes[notes['SUBJECT_ID'].isin(sample['SUBJECT_ID'])]\n",
    "#sentences = pd.read_csv('MIMIC_liver_tokenized.csv.gz', compression='gzip', index_col=0)\n",
    "#sentences['SUBJECT_ID'].value_counts()\n",
    "\n",
    "patient_counts = pd.Series(sentences['SUBJECT_ID'].value_counts())\n",
    "#split by the patients, then add only the subset of notes which are in train data and test data to each respectively\n",
    "#this will allow all notes in the train data to be only those which correspond to the patients\n",
    "#sentences['TOKENS'] = sentences['TEXT'].apply(lambda x:(sentence_to_padded_index_sequence(x, sp)))\n",
    "\n",
    "train_data, test_data = train_test_split(patient_counts.index.values, test_size=0.20, random_state=1)\n",
    "len(train_data)# train_data\n",
    "#train_X, train_y = train_data['TEXT'], train_data['LABEL']\n",
    "#test_X, test_y = test_data['TEXT'], test_data['LABEL']\n",
    "\n",
    "#test_data.groupby('LABEL').count()['TEXT']/len(test_data) * 100\n",
    "#print(test_data['LABEL'].value_counts())\n",
    "#train_data['LABEL'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-906d78223e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv('MIMIC_liver.csv', index_col=0)\n",
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "#     for s in sentences:\n",
    "#         word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", s.lower()))\n",
    "    sentences.apply(lambda x:  word_count.update(re.findall(r\"[\\w']+|[.,!?;]\", x.lower())))\n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n",
    "    \n",
    "vocabulary, vocab_indices = build_vocab(train_X)\n",
    "print((vocabulary)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "155102it [00:02, 65535.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# load embedding\n",
    "emb_dim = 50\n",
    "with open('vectors.txt') as f:\n",
    "    glove_embedding = []\n",
    "    words = {}\n",
    "    chars = {}\n",
    "    idx2words = {}\n",
    "    ordered_words = []\n",
    "\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        s = line.split()\n",
    "        glove_embedding.append(np.asarray(s[1:]))\n",
    "        \n",
    "        words[s[0]] = len(words)\n",
    "        idx2words[i] = s[0]\n",
    "        ordered_words.append(s[0])\n",
    "        \n",
    "# add unknown to word and char\n",
    "glove_embedding.append(np.random.rand(emb_dim))\n",
    "words[\"<UNK>\"] = len(words)\n",
    "\n",
    "# add padding\n",
    "glove_embedding.append(np.zeros(emb_dim))\n",
    "words[\"<PAD>\"] = len(words)\n",
    "\n",
    "chars[\"<UNK>\"] = len(chars)\n",
    "chars[\"<PAD>\"] = len(chars)\n",
    "\n",
    "glove_embedding = np.array(glove_embedding).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_padded_index_sequences(words, sentences, pad_length=100):\n",
    "    padded_sequences = np.zeros((len(sentences), pad_length))\n",
    "    for i, s in enumerate(sentences):\n",
    "        indices = np.ones(pad_length) * words['<PAD>']\n",
    "        # only take the first pad_length tokens\n",
    "        token_indices = np.array([words[w] if w in words else words['<UNK>'] for w in re.findall(r\"[\\w']+|[.,!?;]\", s.lower())[:pad_length]])\n",
    "        indices[:len(token_indices)] = token_indices\n",
    "        padded_sequences[i] = indices\n",
    "    return padded_sequences\n",
    "\n",
    "def sentence_to_padded_index_sequence(sentence, sp, pad_length = 100, pad_token=0):\n",
    "    sequence = sp.EncodeAsIds(sentence)\n",
    "    if(len(sequence)>pad_length):\n",
    "        sequence = sequence[:pad_length]\n",
    "    else:\n",
    "        while len(sequence) <pad_length:\n",
    "            sequence.append(pad_token)\n",
    "    return sequence\n",
    "\n",
    "#print(np.zeros((100, 2)))\n",
    "    \n",
    "#ids = sample['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "#print(sp.DecodeIds([0]))\n",
    "#test = pd.read_csv(\"vectors.txt\", sep = \" \", header=None, index_col = 0)\n",
    "#test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-c126f7c660bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# len(s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrain_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msentence_to_padded_index_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# for i in range(len(train_X)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "#old version of sentences to IDs\n",
    "#train_X = sentences_to_padded_index_sequences(words, train_data['TEXT'], 100)\n",
    "\n",
    "#test_X = sentences_to_padded_index_sequences(words, test_data['TEXT'], 100)\n",
    "# s = []\n",
    "\n",
    "# print(train_X[0])\n",
    "# print(words[\"<UNK>\"])\n",
    "# print(\"<UNK>\" in idx2words.keys())\n",
    "# print(idx2words[int(train_X[0][-1])])\n",
    "# # for word in train_X[0]:\n",
    "#     if word in idx2words:\n",
    "#         s.append(idx2words[int(word)])\n",
    "# print(s)\n",
    "# len(s)\n",
    "\n",
    "train_X = train_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "# for i in range(len(train_X)):\n",
    "#     if(len(train_X.iloc[i]) != 500):\n",
    "#         print(\"bad sequence detected at {}\".format(train_X.ilove[i]))\n",
    "test_X = test_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "print(type(train_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.78 GiB (GPU 0; 8.00 GiB total capacity; 722.70 MiB already allocated; 5.00 GiB free; 235.30 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-a3c4d1efe7c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0my_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-211-b8a62eee33ba>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m#print(\"transformer encoder: {}\".format(output.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m#print(\"decoder: {}\".format(output.device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.78 GiB (GPU 0; 8.00 GiB total capacity; 722.70 MiB already allocated; 5.00 GiB free; 235.30 MiB cached)"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "len(train_X)\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i] != truths[i]):\n",
    "        if(predictions ==0):\n",
    "            fn+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    else:\n",
    "        if(predictions == 0):\n",
    "            tn +=1\n",
    "        else:\n",
    "            tp +=1\n",
    "tpr = (tp /(tp + fn))\n",
    "\n",
    "fpr = (fp/(tn + fp))\n",
    "\n",
    "y_true = []\n",
    "y_score =[]  \n",
    "for batch in test_loader:\n",
    "    for j in batch[1]:\n",
    "        y_true.append(j.item())\n",
    "    for j in batch[0]:\n",
    "        y_score.append(model(j).max(1)[1].item())\n",
    "\n",
    "print(y_true)\n",
    "            \n",
    "print(\"True Positives: {} \\n True Negatives: {} \\n False Positive: {} \\n False Negatives:{}\".format(tp, tn, fp, fn))\n",
    "print('AUC: {}'.format(metrics.roc_auc_score(y_true, y_score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395061728395061% accurate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.801790450928382"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('AUC: {}'.format(metrics.auc(fpr, tpr)))\n",
    "from sklearn import metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(predictions)):\n",
    "    if(predictions[i]== truths[i]):\n",
    "        correct +=1\n",
    "    total+=1\n",
    "print(\"{}\".format(correct/total) + \"% accurate\")\n",
    "fpr, tpr, thresholds = metrics.roc_curve(truths, predictions)\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDatasetNeoplasm(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences.values.tolist()#.astype(int)\n",
    "        self.labels = labels.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        #print(self.sentences)\n",
    "        #print(np.array(self.sentences.iloc[key]))\n",
    "        return [np.array(self.sentences[key]), np.array(self.labels[key])]\n",
    "    \n",
    "class MIMICDatasetMultiLabel(Dataset):\n",
    "    def __init__(self, sentences, sp, diagnoses, maxlength=17):\n",
    "        #print(\"init\")\n",
    "        #sentences['TOKENS'] = sentences['TEXT'].apply(lambda x:self.sentence_to_padded_index_sequence(x, sp))\n",
    "        sentences['TIME'] = sentences['CHARTTIME'].apply(lambda x:self.date2float(x))\n",
    "        #print(sentences['HADM_ID'].iloc[0])\n",
    "        #print(\"sentence time calculated\")\n",
    "        diagnoses = diagnoses[diagnoses['ICD9_CODE'].notnull()]\n",
    "        self.all_diagnoses = pd.Series(diagnoses['ICD9_CODE'].unique())\n",
    "\n",
    "        diagnoses = diagnoses[diagnoses['HADM_ID'].isin(sentences['HADM_ID'])]\n",
    "       # print(\"about to apply diagnoses, length: {}\".format(len(diagnoses)))\n",
    "        self.sentences = sentences[['SUBJECT_ID','HADM_ID', 'TIME', 'TOKENS']] #.values.tolist()#.astype(int)\n",
    "\n",
    "        diagnoses['TIME'] = diagnoses['HADM_ID'].apply(lambda x:self.timestampDiagnoses(x))\n",
    "        #print(\"diagnoses time calculated\")\n",
    "        #print(\"diagnosis check: {}\".format(diagnoses['TIME'].iloc[0]))\n",
    "        #print(diagnoses)\n",
    "\n",
    "        self.diagnoses = diagnoses.sort_values(by=['TIME'])\n",
    "        #print(len(self.all_diagnoses))\n",
    "        self.patients = pd.Series(self.sentences['SUBJECT_ID'].unique())#pd.Series(diagnoses[diagnoses['SEQ_NUM']==maxlength]['SUBJECT_ID'].unique()) ###this should be done before passing in the notes, but just in case it isn't this will pare it down\n",
    "        #self.labels = labels.values\n",
    "        self.maxlength = maxlength\n",
    "        #print(\"ended\")\n",
    "\n",
    "    \n",
    "    def sentence_to_padded_index_sequence(self, sentence, sp, pad_length = 100, pad_token=0):\n",
    "        sequence = sp.EncodeAsIds(sentence)\n",
    "        if(len(sequence)>pad_length):\n",
    "            sequence = sequence[:pad_length]\n",
    "        else:\n",
    "            while len(sequence) <pad_length:\n",
    "                sequence.append(pad_token)\n",
    "        return sequence\n",
    "    \n",
    "    def timestampDiagnoses(self, admission):\n",
    "        #if(len(self.sentences[self.sentences['HADM_ID']==admission]['TIME']) >1):\n",
    "            #print(admission, self.sentences[self.sentences['HADM_ID']==admission]['TIME'])\n",
    "        return self.sentences[self.sentences['HADM_ID']==admission]['TIME'].values[0]\n",
    "    def diagnoses2vector(self, patient):\n",
    "        patientcodes = self.diagnoses[self.diagnoses['SUBJECT_ID']==patient]['ICD9_CODE'].unique()\n",
    "        patientvector = self.all_diagnoses.apply(lambda x: 1 if x in patientcodes else 0)\n",
    "        return patientvector\n",
    "\n",
    "\n",
    "    def patient2label(self, patient):\n",
    "        '''\n",
    "        Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "        '''\n",
    "        patient_codes = self.diagnoses.loc[self.diagnoses['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "        #print(type(patient_codes[0]))\n",
    "        cancer_codes = ['155', '1550', '1551', '1552']\n",
    "        cirrhosis_codes=['5712', '5715', '5716']\n",
    "        #0 = neither liver cancer nor cirrhosis\n",
    "        #1 = liver cancer\n",
    "        #2 = cirrhosis\n",
    "        #3 = both\n",
    "\n",
    "        label = 0 #assume the null hypothesis is true\n",
    "        for code in patient_codes:\n",
    "            if code in cancer_codes:\n",
    "                if(label == 2):\n",
    "                    return 3\n",
    "                label = 1\n",
    "            elif(code in cirrhosis_codes):\n",
    "                if(label == 1):\n",
    "                    return 3\n",
    "                label = 2\n",
    "        return label\n",
    "\n",
    "    def date2float(self, date):\n",
    "    #print(type(date))\n",
    "        \n",
    "    \n",
    "        return datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "    def sortNotes(self, patient):\n",
    "        '''A helper function which marks codes each patient's ICD codes in sequence'''\n",
    "        patientdata = self.sentences.loc[self.sentences['SUBJECT_ID']==patient]\n",
    "  #      patientdata['time'] = patientdata['CHARTTIME'].apply(lambda x: date2float(x))\n",
    "        patientdata.sort_values('TIME', inplace=True)\n",
    "        return patientdata['TOKENS'].values.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        #print(self.sentences)\n",
    "        #print(np.array(self.sentences.iloc[key]))\n",
    "        patient = self.patients[key]\n",
    "        \n",
    "        data = self.sortNotes(patient)\n",
    "        x = random.sample(data, 5) #self.maxlength]\n",
    "        x = [j for i in x for j in i]\n",
    "        #print(\"x type: {}\".format(x))\n",
    "        #y = self.diagnoses[self.diagnoses['SUBJECT_ID']==key]['ICD9_CODE']\n",
    "        y = self.diagnoses2vector(patient)\n",
    "        \n",
    "#        print(\"y type: {}\".format(y.dtypes))\n",
    "        #print(x.values)\n",
    "        return [np.array(x), np.array(y)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 434\n",
      "torch.Size([44, 1000])\n",
      "torch.Size([44])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0])\n",
      "<class 'int'>\n",
      "tensor([[ 2618,  1298, 31941,  ..., 31936,  1276, 31941],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 5918, 11697,    25,  ...,     0,     0,     0],\n",
      "        [ 2618,  1298, 31941,  ...,     0,     0,     0],\n",
      "        [ 7183, 31963,  3918,  ...,     0,     0,     0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "#BATCH_SIZE =1\n",
    "#TODO: fix to make consistent size\n",
    "\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, data, sp, label_column = \"Cirrhosis\"):\n",
    "        super(MIMICDataset, self).__init__()\n",
    "        self.data = data[['TEXT']]\n",
    "        self.labels = data[label_column]\n",
    "        self.sp = sp\n",
    "        #self.sentences = sentences[['SUBJECT_ID', 'TOKENS']]#.astype(int)\n",
    "        #self.labels = patients['LABEL']\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "    \n",
    "    def patient2label(self, patient, df, mode=\"liver\"):\n",
    "        '''\n",
    "        Helper function to convert ICD9 codes to labels based on whether or not they indicate liver cancer, cirrhosis, both, or neither\n",
    "        '''\n",
    "        if(mode==\"liver\" or mode == None):\n",
    "            patient_codes = df.loc[df['SUBJECT_ID']==patient]['ICD9_CODE']\n",
    "            #print(type(patient_codes[0]))\n",
    "            cancer_codes = ['155', '1550', '1551', '1552']\n",
    "            cirrhosis_codes=['5712', '5715', '5716']\n",
    "            #0 = neither liver cancer nor cirrhosis\n",
    "            #1 = liver cancer\n",
    "            #2 = cirrhosis\n",
    "            #3 = both\n",
    "\n",
    "            label = 0 #assume the null hypothesis is true\n",
    "            for code in patient_codes:\n",
    "                if code in cancer_codes:\n",
    "                    if(label == 2):\n",
    "                        return 3\n",
    "                    label = 1\n",
    "                elif(code in cirrhosis_codes):\n",
    "                    if(label == 1):\n",
    "                        return 3\n",
    "                    label = 2\n",
    "        elif(mode==\"sex\"):\n",
    "            label = 0 if df.loc[df['SUBJECT_ID']==patient]['GENDER'][0]=='F' else 1\n",
    "        return label\n",
    "\n",
    "    def sentence_to_padded_index_sequence(self, sentence, pad_length = 100, pad_token=0):\n",
    "        if(type(sentence) == type(\"string\")):\n",
    "            sentence = self.sp.EncodeAsIds(sentence)\n",
    "        sequence = np.array(sentence)\n",
    "        if(len(sequence)>pad_length):\n",
    "            #print(\"truncating\")\n",
    "            sequence = sequence[:pad_length]\n",
    "        else:\n",
    "            #print(\"sequence before\" + str(sequence.shape))\n",
    "\n",
    "            #print(\"not truncating\")\n",
    "            padding = np.full_like(np.zeros(pad_length - len(sequence), dtype=np.int32), pad_token)\n",
    "            #print(padding.shape)\n",
    "            sequence = np.concatenate([sequence, padding])\n",
    "        #print(type(sequence))\n",
    "#             if(pad_token == 0):\n",
    "#                 sequence = np.append(x, np.zeros((pad_length - len(x)),dtype=np.int32))\n",
    "#             else:\n",
    "#                 while len(sequence) <pad_length:\n",
    "#                     sequence.append(pad_token)\n",
    "            #print(\"sequence after: \" + str(sequence.shape))\n",
    "        return sequence\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        patient = self.patients['SUBJECT_ID'][key]\n",
    "        data = self.sentences[self.sentences['SUBJECT_ID']== patient]['TEXT'].apply(\n",
    "            lambda x:self.sentence_to_padded_index_sequence(x, pad_length = 100)).values\n",
    "        x = np.concatenate(data).tolist()\n",
    "#         x = np.array(x)\n",
    "#         x = x.repeat(int((98600-(x.shape[0]%98600))/x.shape[0]) + 1, 0)\n",
    "        #print(x.shape)\n",
    "        x = self.sentence_to_padded_index_sequence(x, pad_length = 100 * self.docNum)\n",
    "        if(x.dtype != np.int32):\n",
    "            x=x.astype(np.int32)\n",
    "        #y = np.array(self.patients[self.patients['SUBJECT_ID']==patient]['LABEL'])\n",
    "        y = int(self.patients[(self.patients['SUBJECT_ID']==patient)]['GENDER']=='F')\n",
    "        #y = np.array(y)\n",
    "        #print(x.shape)\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "#print(sample['SUBJECT_ID'].value_counts()[sample['SUBJECT_ID'].value_counts() == 5])\n",
    "        #return (np.array(self.sentences.iloc[key]), np.array(self.labels.iloc[key]))\n",
    "#model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout)\n",
    "#model = model.cuda()\n",
    "#\n",
    "#print(labels['LABEL'][1140])\n",
    "#model.train()\n",
    "##general baseline portion\n",
    "#print(notes['SUBJECT_ID'].value_counts()[notes['SUBJECT_ID'].value_counts()==5])\n",
    "#patient = patients\n",
    "#liver = pd.read_csv(\"MIMIC_liver.csv\")\n",
    "#patients = liver\n",
    "patient_doc = pd.read_csv(\"mimic/PATIENTS.csv.gz\")\n",
    "docNum = 10\n",
    "subsample = patients.loc[(patients['SUBJECT_ID'].isin(notes['SUBJECT_ID'].value_counts()[notes['SUBJECT_ID'].value_counts()<=docNum].index.values))]\n",
    "subsample = patient_doc.loc[patient_doc['SUBJECT_ID'].isin(subsample['SUBJECT_ID'].values)]\n",
    "n = 20000\n",
    "n = n if n < len(subsample) else len(subsample)\n",
    "print(\"n: {}\".format(n))\n",
    "#print(len(subsample)) q\n",
    "sample = subsample.sample(frac=n/len(subsample))[['SUBJECT_ID', 'GENDER']]\n",
    "sentences = notes[notes['SUBJECT_ID'].isin(sample['SUBJECT_ID'].values)]\n",
    "\n",
    "##neoplasm portion\n",
    "#neoplasm = pd.read_csv(\"MIMIC_neoplasmys.csv\", index_col = 0)\n",
    "#sample = neoplasm[['TEXT', 'LABEL']]\n",
    "train_data, test_data = train_test_split(sample, train_size=.9, random_state=0)\n",
    "# train_X = train_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# train_y = train_data['LABEL']\n",
    "# test_X = test_data['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "# test_y = test_data['LABEL']\n",
    "\n",
    "train_loader = DataLoader(MIMICDataset(train_data, sentences, sp, docNum=docNum), #, labels),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "test_loader = DataLoader(MIMICDataset(test_data, sentences, sp, docNum=docNum), #,labels ),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "torch.manual_seed(111)\n",
    "\n",
    "#print(x.shape)\n",
    "#print(train_loader.dataset.labels[1934])\n",
    "i=0\n",
    "for batch in test_loader:\n",
    "\n",
    "    #print(batch)\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    #model(x)\n",
    "    #model.zero_grad()\n",
    "    break\n",
    "\n",
    "    if(x.dtype != torch.int32 or y.dtype == torch.int32):\n",
    "        print(x)\n",
    "        break\n",
    "    \n",
    "    #print(i)\n",
    "    i+=1\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "xflat = x.flatten()\n",
    "xlist=[]\n",
    "for i in xflat.cpu().numpy():\n",
    "    xlist.append(i.item())\n",
    "print(type(xlist[0]))\n",
    "print(x[:, -(98600-97200):])\n",
    "#print(sp.DecodeIds(xlist))\n",
    "#print(y)\n",
    "#print(len(glove_embedding))\n",
    "\n",
    "\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "nclasses = 2\n",
    "\n",
    "#out = model(x)\n",
    "#print(out[0].shape, out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([44])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #print(pe.shape)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        \n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, nclasses, dropout=0.5, insize = 100):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        ##DOCUMENT TRUNCATOR\n",
    "       # self.input == nn.Linear(142000, 16)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.embedding = nn.Embedding(ntoken, embedding_dim)\n",
    "        \n",
    "#         ntokens = 32000\n",
    "# #print(ntokens/nhead)\n",
    "#         emsize = 40 # embedding dimension\n",
    "#         nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#         nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#         nhead = 8 # the number of heads in the multiheadattention models\n",
    "#         dropout = 0.5 # the dropout value\n",
    "#         nclasses = 4\n",
    "\n",
    "        #standard transformer\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp, scale_grad_by_freq=True)\n",
    "        \n",
    "        #self.fc = nn.Linear(ntoken * ninp, 256)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.reencoder = nn.Embedding(256, ninp)\n",
    "        \n",
    "        #self.offsets = torch.LongTensor([0, 100])\n",
    "\n",
    "        self.ninp = ninp\n",
    "        #self.recoder = nn.Sequential(nn.Linear(ninp, 4), nn.ReLU(), nn.Linear(4, 64), nn.ReLU(), nn.Linear(64,98600 ))\n",
    "        #self.recoder = nn.Linear(4944000, 100)\n",
    "        self.gap1d = nn.AvgPool1d(ninp)\n",
    "\n",
    "        #self.pos_encoder2 = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers2 = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder2 = TransformerEncoder(encoder_layers2, nlayers)\n",
    "        self.bottleneck = nn.Linear(insize, int(insize/100))\n",
    "\n",
    "        self.decoder = nn.Linear(int(insize/100), nhid)\n",
    "        self.fc = nn.Linear(nhid, nclasses * 2)\n",
    "        self.fc2 = nn.Linear(nclasses * 2, nclasses)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        #print(\"input: {}\".format(src.dtype))\n",
    "        #if(src.device == \"cpu\"):\n",
    "        ##outputs =[]\n",
    "        src = src.cuda().long()\n",
    "        \n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            #print(\"src device: {}\".format(src.dtype))\n",
    "\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "        #print(\"mask: {}\".format(src.dtype))\n",
    "#         if(self.encoder.weight.device == \"cpu\"):\n",
    "#             print(\"cpu error\")\n",
    "#         if(self.encoder.padding_idx != None):\n",
    "#             if(self.encoder.padding_idx.device != \"cuda\"):\n",
    "#                 print(self.encoder.padding_idx.device)\n",
    "        src = self.encoder(src)\n",
    "        #print(\"encoder: {}\".format(src.dtype))\n",
    "\n",
    "        src = src * math.sqrt(self.ninp)\n",
    "        #print(\"sqrt: {}\".format(src.dtype))\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        #print(\"positional encoder: {}\".format(src.shape))\n",
    "\n",
    "        t_output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(\"transformer encoder 1: {}\".format(t_output.shape))\n",
    "\n",
    "        output = torch.transpose(t_output, 2, 1)\n",
    "        output = self.bottleneck(output)\n",
    "\n",
    "        output = torch.transpose(output, 2, 1)\n",
    "\n",
    "\n",
    "        output = self.transformer_encoder2(output)\n",
    "\n",
    "        \n",
    "        #print(\"transformer encoder 2: {}\".format(output.shape))\n",
    "        #out2 = self.recoder(output)\n",
    "        \n",
    "        #NEW\n",
    "    \n",
    "        output = self.gap1d(output).squeeze(len(output.shape)-1)\n",
    "        #END NEW\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "\n",
    "        #print(\"decoder: {}\".format(output.dtype))\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        #print(output.shape)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.fc2(output)\n",
    "        #output = self.sigmoid(output)\n",
    "\n",
    "        #output = self.softmax(output)\n",
    "        return output.squeeze()#, out2\n",
    "    \n",
    "emsize = 24 # embedding dimension\n",
    "nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 1\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout, insize=docNum * 100)\n",
    "#model=RNNEncoder(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "model = model.float().cuda()\n",
    "model(x).shape\n",
    "# print(torch.cuda.is_available())\n",
    "# #model(x.to(device)).device\n",
    "\n",
    "\n",
    "# class AttnDecoderRNN(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n",
    "#         super(AttnDecoderRNN, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.output_size = output_size\n",
    "#         self.dropout_p = dropout_p\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "#         self.dropout = nn.Dropout(self.dropout_p)\n",
    "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "#     def forward(self, input, hidden, encoder_outputs):\n",
    "#         embedded = self.embedding(input).view(1, 1, -1)\n",
    "#         embedded = self.dropout(embedded)\n",
    "\n",
    "#         attn_weights = F.softmax(\n",
    "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "#                                  encoder_outputs.unsqueeze(0))\n",
    "\n",
    "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "#         output = F.relu(output)\n",
    "#         output, hidden = self.gru(output, hidden)\n",
    "\n",
    "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d718f0ae86a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;31m#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "#x = torch.Tensor(32, 100).normal_() * 155103/3\n",
    "#x = x.long()\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"A single attention head\"\"\"\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # We will assume the queries, keys, and values all have the same feature size\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.query_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.key_tfm = nn.Linear(d_model, d_feature)\n",
    "        self.value_tfm = nn.Linear(d_model, d_feature)\n",
    " \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.query_tfm(queries) # (Batch, Seq, Feature)\n",
    "        K = self.key_tfm(keys) # (Batch, Seq, Feature)\n",
    "        V = self.value_tfm(values) # (Batch, Seq, Feature)\n",
    "        # compute multiple attention weighted sums\n",
    "        x = self.attn(Q, K, V)\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"The full multihead attention block\"\"\"\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        # in practice, d_model == d_feature * n_heads\n",
    "        assert d_model == d_feature * n_heads\n",
    " \n",
    "        # Note that this is very inefficient:\n",
    "        # I am merely implementing the heads separately because it is \n",
    "        # easier to understand this way\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model, d_feature, dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_feature * n_heads, d_model) \n",
    "     \n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        log_size(queries, \"Input queries\")\n",
    "        x = [attn(queries, keys, values, mask=mask) # (Batch, Seq, Feature)\n",
    "             for i, attn in enumerate(self.attn_heads)]\n",
    "         \n",
    "        # reconcatenate\n",
    "        x = torch.cat(x, dim=Dim.feature) # (Batch, Seq, D_Feature * n_heads)\n",
    "        log_size(x, \"concatenated output\")\n",
    "        x = self.projection(x) # (Batch, Seq, D_Model)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM', k=2):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.conv = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=k)\n",
    "        self.pool = nn.MaxPool1d(1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN', 'ATTN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        #self.attn = \n",
    "        self.linear = nn.Linear(hidden_dim, 1000)\n",
    "        self.fc = nn.Linear(1000, output_dim)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state values\n",
    "        \"\"\"\n",
    "        hidden = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        #print(hidden)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            c = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "            \n",
    "            return hidden, c\n",
    "        #if(self.rnn_fn =='ATTN')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #if([x.shape[0], x.shape[1]] != [32, 100]):\n",
    "            #print(x.shape)\n",
    "        x = x.to(device)\n",
    "\n",
    "        #print(x.shape, \" input\")\n",
    "        x = self.emb(x.long())\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(x.shape, \" permute 1\")\n",
    "        #x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        #print(x.shape, \" conv\")\n",
    "\n",
    "        #x = self.pool(x)\n",
    "        #print(x.shape, \" pool\")\n",
    "\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        #x = x.squeeze()\n",
    "        #print(x.shape, \" permute 2\") \n",
    "\n",
    "\n",
    "       # print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1, self.embedding_dim)\n",
    "        #self.hidden_dim = 50\n",
    "        #self.rnn.hidden_dim = self.hidden_dim\n",
    "        #self.rnn.input_dim = self.hidden_dim\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(x.shape, \" embedding\")\n",
    "        _, last_hidden = self.rnn(x, self.init_hidden(x.shape[0]))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            last_hidden = last_hidden[0]\n",
    "       # print(last_hidden.shape, \" memory\")\n",
    "        last_hidden = self.linear(last_hidden)\n",
    "        last_hidden = self.relu(last_hidden)\n",
    "        out = self.fc(last_hidden)\n",
    "        out = self.relu(out)\n",
    "        out = out.squeeze()\n",
    "        if(len(out.shape)==1):\n",
    "            out = out.unsqueeze()\n",
    "\n",
    "       # print(out.shape, \" output\")\n",
    "        return out\n",
    "    \n",
    "#print(x)\n",
    "model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#print(model(x))\n",
    "# torch.Size([32, 100])  input\n",
    "# torch.Size([32, 100, 50])  embedding\n",
    "# torch.Size([1, 32, 40])  memory\n",
    "# torch.Size([1, 32, 2])  output\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cecd6bc72d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#print(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNNEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'LSTM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "#### DUPLICATE WITHOUT CNN\n",
    "### RETOOLED FOR ATTENTION\n",
    "# x = torch.Tensor(32, 100).normal_() * 155103/3\n",
    "# x = x.long()\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM'):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.e,ed\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN', 'ATTN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        #self.rnn = nnnlp.Attention()\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state values\n",
    "        \"\"\"\n",
    "        hidden = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim)).to(device)\n",
    "        #print(hidden)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            c = nn.Parameter(torch.zeros(1, batch_size, self.hidden_dim)).to(device)\n",
    "            \n",
    "            return hidden, c\n",
    "        return hidden\n",
    "        #if(self.rnn_fn =='ATTN')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #if([x.shape[0], x.shape[1]] != [32, 100]):\n",
    "            #print(x.shape)\n",
    "        x = x.cuda().long()\n",
    "\n",
    "        #print(x.shape, \" input\")\n",
    "        x = self.emb(x)\n",
    "        #print(x.shape, \" emb\")\n",
    "        #x = x.unsqueeze(1)\n",
    "        #x= self.conv(x)\n",
    "        #x = self.pool(x)\n",
    "        #x= self.relu(x)\n",
    "        #x = x.squeeze()\n",
    "        #print(x.shape)\n",
    "        \n",
    "\n",
    "       # print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1, 32)\n",
    "        #self.hidden_dim = 32\n",
    "        #self.rnn.hidden_dim = self.hidden_dim\n",
    "        #self.rnn.input_dim = self.hidden_dim\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(x.shape, \" embedding\")\n",
    "        out, last_hidden = self.rnn(x, self.init_hidden(x.shape[0]))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            last_hidden = last_hidden[0]\n",
    "        #print(last_hidden.shape, \" memory\")\n",
    "        if(len(last_hidden.shape)>2):\n",
    "            last_hidden = last_hidden.squeeze(0)\n",
    "        \n",
    "    \n",
    "        out = self.fc(last_hidden)\n",
    "        #print(out.shape, \" output\")\n",
    "        return out#, last_hidden\n",
    "    \n",
    "#print(x)\n",
    "model = RNNEncoder(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#print(model(x))\n",
    "# torch.Size([32, 100])  input\n",
    "# torch.Size([32, 100, 50])  embedding\n",
    "# torch.Size([1, 32, 40])  memory\n",
    "# torch.Size([1, 32, 2])  output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test = 0\n",
    "def train(model, train_loader, test_loader, \n",
    "          learning_rate=0.005, num_epoch=10, print_every=100):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    best_auc = 0\n",
    "    loss_fn = nn.BCELoss()\n",
    "    #loss_fn2 = nn.CrossEntropyLoss()\n",
    "    acc = 0\n",
    "    accs =[]\n",
    "    aucs = []\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print('Beginning training for {} epochs'.format(num_epoch))\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        \n",
    "        #print(\"Beginning Epoch {}\".format(str(epoch +1)))\n",
    "        #for i, (data, labels) in tqdm(enumerate(train_loader)):\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "\n",
    "            #print(i, sep=' ', end='', flush=True) \n",
    "            \n",
    "            #print(\"batch {}, data shape {P} \".format(i, data.shape))\n",
    "            labels = labels.to(device)\n",
    "            data = data.long().to(device)\n",
    "            outputs = model(data)\n",
    "            \n",
    "            model.zero_grad()\n",
    "#             if(outputs.shape[0] != 8 or outputs.shape[1]!= 2):\n",
    "#                 print(outputs.shape, labels.shape)\n",
    "            #print(\"outputs: {} labels: {}\".format(outputs.shape, labels.shape))\n",
    "\n",
    "#            outputs = model(data)\n",
    "            if(len(outputs.shape)<2):\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "#             if(len(outputs.shape)<2):\n",
    "#                 output2 = outputs[1].unsqueeze(0)\n",
    "            #print(outputs.shape, labels.shape)\n",
    "\n",
    "            loss = loss_fn(outputs.float(), labels.float())\n",
    "#             loss2 = loss_fn2(output2, data)\n",
    "            loss.backward()\n",
    "            #loss2.backward()\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "#             if(y.shape[0]!=8):\n",
    "#                 print(y.shape)\n",
    "\n",
    "             # report performance\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d}/{} | {:6d}/{:6d} batches | Loss: {:6.4f} |Elapsed Time: {:>9}'.format(\n",
    "                    epoch + 1, num_epoch, i + 1, len(train_loader), loss.item(), time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))))     \n",
    "#                 print('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4}, Validation Acc:{5}, AUC:{6}'.format(\n",
    "#                     epoch + 1, EPOCHS, i + 1, len(train_loader), loss.data[0], test_acc, test_auc))\n",
    "        \n",
    "    # Evaluate after every epoch\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        full_output = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, labels) in enumerate(test_loader):\n",
    "                #print(data.shape)\n",
    "                #random.sample(list(train_data), int(len(train_data)/400))\n",
    "                output = model(data)\n",
    "                while(len(output.shape)<2):\n",
    "                    output = output.unsqueeze(0)\n",
    "                #print(output)\n",
    "                output = nn.Softmax(dim=1)(output)\n",
    "#                 if(len(outputs.shape)<2):\n",
    "#                     output2 = outputs[1].unsqueeze(0)\n",
    "#                 import ipdb; ipdb.set_trace()\n",
    "#                 predicted = ((outputs > 0.5).long()).view(-1)\n",
    "                #print(outputs.data.max(1))\n",
    "                #pred = outputs.data.max(1)[1]\n",
    "                pred = torch.max(output, 1)[1]\n",
    "                #pred = torch.round(output)\n",
    "                #print(pred.shape)\n",
    "                full_output += list(output.cpu().numpy())\n",
    "                predictions += list(pred.cpu().numpy())\n",
    "                truths += list(labels.cpu().numpy().flatten())\n",
    "                total += labels.size(0)\n",
    "                correct += (pred.cpu() == labels.squeeze().long().cpu()).sum()\n",
    "#                 print(total, correct)\n",
    "#                 print(pred.cpu(), labels.long().cpu())\n",
    "                \n",
    "            acc = (100.0 * correct / total)\n",
    "            accs.append(acc)\n",
    "            losses.append(loss)\n",
    "            ###not compatible with multiclass\n",
    "            #print(truths)\n",
    "            #output_total = np.array(output_total)\n",
    "            #print(output_total)\n",
    "            \n",
    "            #output_total = np.concatenate((output_total))\n",
    "            #print(output_total.shape)\n",
    "            #fpr, tpr, thresholds\n",
    "            #print(truths, predictions)\n",
    "            full_output = np.concatenate(full_output).tolist()\n",
    "            #print(full_output, truths)\n",
    "            \n",
    "#             full_output = np.array(full_output)\n",
    "#             print(full_output.shape, truths)\n",
    "\n",
    "            #             full_output = np.concatenate(full_output, axis=1)\n",
    "#             print(full_output)\n",
    "#             print(full_output.shape)\n",
    "            auc = metrics.roc_auc_score(truths, full_output)#, multi_class = \"ovo\")\n",
    "            aucs.append(auc)\n",
    "#             #print(test)\n",
    "#             #auc = metrics.auc(fpr, tpr)\n",
    "            if(auc > best_auc):\n",
    "                best_auc = auc\n",
    "                best_model_wts = model.state_dict()\n",
    "#             if(acc > best_acc):\n",
    "#                 best_acc = acc\n",
    "#                 best_model_wts = model.state_dict()\n",
    "\n",
    "            cm = \"\"#metrics.confusion_matrix(truths, predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "#             print('Test set | Epoch: {} | Accuracy: {:6.4f} | AUC: {:4.2f} | time elapse: {:>9}'.format(\n",
    "#                epoch + 1, acc, auc, elapse))\n",
    "            print('Test set | Epoch: {} | Accuracy: {:6.4f} | AUC: {} | Last Loss: {} | Time elapsed: {:>9} | Sample output: {}'.format(\n",
    "               epoch + 1, acc, auc, loss.item(), elapse, outputs))\n",
    "            #print(cm)\n",
    "            \n",
    "\n",
    "            #print('Test set | Epoch: {} | Accuracy: {:4.2f} | Time Elapsed: {:>9}'.format(epoch+1, \n",
    "             #   acc, elapse))\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, predictions, truths, accs, losses, aucs ##save this for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76327    796\n",
       "70516    772\n",
       "32425    574\n",
       "50822    448\n",
       "74701    369\n",
       "        ... \n",
       "4513       2\n",
       "5773       2\n",
       "10986      2\n",
       "21852      1\n",
       "92324      1\n",
       "Name: SUBJECT_ID, Length: 529, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes[notes['SUBJECT_ID'].isin(test_data)]['SUBJECT_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-137-5eb18229d570>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-137-5eb18229d570>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    i+=1\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "d = {}\n",
    "token2word = {}\n",
    "print(sp.DecodeIds([9016]))\n",
    "for miss in misses:\n",
    "#     if(i>0):\n",
    "#         break\n",
    "\n",
    "    for token in miss[0]:\n",
    "        if(sp.DecodeIds([token.item()]) not in d.keys()):\n",
    "            d[sp.DecodeIds([token.item()])] = 1\n",
    "        else:\n",
    "            d[sp.DecodeIds([token.item()])]+=1\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of inaccurate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_X = sample['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "sample_loader = DataLoader(MIMICDataset(sample_X, sample['LABEL']),\n",
    "                          batch_size=1,\n",
    "                          shuffle=True)\n",
    "\n",
    "corrects = []\n",
    "misses = []\n",
    "missed_labels = []\n",
    "correct_labels = []\n",
    "for batch in sample_loader:\n",
    "    data = batch[0]\n",
    "    label = batch[1]\n",
    "    output = model(data)\n",
    "    pred = output.max(1)[1]\n",
    "    if(pred.cpu() != label.cpu()):\n",
    "        misses.append(data)\n",
    "        missed_labels.append(label)\n",
    "    else:\n",
    "        corrects.append(data)\n",
    "        correct_labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-728e8aadd972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LABEL\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#df.to_csv(r'misses_2-24-20-100_epochs_32k.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecodeIds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     print(sp.DecodeIds(list(misses[i].tolist()[0])))\n",
    "#     print(missed_labels[i])\n",
    "d = {}\n",
    "misses_str = []\n",
    "#print(sp.DecodeIds(misses[0].tolist()[0]))\n",
    "for i in range(len(misses)):\n",
    "    d[sp.DecodeIds(misses[i].tolist()[0])] = missed_labels[i].cpu().item()\n",
    "    misses_str.append(sp.DecodeIds(misses[i].tolist()[0]))\n",
    "df = pd.DataFrame(columns = (\"TEXT\", \"LABEL\"), data=zip(d.keys(), d.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating similarity of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "def pad(s, maxlen):\n",
    "    length = maxlen - len(s)\n",
    "    return np.concatenate([s, [0] * length], axis=None)\n",
    "\n",
    "print(len(sentences[0]))\n",
    "print(len(pad(sentences[0], maxlen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f054326383a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "misses = pd.read_csv(r'misses_2-24-20-100_epochs_32k.csv')\n",
    "#print(misses)\n",
    "sentences = misses['TEXT'].apply(lambda x: sentence_to_padded_index_sequence(x, sp))\n",
    "\n",
    "maxlen = 0\n",
    "for sentence in sentences:\n",
    "    if(len(sentence)> maxlen):\n",
    "        maxlen = len(sentence)\n",
    "#vectors = sentences.apply(lambda x:pad(x, maxlen))\n",
    "base = []\n",
    "i = 0\n",
    "for vector in vectors:\n",
    "    base.append(vector)\n",
    "    \n",
    "base = np.array(base)\n",
    "cosine = pairwise.cosine_similarity(base)\n",
    "\n",
    "\n",
    "sns.clustermap(cosine, col_cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "a = [1, 2]\n",
    "b = [[1, 2], 1]\n",
    "sample['IDs'] = ids\n",
    "sample['MISSED'] = np.zeros(len(sample))\n",
    "#ids = sample['TEXT']#.apply(lambda x: sp.EncodeAsIds(x))\n",
    "#overlap = pd.Series(list(set(sentences) & set(ids)))\n",
    "\n",
    "#for sentence in sentences.values.tolist():\n",
    "for i in range(len(sample)):\n",
    "    if(sample['IDs'].values[i] in sentences.values.tolist()):\n",
    "        sample['MISSED'][i] = 1\n",
    "missed = sample.loc[sample['MISSED']==1]\n",
    "missed.to_csv(r'missed_expanded.csv', index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15\n",
      "Name: LABEL, dtype: int64\n",
      "0    14\n",
      "Name: LABEL, dtype: int64\n",
      "1    14\n",
      "Name: LABEL, dtype: int64\n",
      "0    11\n",
      "Name: LABEL, dtype: int64\n",
      "0    10\n",
      "Name: LABEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(missed['SUBJECT_ID'].value_counts())\n",
    "missed[missed['SUBJECT_ID']>="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1275\n",
       "0     523\n",
       "3     196\n",
       "1     121\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-479a3901d399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Binarize the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "n_classes = 6984\n",
    "# Binarize the output\n",
    "y_score = label_binarize(predictions, classes=np.arange(n_classes))\n",
    "y_test = label_binarize(truths, classes=np.arange(n_classes))\n",
    "\n",
    "print(y_test.shape)\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute macro-average ROC curve and ROC area\n",
    "lw = 2\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', ])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='AUC of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0      2.864725\n",
       "1      2.199209\n",
       "2     10.039456\n",
       "3     50.217645\n",
       "4      6.784873\n",
       "5      0.259714\n",
       "6      0.045767\n",
       "7      1.522407\n",
       "8      0.451425\n",
       "9      0.395525\n",
       "10     0.127792\n",
       "11     0.004850\n",
       "12     0.004706\n",
       "13    25.081905\n",
       "Name: TEXT, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testnotes= testnotes.sample(frac=0.05, random_state=0)\n",
    "#testnotes['CATEGORY'].unique()\n",
    "\n",
    "testnotes.groupby('CATEGORY').count()['TEXT']/len(testnotes) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training for 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 1 | Accuracy: 0.0000 | AUC: 0.45684210526315794 | Last Loss: 0.81988924741745 | Time elapsed:  00:00:10 | Sample output: tensor([[0.3676, 0.3676, 0.3676, 0.3676, 0.3676, 0.3676]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 2 | Accuracy: 0.0000 | AUC: 0.35473684210526313 | Last Loss: 0.6397646069526672 | Time elapsed:  00:00:21 | Sample output: tensor([[0.3720, 0.3720, 0.3720, 0.3720, 0.3720, 0.3720]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 3 | Accuracy: 0.0000 | AUC: 0.34736842105263155 | Last Loss: 0.8990545272827148 | Time elapsed:  00:00:32 | Sample output: tensor([[0.3733, 0.3733, 0.3733, 0.3733, 0.3733, 0.3733]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 4 | Accuracy: 0.0000 | AUC: 0.3989473684210526 | Last Loss: 0.5572531223297119 | Time elapsed:  00:00:42 | Sample output: tensor([[0.3774, 0.3774, 0.3774, 0.3774, 0.3774, 0.3774]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 5 | Accuracy: 0.0000 | AUC: 0.5536842105263158 | Last Loss: 0.5569936037063599 | Time elapsed:  00:00:53 | Sample output: tensor([[0.3771, 0.3771, 0.3770, 0.3771, 0.3771, 0.3771]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 6 | Accuracy: 0.0000 | AUC: 0.40210526315789463 | Last Loss: 0.7260100245475769 | Time elapsed:  00:01:04 | Sample output: tensor([[0.3739, 0.3739, 0.3739, 0.3739, 0.3739, 0.3739]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 7 | Accuracy: 0.0000 | AUC: 0.48 | Last Loss: 0.6399948596954346 | Time elapsed:  00:01:15 | Sample output: tensor([[0.3736, 0.3735, 0.3735, 0.3735, 0.3736, 0.3735]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 8 | Accuracy: 0.0000 | AUC: 0.29894736842105263 | Last Loss: 0.6397610306739807 | Time elapsed:  00:01:26 | Sample output: tensor([[0.3722, 0.3722, 0.3722, 0.3722, 0.3722, 0.3722]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 9 | Accuracy: 0.0000 | AUC: 0.5726315789473684 | Last Loss: 0.5509336590766907 | Time elapsed:  00:01:36 | Sample output: tensor([[0.3702, 0.3702, 0.3704, 0.3703, 0.3703, 0.3704]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 10 | Accuracy: 0.0000 | AUC: 0.52 | Last Loss: 0.6390456557273865 | Time elapsed:  00:01:47 | Sample output: tensor([[0.3677, 0.3678, 0.3676, 0.3675, 0.3676, 0.3677]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 11 | Accuracy: 0.0000 | AUC: 0.52 | Last Loss: 0.7307631969451904 | Time elapsed:  00:01:58 | Sample output: tensor([[0.3652, 0.3654, 0.3655, 0.3655, 0.3659, 0.3656]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 12 | Accuracy: 0.0000 | AUC: 0.4421052631578947 | Last Loss: 0.45668888092041016 | Time elapsed:  00:02:08 | Sample output: tensor([[0.3666, 0.3659, 0.3668, 0.3674, 0.3660, 0.3669]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 13 | Accuracy: 0.0000 | AUC: 0.5242105263157895 | Last Loss: 0.5438287854194641 | Time elapsed:  00:02:19 | Sample output: tensor([[0.3641, 0.3639, 0.3627, 0.3617, 0.3632, 0.3619]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 14 | Accuracy: 0.0000 | AUC: 0.47789473684210526 | Last Loss: 0.5380451083183289 | Time elapsed:  00:02:30 | Sample output: tensor([[0.3582, 0.3573, 0.3552, 0.3560, 0.3535, 0.3588]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 15 | Accuracy: 0.0000 | AUC: 0.5957894736842105 | Last Loss: 0.6266001462936401 | Time elapsed:  00:02:40 | Sample output: tensor([[0.3546, 0.3540, 0.3415, 0.3603, 0.3453, 0.3456]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 16 | Accuracy: 0.0000 | AUC: 0.48210526315789465 | Last Loss: 0.6266546249389648 | Time elapsed:  00:02:51 | Sample output: tensor([[0.3508, 0.3357, 0.3284, 0.3600, 0.3283, 0.3461]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 17 | Accuracy: 0.0000 | AUC: 0.6484210526315789 | Last Loss: 0.8079476356506348 | Time elapsed:  00:03:01 | Sample output: tensor([[0.2962, 0.3433, 0.4093, 0.3644, 0.2934, 0.3828]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 18 | Accuracy: 0.0000 | AUC: 0.5642105263157894 | Last Loss: 0.579710066318512 | Time elapsed:  00:03:12 | Sample output: tensor([[0.4586, 0.3441, 0.5103, 0.2872, 0.3088, 0.2264]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 19 | Accuracy: 0.0000 | AUC: 0.5978947368421053 | Last Loss: 0.510249137878418 | Time elapsed:  00:03:22 | Sample output: tensor([[0.1796, 0.6196, 0.3627, 0.2216, 0.2423, 0.6009]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([1, 6])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Epoch: 20 | Accuracy: 0.0000 | AUC: 0.6147368421052631 | Last Loss: 0.2911659777164459 | Time elapsed:  00:03:33 | Sample output: tensor([[0.7015, 0.1811, 0.6233, 0.0481, 0.5804, 0.1188]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([1, 64])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "reduce failed to synchronize: cudaErrorAssert: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-439-e28ce83603d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding).to(device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maucs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-434-7b309e6b238f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, test_loader, learning_rate, num_epoch, print_every)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;31m#print(outputs.shape, labels.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;31m#             loss2 = loss_fn2(output2, data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[1;32m-> 2065\u001b[1;33m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: reduce failed to synchronize: cudaErrorAssert: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "##TODO: FIX\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "CUDA_LAUNCH_BLOCKING=0\n",
    "BATCH_SIZE = 32\n",
    "#random.sample(list(train_data), int(len(train_data) * 0.05))\n",
    "#random.sample(list(train_data), int(len(train_data)/400))\n",
    "# train_loader = DataLoader(MIMICDataset(train_data, sentences, labels, sp),\n",
    "#                           batch_size=BATCH_SIZE,\n",
    "#                           shuffle=True)\n",
    "# test_loader = DataLoader(MIMICDataset(test_data,  sentences, labels, sp),\n",
    "#                           batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "torch.manual_seed(111)\n",
    "# glove_model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM', k=2)\n",
    "# glove_model = glove_model.to(device)\n",
    "\n",
    "#ntokens = len(words)# the size of vocabulary\n",
    "ntokens = 32000\n",
    "#print(ntokens/nhead)\n",
    "emsize = 24 # embedding dimension\n",
    "nhid = 50 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "nclasses = 1\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nclasses, dropout, insize=docNum * 100)\n",
    "#model=RNNEncoder(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "#model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding))\n",
    "\n",
    "model = model.float().cuda()\n",
    "\n",
    "#model.encoder.weight.data.copy_(torch.from_numpy(glove_embedding).to(device))\n",
    "\n",
    "model, predictions, truths, accs, losses, aucs = train(model, train_loader=train_loader, test_loader=test_loader, learning_rate = 0.1e-2,num_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 4\n",
      "Parameter: GPU pinned 4 × 32\n",
      "Parameter: GPU pinned 32\n",
      "Parameter: GPU pinned 32 × 16000000\n",
      "Parameter: GPU pinned 32000\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 120 × 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Series.data is deprecated and will be removed in a future version\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Index.data is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: GPU pinned 5000 × 1 × 40\n",
      "Tensor: GPU pinned 8 × 8\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 120 × 40\n",
      "Tensor: GPU pinned 120\n",
      "Tensor: GPU pinned 40 × 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 200 × 40\n",
      "Tensor: GPU pinned 200\n",
      "Tensor: GPU pinned 40 × 200\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000 × 40\n",
      "Tensor: GPU pinned 32000\n",
      "Tensor: GPU pinned 32 × 16000000\n",
      "Tensor: GPU pinned 8 × 500\n",
      "Tensor: GPU pinned 8\n",
      "Tensor: GPU pinned 8 × 4\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 32 × 16000000\n",
      "Tensor: GPU pinned 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: RangeIndex.data is deprecated and will be removed in a future version\n",
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: FutureWarning: Int64Index.data is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000 × 40\n",
      "Parameter: GPU pinned 32000\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 120 × 40\n",
      "Parameter: GPU pinned 120\n",
      "Parameter: GPU pinned 200 × 40\n",
      "Parameter: GPU pinned 200\n",
      "Parameter: GPU pinned 40 × 200\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Parameter: GPU pinned 40 × 40\n",
      "Parameter: GPU pinned 40\n",
      "Tensor: GPU pinned 5000 × 1 × 40\n",
      "Total size: 1546987949\n"
     ]
    }
   ],
   "source": [
    "#subset, k=3\n",
    "# Test set | Epoch: 0 | Accuracy: 59.0000 | time elapse:  00:00:11\n",
    "# Test set | Epoch: 1 | Accuracy: 52.0000 | time elapse:  00:00:22\n",
    "# Test set | Epoch: 2 | Accuracy: 52.0000 | time elapse:  00:00:34\n",
    "# Test set | Epoch: 3 | Accuracy: 66.0000 | time elapse:  00:00:45\n",
    "# Test set | Epoch: 4 | Accuracy: 69.0000 | time elapse:  00:00:55\n",
    "# Test set | Epoch: 5 | Accuracy: 58.0000 | time elapse:  00:01:04\n",
    "# Test set | Epoch: 6 | Accuracy: 71.0000 | time elapse:  00:01:14\n",
    "# Test set | Epoch: 7 | Accuracy: 72.0000 | time elapse:  00:01:23\n",
    "# Test set | Epoch: 8 | Accuracy: 70.0000 | time elapse:  00:01:33\n",
    "# Test set | Epoch: 9 | Accuracy: 64.0000 | time elapse:  00:01:43\n",
    "torch.cuda.is_available()\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "#k=4\n",
    "# Train set | epoch:   1/10 |    100/   237 batches | Loss: 0.7166\n",
    "# Train set | epoch:   1/10 |    200/   237 batches | Loss: 0.3979\n",
    "# Test set | Epoch: 1 | Accuracy: 74.0000 | time elapse:  00:00:08\n",
    "# Train set | epoch:   2/10 |    100/   237 batches | Loss: 0.3456\n",
    "# Train set | epoch:   2/10 |    200/   237 batches | Loss: 0.4418\n",
    "# Test set | Epoch: 2 | Accuracy: 80.0000 | time elapse:  00:00:17\n",
    "# Train set | epoch:   3/10 |    100/   237 batches | Loss: 0.0871\n",
    "# Train set | epoch:   3/10 |    200/   237 batches | Loss: 0.1620\n",
    "# Test set | Epoch: 3 | Accuracy: 80.0000 | time elapse:  00:00:26\n",
    "# Train set | epoch:   4/10 |    100/   237 batches | Loss: 0.0428\n",
    "# Train set | epoch:   4/10 |    200/   237 batches | Loss: 0.0070\n",
    "# Test set | Epoch: 4 | Accuracy: 77.0000 | time elapse:  00:00:35\n",
    "# Train set | epoch:   5/10 |    100/   237 batches | Loss: 0.0042\n",
    "# Train set | epoch:   5/10 |    200/   237 batches | Loss: 0.0432\n",
    "# Test set | Epoch: 5 | Accuracy: 78.0000 | time elapse:  00:00:44\n",
    "# Train set | epoch:   6/10 |    100/   237 batches | Loss: 0.0078\n",
    "# Train set | epoch:   6/10 |    200/   237 batches | Loss: 0.0751\n",
    "# Test set | Epoch: 6 | Accuracy: 80.0000 | time elapse:  00:00:54\n",
    "# Train set | epoch:   7/10 |    100/   237 batches | Loss: 0.0047\n",
    "# Train set | epoch:   7/10 |    200/   237 batches | Loss: 0.0414\n",
    "# Test set | Epoch: 7 | Accuracy: 79.0000 | time elapse:  00:01:03\n",
    "# Train set | epoch:   8/10 |    100/   237 batches | Loss: 0.0024\n",
    "# Train set | epoch:   8/10 |    200/   237 batches | Loss: 0.0015\n",
    "# Test set | Epoch: 8 | Accuracy: 78.0000 | time elapse:  00:01:12\n",
    "# Train set | epoch:   9/10 |    100/   237 batches | Loss: 0.0012\n",
    "# Train set | epoch:   9/10 |    200/   237 batches | Loss: 0.0069\n",
    "# Test set | Epoch: 9 | Accuracy: 78.0000 | time elapse:  00:01:20\n",
    "# Train set | epoch:  10/10 |    100/   237 batches | Loss: 0.0131\n",
    "# Train set | epoch:  10/10 |    200/   237 batches | Loss: 0.0016\n",
    "# Test set | Epoch: 10 | Accuracy: 79.0000 | time elapse:  00:01:29\n",
    "\n",
    "#torch.cuda.max_memory_allocated(0)#-torch.cuda.memory_allocated(0)\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()\n",
    "\n",
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)\n",
    "\n",
    "dump_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "# import os\n",
    "# import torch\n",
    "# import time\n",
    "# import sys\n",
    "exp_truths, exp_preds = [0, 3, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 3, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 3, 2, 0, 2, 3, 2, 2, 2, 0, 3, 0, 0, 2, 1, 3, 2, 2, 2, 2, 3, 0, 0, 2, 2, 0, 0, 0, 0, 2, 2, 2, 3, 2, 2, 0, 2, 2, 3, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 0, 3, 2, 2, 2, 2, 3, 1, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 1, 2, 0, 2, 3, 2, 2, 2, 2, 1, 0, 0, 2, 2, 3, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 3, 0, 1, 3, 1, 1, 2, 2, 1, 2, 0, 0, 2, 0, 1, 0, 2, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 3, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 3, 0, 0, 2, 2, 1, 2, 2, 2, 0, 3, 3, 0, 0, 2, 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 0, 2, 3, 3, 3, 2, 3, 3, 1, 1, 0, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 3, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 3, 2, 2, 3, 3, 2, 0, 0, 0, 2, 3, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 3, 3, 2, 1, 2, 1, 1, 2, 2, 3, 1, 2, 0, 0, 2, 2, 2, 2, 2, 0, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 0, 3, 2, 1, 0, 0, 0, 2, 3, 2, 2, 0, 2, 2, 2, 2, 3, 2, 0, 0, 3, 0, 0, 2, 0, 3, 2, 3, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 1, 0, 2, 0, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 3, 0, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 3, 2, 2, 2, 2, 2, 0, 0, 2, 3, 2, 3, 1, 0, 1, 3, 0, 0, 0, 3, 2, 0, 1, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 3, 2, 2, 0, 0, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 3, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 3, 0, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 1, 0, 3, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 1, 0, 2, 0, 2, 0, 0, 3, 2, 0, 3, 2, 2, 1, 1, 0, 2, 0, 2, 2, 2, 3, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 3, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 3, 0, 1, 0, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 3, 3, 0, 2, 2, 0, 2, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2, 3, 2, 0, 0, 3, 2, 0, 2, 2, 2, 2, 1, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 2, 0, 2, 2, 0, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 3, 3, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 3, 0, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 1, 2, 3, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 3, 2, 3, 0, 2, 2, 0, 2, 0, 2, 2, 2, 3, 3, 0, 0, 0, 3, 2, 1, 2, 2, 2, 0, 2, 3, 0, 2, 2, 2, 2, 2, 2, 3, 2, 3, 3, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 0, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 3, 0, 2, 2, 2, 2, 0, 2, 1, 0, 0, 2, 3, 3, 3, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 3, 3, 2, 3, 2, 0, 2, 2, 2, 0, 2, 1, 3, 3, 2, 1, 2, 2, 3, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 2, 0, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 0, 2, 2, 3, 3, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 0, 0, 3, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 1, 2, 2, 3, 0, 2, 0, 3, 0, 3, 2, 2, 2, 3, 2, 2, 0, 3, 1, 2, 3, 3, 0, 3, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 3, 2, 2, 0, 2, 0, 2, 2, 2, 0, 3, 2, 2, 0, 2, 1, 3, 1, 2, 2, 3, 0, 3, 0, 3, 2, 2, 3, 2, 2, 2, 3, 0, 2, 2, 2, 2, 1, 3, 3, 2, 0, 0, 2, 3, 2, 2, 2, 1, 2, 2, 2, 3, 3, 0, 2, 2, 0, 0, 2, 1, 3, 2, 3, 0, 2, 3, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 3, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 2, 0, 2, 0, 3, 2, 3, 0, 2, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 0, 1, 3, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 3, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 1, 2, 0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 0, 3, 2, 0, 1, 3, 3, 3, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 3, 1, 2, 2, 2, 0, 2, 0, 0, 3, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 1, 0, 2, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 0, 2, 0, 3, 3, 1, 0, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 2, 2, 2, 3, 0, 0, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 3, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 3, 2, 2, 2, 2, 0, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 1, 2, 2, 0, 2, 0, 0, 2, 2, 1, 0, 3, 2, 2, 2, 1, 2, 0, 0, 0, 2, 3, 2, 1, 0, 0, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 0, 2, 2, 3, 0, 2, 0, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 2, 3, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 2, 3, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 3, 2, 2, 2, 2, 0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 3, 2, 0, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 0, 1, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 3, 2, 2, 0, 2, 2, 2, 3, 2, 0, 2, 0, 2, 2, 3, 0, 2, 3, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 1, 2, 3, 0, 2, 0, 1, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 1, 1, 2, 0, 2, 3, 2, 2, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 3, 0, 2, 2, 3, 2, 3, 2, 1, 2, 0, 2, 2, 0, 0, 2, 2, 3, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 1, 2, 2, 0, 0, 2, 1, 1, 0, 3, 0, 2, 2, 2, 2, 0, 0, 0, 3, 0, 0, 2, 2, 0, 2, 2, 2, 1, 0, 2, 0, 0, 2, 2, 2, 0, 3, 0, 2, 0, 3, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 3, 2, 2, 0, 2, 2, 1, 1, 3, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 3, 1, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 3, 1, 0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 3, 1, 3, 2, 2, 3, 2, 2, 2, 0, 2, 0, 3, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 3, 2, 2, 3, 0, 2, 2, 3, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 0, 2, 2, 3, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 3, 2, 2, 2, 3, 0, 0, 2, 2, 2, 2, 2, 2, 3, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 3, 3, 0, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 3, 0, 2, 0, 2, 2, 1, 0, 2, 0, 2, 3, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 2, 2, 3, 2, 2, 2, 2, 2, 0, 2, 2, 3, 0, 2, 3, 0, 2, 2, 0, 2, 1, 0, 2, 2, 1, 2, 0, 2, 2, 0, 2, 2, 2, 3, 0, 2, 1, 0, 0, 1, 3, 3, 2, 2, 2, 2, 0, 2, 0, 3, 2, 2, 2, 2, 1, 2, 2, 0, 2, 0, 2, 2, 2, 3, 2, 2, 2, 2, 0, 2, 3, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 1, 0, 2, 2, 0, 0, 0, 2, 1, 0, 1, 2, 3, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "# print(os.environ)\n",
    "# print(torch.cuda.device_count())\n",
    "# print(torch.cuda.current_device())\n",
    "# print(os.getpid())\n",
    "# sys.stdout.flush()\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# a = torch.randn(10, 10, device=device)\n",
    "# c\n",
    "# os.system('nvidia-smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_preds_array = np.zeros((len(exp_preds), 4))\n",
    "exp_preds_array[:,1] = 1\n",
    "\n",
    "auc = metrics.roc_auc_score(exp_truths,exp_preds_array, multi_class = \"ovo\")\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]], dtype=torch.float64)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ndarray((100, 4))\n",
    "a = torch.from_numpy(a)\n",
    "a = nn.Softmax(dim=1)(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
