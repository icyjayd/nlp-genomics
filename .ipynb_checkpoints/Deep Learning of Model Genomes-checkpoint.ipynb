{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import youtokentome as yttm\n",
    "from functools import partial\n",
    "import gzip\n",
    "from BCBio import GFF\n",
    "from functools import partial\n",
    "model_path = \"/gpfs/data/johnsonslab/nlp-genomics/pasolli-2019/metagenomes_8192k.model\"\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import time\n",
    "bpe = yttm.BPE(model=model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_genomes(path, gzipped=False):\n",
    "    if(gzipped):\n",
    "        with gzip.open(path, 'r') as f:\n",
    "            file_content = f.readlines()\n",
    "            file_content = [content.strip() for content in file_content]\n",
    "            \n",
    "    else:\n",
    "        with open(path, 'r') as f:\n",
    "            file_content = f.readlines()\n",
    "            file_content = [content.strip() for content in file_content]\n",
    "    if isinstance(file_content[0], bytes):\n",
    "        file_content = [line.decode('utf') for line in file_content]\n",
    "    genomes = {}\n",
    "    bases = \"\"\n",
    "    header = file_content[0]\n",
    "    \n",
    "    for i in range(1, len(file_content)):\n",
    "        if(\">\") in file_content[i]:\n",
    "            genomes[header] = bases\n",
    "            header = file_content[i]\n",
    "            bases=\"\"\n",
    "        else:\n",
    "            bases+= file_content[i] \n",
    "            if(i==len(file_content)-1):\n",
    "                genomes[header] = bases\n",
    "                header = file_content[i]\n",
    "                bases=\"\"\n",
    "    return genomes\n",
    "\n",
    "def retrieve_genome_from_gff(in_file):\n",
    "    with(open(in_file)) as in_handle:\n",
    "        return \"\".join([str(rec.seq) for rec in GFF.parse(in_handle)])\n",
    "\n",
    "def tokenize_with_length_threshold(string, length_threshold=100):\n",
    "    tokens= bpe.encode(string, output_type =yttm.OutputType.SUBWORD )\n",
    "    out = []\n",
    "    for token in tokens:\n",
    "        if(len(token)>=length_threshold):\n",
    "            out.append(token)\n",
    "    if(len(out)==0):\n",
    "        out.append(\"none\")\n",
    "#     print(len(tokens), len(out))\n",
    "    return out\n",
    "\n",
    "def get_clades(filename, df):\n",
    "    idx = filename[:-4]\n",
    "    lineage = df.loc[idx].Lineage\n",
    "    domain = lineage[lineage.find('d__')+3:lineage.find(';')]\n",
    "    phylum_start_idx = lineage.find('p__')+3\n",
    "    phylum_end_idx = lineage[phylum_start_idx:].find(';')+phylum_start_idx\n",
    "    phylum = lineage[phylum_start_idx:phylum_end_idx]\n",
    "    class_start_idx = lineage.find('c__')+3\n",
    "    class_end_idx = lineage[class_start_idx:].find(';')+class_start_idx\n",
    "    class_ = lineage[class_start_idx:class_end_idx] #named so as not to break python\n",
    "    order_start_idx = lineage.find('o__')+3\n",
    "    order_end_idx = lineage[order_start_idx:].find(';')+order_start_idx\n",
    "    order = lineage[order_start_idx:order_end_idx]\n",
    "    family_start_idx = lineage.find('f__')+3\n",
    "    family_end_idx = lineage[family_start_idx:].find(';')+family_start_idx\n",
    "    family = lineage[family_start_idx:family_end_idx]\n",
    "    genus_start_idx = lineage.find('g__')+3\n",
    "    genus_end_idx = lineage[genus_start_idx:].find(';')+genus_start_idx\n",
    "    genus = lineage[genus_start_idx:genus_end_idx]\n",
    "    species = get_name_from_file(filename, df)\n",
    "    return domain, phylum, class_, order, family, genus, species\n",
    "\n",
    "\n",
    "    return domain, phylum\n",
    "def get_name_from_file(filename, df):\n",
    "    idx = filename[:-4]\n",
    "    lineage = df.loc[idx].Lineage\n",
    "    name = lineage[lineage.find('s__')+3:lineage.find('s__')+4] + \".\" + lineage[lineage.rfind(\" \"):]\n",
    "    return name\n",
    "def get_species_status(filename, df):\n",
    "    if(filename[:-4] not in df.index):\n",
    "        return False\n",
    "    name = get_name_from_file(filename, df)\n",
    "    if(name==\"._\"):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "    \n",
    "threshold = 0\n",
    "tokenize = partial(tokenize_with_length_threshold, length_threshold = threshold)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dataloader:\n",
    "#must store:\n",
    "#dataframe containing metadata\n",
    "#tokenizer\n",
    "#length = len(df)\n",
    "#dictionary of species rep (unique df['Species_rep'])\n",
    "#getitem: \n",
    "    #Retrieve Genome entry from df\n",
    "    #X = corresponding .gff sequence\n",
    "    #tokenize X and pad to sufficient length (maybe 500k?)\n",
    "    #y = retrieve int from species rep dict\n",
    "    #return X, y\n",
    "\n",
    "class UHGGDataset(Dataset):\n",
    "    def __init__(self, df, bpe, path_to_files, pad_length=500000, label_col = 'Species_rep'):\n",
    "        self.df = df\n",
    "        self.bpe = bpe\n",
    "        self.idx_dict = dict(zip(np.arange(len(df)), list(df.index)))\n",
    "        self.label_col = label_col\n",
    "        self.class_dict = dict(zip(df[label_col].unique(), np.arange(len(df[label_col].unique()))))\n",
    "#         print(len(df), len(self.class_dict.keys()))\n",
    "        self.path = path_to_files\n",
    "        self.pad_length = pad_length\n",
    "    def __len__(self):\n",
    "        return(len(self.df))\n",
    "    def pad_to_length(self, tkns):\n",
    "        if(len(tkns)<self.pad_length):\n",
    "            tkns += [0] * (self.pad_length-len(tkns))\n",
    "        else:\n",
    "            tkns=tkns[:self.pad_length]\n",
    "        return tkns\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.idx_dict[idx]\n",
    "        seq = retrieve_genome_from_gff(os.path.join(self.path, name + \".gff\"))\n",
    "        tokens = self.bpe.encode(seq)\n",
    "        x = self.pad_to_length(tokens)\n",
    "        x = torch.tensor(x)\n",
    "        species_rep = self.df.loc[name][self.label_col]\n",
    "        y = self.class_dict[species_rep]\n",
    "        y = torch.tensor(y).long()\n",
    "        return x.cuda(), y.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Model:\n",
    "##Convolutions until manageable?\n",
    "##Transformer?\n",
    "##Linear layer output\n",
    "\n",
    "class GenomeNet(nn.Module):\n",
    "    def __init__(self, embedding_length, embedding_size=40, out_size=10):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(embedding_length, embedding_size)\n",
    "#         self.convs = nn.Sequential(self.ConvBlock(1, 4, 3), self.ConvBlock(4, 8, 3),\n",
    "#                                   self.ConvBlock(8,16,3), self.ConvBlock(8, 32, 3))\n",
    "        self.convs = nn.Sequential(nn.Conv1d(40, 80, 7, stride=2),nn.ReLU(),\n",
    "                                   nn.Conv1d(80, 160, 5, stride=2),nn.ReLU(),\n",
    "                                  nn.Conv1d(160, 320, 3, stride=2),nn.ReLU(),\n",
    "                                   nn.Conv1d(320, 512, 3, stride=2),nn.ReLU(),\n",
    "                                  nn.Conv1d(512, 128, 3, stride=2),\n",
    "                                  nn.Conv1d(128, 32, 3, stride=2),\n",
    "                                  nn.Conv1d(32, 4, 3, stride=2))\n",
    "#         self.transformer = nn.Transformer()\n",
    "        self.linear = nn.Sequential(nn.Linear(15620, out_size), nn.ReLU())\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def ConvBlock(*args, **kwargs):\n",
    "        \n",
    "#         conv = nn.Conv1d(*args, *kwargs)\n",
    "#         relu = nn.ReLU()\n",
    "#         return nn.Sequential([conv, relu]) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.reshape(x.shape[0], x.shape[2], x.shape[1])\n",
    "        x = self.convs(x)\n",
    "        x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "#         return self.transformer(x)\n",
    "    \n",
    "\n",
    "model = GenomeNet(8192000).cuda()\n",
    "x = torch.randint(0, 8192000, [1,500000]).cuda()\n",
    "model(x).shape\n",
    "# def foo(*args, **kwargs):\n",
    "                         \n",
    "#     return bar(*args, *kwargs)\n",
    "\n",
    "# def bar(a, b, c=\"c\", d=\"d\"):\n",
    "#     return [a, b, c, d]\n",
    "# foo(1, 2, '3','4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/gpfs/data/johnsonslab/nlp-genomics/almeida-2020/'\n",
    "df = pd.read_csv(os.path.join(path, 'genomes-all_metadata.tsv'), sep='\\t', index_col=0)\n",
    "## Temporary code for not fully downloaded dataset\n",
    "files = os.listdir(os.path.join(path, 'gffs'))\n",
    "files = [file[:-4] for file in files]\n",
    "df = df.loc[files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # tokens = bpe.encode(seq)\n",
    "# out = model(test)\n",
    "# print(len(os.listdir(path + \"/gffs\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China            57232\n",
      "United States    49429\n",
      "Denmark          28107\n",
      "Spain            21163\n",
      "Sweden           19498\n",
      "                 ...  \n",
      "Zimbabwe             1\n",
      "Bolivia              1\n",
      "Turkmenistan         1\n",
      "Burkina Faso         1\n",
      "Afghanistan          1\n",
      "Name: Country, Length: 100, dtype: int64\n",
      "235750\n"
     ]
    }
   ],
   "source": [
    "#get index of top 10\n",
    "idx = df['Country'].value_counts().index\n",
    "print((df['Country'].value_counts()))\n",
    "df = df[df['Country'].isin(idx[:10])]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "label_col = \"Country\"\n",
    "np.random.seed(42)\n",
    "train_set, val_set = train_test_split(df, test_size=0.3)\n",
    "val_set, test_set = train_test_split(val_set, test_size=0.5)\n",
    "\n",
    "train_loader = DataLoader(UHGGDataset(train_set, bpe, os.path.join(path + \"gffs\"), label_col=label_col),batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(UHGGDataset(val_set, bpe, os.path.join(path + \"gffs\"), label_col=label_col),batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(UHGGDataset(test_set, bpe, os.path.join(path + \"gffs\"), label_col = label_col),batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_epochs = 50\n",
    "model = GenomeNet(8192000).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    epoch_time = time.time()\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "#         loss = 0\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'Train: [{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f} epoch time: {time.time()-epoch_time}')\n",
    "            running_loss = 0.0\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),            \n",
    "    }, '/gpfs/data/johnsonslab/nlp-genomics/model_weights/geneomenet_conv_weights/genomenet_conv_epoch_{}'.format(epoch))\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch time: {time.time()-start_time}\")\n",
    "    model.eval()\n",
    "    for i,(x, y) in enumerate(val_loader):\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        running_loss += loss.item()\n",
    "    print(f'Val: [{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "\n",
    "\n",
    "##for epoch in range(num_epochs)\n",
    "#for x, y in train_loader:\n",
    "#optimizer.zero_grad\n",
    "#out = model(x)\n",
    "#loss = crossentropyloss(out, y)\n",
    "#optimizer.step()\n",
    "#print training loss\n",
    "#for x, y in val_loader:\n",
    "#with model.no_grad():\n",
    "#repeat above but without optimizer and evaluate Recall, Precision, Accuracy, Sensitivity, and #AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
